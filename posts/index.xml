<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>Posts on SDN-Warrior | Daniel Krieger</title>
		<link>https://sdn-warrior.org/posts/</link>
		<description>Recent content in Posts on SDN-Warrior | Daniel Krieger</description>
		<generator>Hugo -- gohugo.io</generator>
		<language>en-us</language>
		<copyright>Daniel Krieger</copyright>
		<lastBuildDate>Sat, 11 Jan 2025 02:00:00 +0100</lastBuildDate>
		<atom:link href="https://sdn-warrior.org/posts/index.xml" rel="self" type="application/rss+xml" />
		
		<item>
			<title>How Apply To works in NSX DFW</title>
			<link>https://sdn-warrior.org/posts/nsx-apply-to/</link>
			<pubDate>Sat, 11 Jan 2025 02:00:00 +0100</pubDate>
			
			<guid>https://sdn-warrior.org/posts/nsx-apply-to/</guid>
			<description><![CDATA[How Apply To works in NSX DFW and you can use it.]]></description>
			<content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>When working with the <strong>NSX Distributed Firewall (DFW)</strong>, one feature that often goes unnoticed or misunderstood is <strong>&lsquo;Apply To&rsquo;</strong>. Despite its importance, it is frequently underestimated or even ignored. This is unfortunate, as <strong>&lsquo;Apply To&rsquo;</strong> is a powerful feature that can significantly influence how firewall rules are applied within an NSX environment.</p>
<p>In many VMware training courses, <strong>&lsquo;Apply To&rsquo;</strong> is either poorly explained or not mentioned at all. As a result, administrators and engineers might miss out on opportunities to optimize their firewall rule configurations. Misunderstanding or neglecting this feature can lead to overly complex rulesets or unexpected behavior in distributed environments.</p>
<p>In this post, I aim to demystify the <strong>&lsquo;Apply To&rsquo;</strong> feature. I will explore its functionality, demonstrate its practical use cases, and analyze how it impacts the behavior of firewall rules. By the end, you should have a solid understanding of how and when to use <strong>&lsquo;Apply To&rsquo;</strong> effectively in your NSX environment.</p>
<h2 id="questions-to-address">Questions to Address</h2>
<p>To fully understand the <strong>&lsquo;Apply To&rsquo;</strong> feature in the NSX Distributed Firewall, it is essential to examine its behavior under various scenarios. In this post, I will address the following questions:</p>
<ol>
<li>
<p><strong>What happens if we use the <code>&lt;Applied To&gt;</code> field on the policy and set the <code>&lt;Applied To&gt;</code> rule as DFW?</strong><br>
How does this configuration affect the scope and enforcement of firewall rules?</p>
</li>
<li>
<p><strong>What will happen if we use a Security Group in the <code>&lt;Applied To&gt;</code> field in the rule, while the <code>&lt;Applied To&gt;</code> field in the policy is set as DFW?</strong><br>
What interactions or overlaps should be expected between these configurations?</p>
</li>
<li>
<p><strong>What happens if we use <code>security group1</code> in the policy <code>&lt;Applied To&gt;</code> field and <code>security group2</code> in the rule <code>&lt;Applied To&gt;</code> field?</strong><br>
How do these overlapping or conflicting settings impact the rule application?</p>
</li>
<li>
<p><strong>Does the <code>&lt;Applied To&gt;</code> field in the rule or the <code>&lt;Applied To&gt;</code> field in the policy take precedence?</strong><br>
When both are defined, which one ultimately dictates the scope of rule enforcement?</p>
</li>
</ol>
<p>By exploring these scenarios, I aim to clarify the nuanced behavior of the <strong>&lsquo;Apply To&rsquo;</strong> feature and provide actionable insights for optimizing your NSX DFW configurations.</p>
<h2 id="practical-demonstrations-with-cli">Practical Demonstrations with CLI</h2>
<p>In addition to exploring the theoretical aspects of the <strong>&lsquo;Apply To&rsquo;</strong> feature, I will use practical CLI commands to demonstrate where the rules are actually enforced within the NSX infrastructure. This hands-on approach will help you visualize and verify how <strong>&lsquo;Apply To&rsquo;</strong> configurations are realized in practice.</p>
<p>By combining both conceptual explanations and practical examples, you will gain a deeper understanding of how to effectively use the <strong>&lsquo;Apply To&rsquo;</strong> feature in real-world scenarios.</p>
<h2 id="test-setup">Test Setup</h2>
<p>For this blog, I am using <strong>NSX version 4.2.2.1</strong> to demonstrate the behavior of the <strong>&lsquo;Apply To&rsquo;</strong> feature in the Distributed Firewall. The goal is to keep the setup simple yet effective for understanding how different configurations influence the enforcement of firewall rules.</p>
<h3 id="environment-overview">Environment Overview</h3>
<p>The test environment consists of three lightweight Linux virtual machines running Alpine:</p>
<ul>
<li><strong>Alpine01</strong>: <code>10.10.20.10</code></li>
<li><strong>Alpine02</strong>: <code>10.10.20.20</code></li>
<li><strong>Alpine03</strong>: <code>10.10.20.30</code> (Control VM)</li>
</ul>
<p>All VMs reside on the <strong>same NSX segment</strong>, simplifying the network to focus on rule behavior and ensuring consistent connectivity between VMs. Notably, <strong>Alpine03</strong> serves as a control VM and is not part of any NSX Security Group. This ensures it remains unaffected by specific <strong>&lsquo;Apply To&rsquo;</strong> configurations, providing a baseline for comparison.</p>
<h3 id="security-groups">Security Groups</h3>
<p>To organize and manage the test VMs, I have created the following NSX Security Groups:</p>
<ol>
<li>
<p><strong>dFG_all_Alpine</strong><br>
This group includes <strong>Alpine01</strong> and <strong>Alpine02</strong>. It represents all Alpine VMs involved in the main testing scenarios.</p>
</li>
<li>
<p><strong>dFG_Alpine01</strong><br>
A dedicated Security Group for <strong>Alpine01</strong>, enabling granular control over rules specific to this VM.</p>
</li>
<li>
<p><strong>dFG_Alpine02</strong><br>
A separate Security Group for <strong>Alpine02</strong>, allowing isolated configurations tailored to this VM.</p>
</li>
</ol>
<p><strong>Alpine03</strong> does not belong to any Security Group, making it a neutral VM for verifying how rules or configurations behave when no specific policies are applied. This provides a clear reference point to validate the impact of <strong>&lsquo;Apply To&rsquo;</strong> settings.</p>
<h3 id="why-alpine03-as-a-control-vm">Why Alpine03 as a Control VM?</h3>
<p>Having a control VM like <strong>Alpine03</strong> ensures we have a clean baseline to compare against during the tests. By keeping it outside of any Security Group, we can confirm that any observed effects are solely due to the configurations applied to <strong>Alpine01</strong> and <strong>Alpine02</strong>. This approach eliminates ambiguity and helps highlight the true behavior of the <strong>&lsquo;Apply To&rsquo;</strong> feature.</p>
<h3 id="additional-tools-for-validation">Additional Tools for Validation</h3>
<p>To analyze the behavior further, I will use CLI commands such as <strong><code>summarize-dvfilter</code></strong> and <strong><code>vsipioctl getrules</code></strong>, as well as the NSX <strong>Traceflow</strong> tool. This combination ensures an in-depth understanding of where and how rules are enforced, both at the hypervisor and the network level.</p>
<h3 id="naming-convention-a-recommended-best-practice">Naming Convention: A Recommended Best Practice</h3>
<p>As a personal convention, I prefix all custom Security Groups with <strong>dFG</strong>, which stands for Distributed Firewall Group. This prefix helps me quickly identify and differentiate groups used for NSX Distributed Firewall purposes from other objects in the environment.</p>
<p>I strongly recommend adopting a consistent and meaningful naming scheme for your NSX environment. A clear structure not only improves day-to-day management but also prevents confusion in larger setups with potentially hundreds of objects. Whether you use prefixes like <code>dFG</code> or other naming conventions, the key is consistency.</p>
<h3 id="why-use-a-single-esxi-host">Why Use a Single ESXi Host?</h3>
<p>Running both Alpine VMs on a single ESXi host is intentional. It allows me to leverage the <strong>ESXi CLI</strong> to show where and how the Distributed Firewall rules are realized during the tests. This provides deeper insights into the inner workings of the NSX DFW, bridging the gap between configuration in the NSX Manager and actual enforcement at the hypervisor level.</p>
<p>This setup offers a practical foundation to explore and analyze the <strong>&lsquo;Apply To&rsquo;</strong> feature in detail, combining theoretical explanations with real-world CLI examples.</p>
<h3 id="verification-with-cli-tools">Verification with CLI Tools</h3>
<p>To complement the theoretical understanding of the <strong>&lsquo;Apply To&rsquo;</strong> feature, I will use practical CLI tools during the tests to demonstrate where and how firewall rules are enforced. This includes inspecting the ESXi host to see how the Distributed Firewall implements the configurations. Below are the CLI commands and tools I will use:</p>
<ol>
<li>
<p><strong><code>summarize-dvfilter | grep -A16 &lt;VMName&gt;</code></strong><br>
This command retrieves detailed information about the virtual NIC (vNIC) associated with a specific VM. By identifying the correct vNIC name (e.g., <code>&lt;nic-XXXXXXX-eth0-vmware-sfw.2&gt;</code>), we can pinpoint the interface where the firewall rules are applied.</p>
</li>
<li>
<p><strong><code>vsipioctl getrules -f &lt;name from the vNIC&gt;</code></strong><br>
Once the vNIC name is identified, this command provides the complete list of firewall rules applied to that specific VM. This is a powerful way to verify the exact rules enforced at the hypervisor level.</p>
</li>
<li>
<p><strong>NSX Traceflow Tool</strong><br>
In addition to ESXi CLI commands, the NSX <strong>Traceflow</strong> tool can be used to simulate and trace packet flow through the network. This tool helps visualize the path packets take and how firewall rules affect traffic between VMs.</p>
</li>
</ol>
<h3 id="practical-workflow">Practical Workflow</h3>
<p>During the tests, the workflow will involve the following steps:</p>
<ol>
<li>Use the <strong><code>summarize-dvfilter</code></strong> command to locate the vNIC name for the VM being tested (e.g., Alpine01 or Alpine02).</li>
<li>Retrieve the enforced firewall rules using <strong><code>vsipioctl getrules</code></strong> with the vNIC name as input.</li>
<li>Validate the observed behavior using the NSX Traceflow tool to ensure that the rules are applied as expected and to simulate specific traffic flows for further analysis.</li>
</ol>
<p>These tools provide a hands-on approach to understanding how the <strong>&lsquo;Apply To&rsquo;</strong> feature works, bridging the gap between configuration and real-world enforcement.</p>
<h2 id="test-1-what-happens-if-we-use-the-applied-to-field-on-the-policy-and-set-the-applied-to-rule-as-dfw">Test 1: What Happens If We Use the <code>&lt;Applied To&gt;</code> Field on the Policy and Set the <code>&lt;Applied To&gt;</code> Rule as DFW?</h2>
<p>To begin our exploration of the <strong>&lsquo;Apply To&rsquo;</strong> feature, we start with a simple scenario. In this test, I create a policy that allows <strong>ICMP traffic</strong> from <strong>any</strong> source to the Security Group <strong>dFG_all_Alpine</strong>. The <strong>&lsquo;Apply To&rsquo;</strong> configuration is as follows:</p>
<ul>
<li>The <strong>rule&rsquo;s <code>&lt;Applied To&gt;</code> field</strong> is set to <strong>DFW (Distributed Firewall)</strong>.</li>
<li>The <strong>policy&rsquo;s <code>&lt;Applied To&gt;</code> field</strong> is set to the Security Group <strong>dFG_all_Alpine</strong>.</li>
</ul>
<h3 id="configuration-details">Configuration Details</h3>
<ol>
<li>
<p><strong>Policy</strong>:</p>
<ul>
<li>Name: <strong>ICMP Allow Test</strong></li>
<li>Source: <strong>Any</strong></li>
<li>Destination: <strong>dFG_all_Alpine</strong></li>
<li>Service: <strong>ICMP</strong></li>
<li>Action: <strong>Allow</strong></li>
<li><code>&lt;Applied To&gt;</code>: <strong>dFG_all_Alpine</strong></li>
</ul>
</li>
<li>
<p><strong>Rule</strong>:</p>
<ul>
<li><code>&lt;Applied To&gt;</code>: <strong>DFW</strong></li>
</ul>
</li>
</ol>
<h3 id="expected-behavior">Expected Behavior</h3>
<p>In this configuration, the <strong>policy&rsquo;s <code>&lt;Applied To&gt;</code> field</strong> limits the scope of the policy to the members of the <strong>dFG_all_Alpine</strong> group.</p>
<p>The expected result is that the rule is <strong>only enforced</strong> for traffic destined to the <strong>dFG_all_Alpine</strong> group (as defined in the policy&rsquo;s <code>&lt;Applied To&gt;</code>) even if the rule is set do <strong>DFW</strong>.</p>
<h3 id="test-results">Test Results</h3>
<p>Using the ESXi CLI, I collected the following details for the vNICs associated with each VM:</p>
<ul>
<li>
<p><strong>Alpine01</strong>:</p>
<ul>
<li>Port: <code>67108887</code></li>
<li>vNIC name: <code>nic-533240-eth0-vmware-sfw.2</code></li>
</ul>
</li>
<li>
<p><strong>Alpine02</strong>:</p>
<ul>
<li>Port: <code>67108888</code></li>
<li>vNIC name: <code>nic-533279-eth0-vmware-sfw.2</code></li>
</ul>
</li>
<li>
<p><strong>Alpine03 (Control VM)</strong>:</p>
<ul>
<li>Port: <code>67108889</code></li>
<li>vNIC name: <code>nic-544799-eth0-vmware-sfw.2</code></li>
</ul>
</li>
</ul>
<h4 id="firewall-rules-observed">Firewall Rules Observed</h4>
<ol>
<li><strong>Alpine01 (nic-533240-eth0-vmware-sfw.2)</strong> and <strong>Alpine02 (nic-533279-eth0-vmware-sfw.2)</strong>:<br>
Both VMs have the following rules applied:</li>
</ol>
<pre tabindex="0"><code>ruleset mainrs {
# PRE_FILTER rules
rule 10216 at 1 inout protocol tcp strict from any to addrset a34212cb-acb2-49b3-b74c-7683c0345a19 port 22 accept;
# FILTER (APP Category) rules
rule 10217 at 1 inout protocol icmp from any to addrset a34212cb-acb2-49b3-b74c-7683c0345a19 accept;
rule 10217 at 2 inout protocol ipv6-icmp from any to addrset a34212cb-acb2-49b3-b74c-7683c0345a19 accept;
rule 2 at 3 inout protocol any from any to any reject with log tag &#39;debug&#39;;
}
</code></pre><p>Key Observations:</p>
<p>ICMP traffic is explicitly allowed (rule 10217) for both IPv4 and IPv6 from any source to the address set associated with dFG_all_Alpine.
All other traffic is rejected (rule 2), demonstrating that the rules are scoped correctly to the policy&rsquo;s <!-- raw HTML omitted --> field.
Alpine02 (nic-533279-eth0-vmware-sfw.2):
The rules applied to Alpine02 are identical to those observed for Alpine01</p>
<ol start="2">
<li><strong>Alpine03 (Control VM, nic-544799-eth0-vmware-sfw.2)</strong>:</li>
</ol>
<pre tabindex="0"><code>ruleset mainrs {
# FILTER (APP Category) rules
rule 2 at 1 inout protocol any from any to any reject with log tag &#39;debug&#39;;
}
</code></pre><p>Key Observations:</p>
<p>Only the default reject rule is applied. Since Alpine03 is not a member of dFG_all_Alpine, the policy does not apply to this VM, confirming that the <!-- raw HTML omitted --> field in the policy limits enforcement to the intended scope.</p>
<p>Conclusion for Test 1
The results confirm the expected behavior:</p>
<p>The policy&rsquo;s <!-- raw HTML omitted --> field effectively limits the scope of enforcement to the Security Group dFG_all_Alpine.
Although the rule&rsquo;s <!-- raw HTML omitted --> field is set to DFW, rules are only applied to the VMs within the policy&rsquo;s scope.
The Control VM (Alpine03), which is outside the Security Group, does not have the ICMP allow rule applied, demonstrating the precision of the <!-- raw HTML omitted --> field.</p>
<h2 id="test-2-what-will-happen-if-we-use-a-security-group-in-the-applied-to-field-in-the-rule-and-the-applied-to-field-in-the-policy-is-set-as-dfw">Test 2: What Will Happen If We Use a Security Group in the <code>&lt;Applied To&gt;</code> Field in the Rule and the <code>&lt;Applied To&gt;</code> Field in the Policy Is Set as DFW?</h2>
<p>In this scenario, I configure a policy to allow <strong>ICMP traffic</strong> from <strong>any</strong> source to the <strong>dFG_all_Alpine</strong> Security Group. However, this time I modify the <strong><code>&lt;Applied To&gt;</code> field</strong> as follows:</p>
<ul>
<li>The <strong>policy’s <code>&lt;Applied To&gt;</code> field</strong> is set to <strong>DFW (Distributed Firewall)</strong>.</li>
<li>The <strong>rule’s <code>&lt;Applied To&gt;</code> field</strong> is set to the Security Group <strong>dFG_Alpine01</strong>.</li>
</ul>
<h3 id="configuration-details-1">Configuration Details</h3>
<ol>
<li>
<p><strong>Policy</strong>:</p>
<ul>
<li>Name: <strong>ICMP Allow Test 2</strong></li>
<li>Source: <strong>Any</strong></li>
<li>Destination: <strong>dFG_all_Alpine</strong></li>
<li>Service: <strong>ICMP</strong></li>
<li>Action: <strong>Allow</strong></li>
<li><code>&lt;Applied To&gt;</code>: <strong>DFW</strong></li>
</ul>
</li>
<li>
<p><strong>Rule</strong>:</p>
<ul>
<li><code>&lt;Applied To&gt;</code>: <strong>dFG_Alpine01</strong></li>
</ul>
</li>
</ol>
<h3 id="expected-behavior-1">Expected Behavior</h3>
<p>This configuration introduces a more restrictive scope at the <strong>rule</strong> level:</p>
<ul>
<li>Since the rule’s <code>&lt;Applied To&gt;</code> field is set to <strong>dFG_Alpine01</strong>, it should only be <strong>enforced on the VMs in this group</strong>.</li>
<li>The policy’s broader <code>&lt;Applied To&gt;</code> field (set to <strong>DFW</strong>) may lead you to assume that the policy and its rules are applied everywhere, but actual enforcement should be limited to <strong>dFG_Alpine01</strong> due to the rule-level restriction.</li>
</ul>
<h3 id="results">Results</h3>
<h4 id="firewall-rules-observed-1">Firewall Rules Observed</h4>
<ol>
<li><strong>Alpine01 (nic-533240-eth0-vmware-sfw.2)</strong>:<br>
The following rules were observed:</li>
</ol>
<pre tabindex="0"><code>ruleset mainrs {
# PRE_FILTER rules
rule 10216 at 1 inout protocol tcp strict from any to addrset a34212cb-acb2-49b3-b74c-7683c0345a19 port 22 accept;
# FILTER (APP Category) rules
rule 10217 at 1 inout protocol icmp from any to addrset a34212cb-acb2-49b3-b74c-7683c0345a19 accept;
rule 10217 at 2 inout protocol ipv6-icmp from any to addrset a34212cb-acb2-49b3-b74c-7683c0345a19 accept;
rule 2 at 3 inout protocol any from any to any reject with log tag &#39;debug&#39;;
}
</code></pre><p>Key Observations:</p>
<p>The ICMP allow rule is enforced as expected, scoped to Alpine01, which is part of dFG_Alpine01.</p>
<ol start="2">
<li><strong>Alpine02 (nic-533279-eth0-vmware-sfw.2):</strong>
The following rules were observed:</li>
</ol>
<pre tabindex="0"><code>ruleset mainrs {
# FILTER (APP Category) rules
 rule 2 at 1 inout protocol any from any to any reject with log tag &#39;debug&#39;;
}
</code></pre><p>Key Observations:</p>
<p>The ICMP allow rule is not applied to Alpine02, as it is not part of dFG_Alpine01.</p>
<ol start="3">
<li><strong>Alpine03 (Control VM, nic-544799-eth0-vmware-sfw.2):</strong>
The following rules were observed:</li>
</ol>
<pre tabindex="0"><code>ruleset mainrs {
 # FILTER (APP Category) rules
 rule 2 at 1 inout protocol any from any to any reject with log tag &#39;debug&#39;;
}
</code></pre><p>Key Observations:</p>
<p>As expected, no ICMP allow rule is applied since Alpine03 is not part of any relevant Security Group.</p>
<h2 id="test-3-what-happens-if-we-use-security-group1-in-the-policy-applied-to-field-and-group2-in-the-rule-applied-to-field">Test 3: What Happens If We Use Security <code>&lt;group1&gt;</code> in the Policy <code>&lt;Applied To&gt;</code> Field and <code>&lt;group2&gt;</code> in the Rule <code>&lt;Applied To&gt;</code> Field?</h2>
<p>In this test, I explore the interaction between the <code>&lt;Applied To&gt;</code> fields at the policy and rule levels when they reference different Security Groups. The configuration is as follows:</p>
<ul>
<li>The <strong>policy’s <code>&lt;Applied To&gt;</code> field</strong> is set to <strong>dFG_Alpine01</strong>.</li>
<li>The <strong>rule’s <code>&lt;Applied To&gt;</code> field</strong> is set to <strong>dFG_Alpine02</strong>.</li>
<li>The rule allows <strong>ICMP traffic</strong> from <strong>any</strong> source to the <strong>dFG_all_Alpine</strong> Security Group.</li>
</ul>
<h3 id="configuration-details-2">Configuration Details</h3>
<ol>
<li>
<p><strong>Policy</strong>:</p>
<ul>
<li>Name: <strong>ICMP Allow Test 3</strong></li>
<li>Source: <strong>Any</strong></li>
<li>Destination: <strong>dFG_all_Alpine</strong></li>
<li>Service: <strong>ICMP</strong></li>
<li>Action: <strong>Allow</strong></li>
<li><code>&lt;Applied To&gt;</code>: <strong>dFG_Alpine01</strong></li>
</ul>
</li>
<li>
<p><strong>Rule</strong>:</p>
<ul>
<li><code>&lt;Applied To&gt;</code>: <strong>dFG_Alpine02</strong></li>
</ul>
</li>
</ol>
<h3 id="expected-behavior-2">Expected Behavior</h3>
<p>In this configuration, the <strong>policy’s <code>&lt;Applied To&gt;</code> field</strong> restricts the scope of the policy to <strong>dFG_Alpine01</strong>, which includes only <strong>Alpine01</strong>. However, the <strong>rule’s <code>&lt;Applied To&gt;</code> field</strong> is set to <strong>dFG_Alpine02</strong>, which includes only <strong>Alpine02</strong>.</p>
<p>The expected behavior is:</p>
<ol>
<li>The policy’s <code>&lt;Applied To&gt;</code> field should dictate the overall scope, meaning the rule will only be <strong>enforced for members of dFG_Alpine01</strong>.</li>
<li>Since the rule’s <code>&lt;Applied To&gt;</code> field is set to <strong>dFG_Alpine02</strong>, the rule will not be applied to <strong>Alpine02</strong>, as it is outside the policy’s scope.</li>
<li>ICMP traffic from <strong>any source</strong> to <strong>dFG_all_Alpine</strong> will only be allowed for VMs within <strong>dFG_Alpine01</strong>.</li>
</ol>
<h3 id="test-results-1">Test Results</h3>
<h4 id="firewall-rules-observed-2">Firewall Rules Observed</h4>
<ol>
<li><strong>Alpine01 (nic-533240-eth0-vmware-sfw.2)</strong>:</li>
</ol>
<pre tabindex="0"><code>ruleset mainrs {
# PRE_FILTER rules
rule 10216 at 1 inout protocol tcp strict from any to addrset a34212cb-acb2-49b3-b74c-7683c0345a19 port 22 accept;
# FILTER (APP Category) rules
rule 10217 at 1 inout protocol icmp from any to addrset a34212cb-acb2-49b3-b74c-7683c0345a19 accept;
rule 10217 at 2 inout protocol ipv6-icmp from any to addrset a34212cb-acb2-49b3-b74c-7683c0345a19 accept;
rule 2 at 3 inout protocol any from any to any reject with log tag &#39;debug&#39;;
}
ruleset mainrs_L2 {
# FILTER rules
rule 1 at 1 inout ethertype any stateless from any to any accept;
}
</code></pre><p>Key Observations:</p>
<p>ICMP traffic (IPv4 and IPv6) is explicitly allowed (rule 10217) for Alpine01.
This behavior aligns with the policy&rsquo;s <!-- raw HTML omitted --> field, as Alpine01 is part of dFG_Alpine01.</p>
<ol start="2">
<li><strong>Alpine02 (nic-533279-eth0-vmware-sfw.2):</strong></li>
</ol>
<pre tabindex="0"><code>ruleset mainrs {
# PRE_FILTER rules
rule 10216 at 1 inout protocol tcp strict from any to addrset a34212cb-acb2-49b3-b74c-7683c0345a19 port 22 accept;
# FILTER (APP Category) rules
rule 2 at 1 inout protocol any from any to any reject with log tag &#39;debug&#39;;
}
</code></pre><p>Key Observations:
No ICMP allow rule is present for Alpine02, as it is not part of dFG_Alpine01, which is referenced in the policy&rsquo;s <!-- raw HTML omitted --> field.
All traffic is rejected at the main ruleset level (rule 2), confirming that the rule-level <!-- raw HTML omitted --> field (set to dFG_Alpine02) does not enforce the rule here.</p>
<ol start="3">
<li><strong>Alpine03 (Control VM, nic-544799-eth0-vmware-sfw.2)</strong>:</li>
</ol>
<pre tabindex="0"><code>ruleset mainrs {
# FILTER (APP Category) rules
rule 2 at 1 inout protocol any from any to any reject with log tag &#39;debug&#39;;
}
</code></pre><p>Key Observations:</p>
<p>The only rule applied at the main ruleset level is a default reject rule (rule 2), which blocks all traffic. This confirms that no ICMP allow rule from the policy or rule is applied to Alpine03, as it is not part of dFG_Alpine01.</p>

    <aside class="admonition info">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-info">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="16" x2="12" y2="12"></line>
      <line x1="12" y1="8" x2="12.01" y2="8"></line>
   </svg></div><b>Important Note: Double Enforcement in NSX Distributed Firewall</b>
        </div>
        <div class="admonition-content"><p>Even though traffic to <strong>Alpine01</strong> is allowed from <strong>any</strong> source, <strong>Alpine02</strong> cannot ping <strong>Alpine01</strong>. This is because <strong>Alpine02</strong> does not have a corresponding firewall rule permitting ICMP traffic for outbound communication.</p>
<p>In <strong>NSX Distributed Firewall</strong>, firewall rules are evaluated <strong>twice</strong> for traffic between VMs managed by NSX:</p>
<ol>
<li><strong>At the source VM</strong>: Outbound traffic must match a rule that permits it to leave the VM.</li>
<li><strong>At the destination VM</strong>: Inbound traffic must match a rule that permits it to reach the VM.</li>
</ol>
<p>This double-enforcement model ensures precise control over traffic flow but requires careful consideration when designing firewall policies. Both source and destination rules must be configured to allow traffic for successful communication.</p>
<h3 id="using-the-nsx-traceflow-tool">Using the NSX Traceflow Tool</h3>
<p>The <strong>Traceflow Tool</strong> in NSX provides an excellent way to visualize this behavior. By tracing packets, you can observe how traffic is evaluated and where it is blocked or allowed. In this scenario, Traceflow clearly demonstrates that traffic originating from <strong>Alpine02</strong> is blocked at the source due to the absence of an ICMP allow rule, even though the destination (<strong>Alpine01</strong>) has an allow rule.</p>
<p>Understanding this double-enforcement logic is crucial for troubleshooting and optimizing NSX Distributed Firewall configurations.</p>
</div>
    </aside>

<figure><a href="traceflow.png"><picture>
          <source srcset="/nsx-apply-to/traceflow_hu6341709273789521671.webp" type="image/webp">
          <source srcset="/nsx-apply-to/traceflow_hu1870482407599317965.jpg" type="image/jpeg">
          <img src="/nsx-apply-to/traceflow_hu6341709273789521671.webp"alt="Traceflow"  width="1335"  height="849" />
        </picture></a><figcaption>
            <p>NSX Traceflow (click to enlarge)</p>
          </figcaption></figure>
<h2 id="answering-question-4-does-the-rule-applied-to-field-or-the-policy-applied-to-field-take-precedence">Answering Question 4: Does the Rule <code>&lt;Applied To&gt;</code> Field or the Policy <code>&lt;Applied To&gt;</code> Field Take Precedence?</h2>
<p>Through our tests, we can confidently answer this question: <strong>The policy <code>&lt;Applied To&gt;</code> field takes precedence over the rule <code>&lt;Applied To&gt;</code> field</strong>.</p>
<h3 id="key-findings">Key Findings:</h3>
<ul>
<li>
<p>The <strong>policy’s <code>&lt;Applied To&gt;</code> field</strong> defines the overall scope of enforcement. This means that if a VM or Security Group is excluded by the policy <code>&lt;Applied To&gt;</code> field, no rules from that policy will apply to it, regardless of the rule <code>&lt;Applied To&gt;</code> field.</p>
</li>
<li>
<p>The rule&rsquo;s <code>&lt;Applied To &gt;</code> field cannot further restrict or extend enforcement within the scope defined by the policy. It simply ignored.</p>
</li>
</ul>
<h3 id="evidence">Evidence:</h3>
<ul>
<li>In <strong>Test 3</strong>, we demonstrated that even though the rule <code>&lt;Applied To&gt;</code> field was set to <strong>dFG_Alpine02</strong>, the rule was not applied to <strong>Alpine02</strong> because the policy <code>&lt;Applied To&gt;</code> field limited enforcement to <strong>dFG_Alpine01</strong>.</li>
<li>This behavior clearly shows that the policy <code>&lt;Applied To&gt;</code> field is the deciding factor in scoping firewall rule enforcement.</li>
</ul>
<p>By understanding this precedence, administrators can better design their NSX firewall policies to avoid conflicts or unintended behavior.</p>
<h2 id="why-use-applied-to-in-nsx">Why Use <code>&lt;Applied To&gt;</code> in NSX?</h2>
<p>The <strong><code>&lt;Applied To&gt;</code></strong> field in NSX Distributed Firewall is a powerful tool that enables administrators to optimize rule enforcement and improve overall performance in their environments. While it might seem optional at first glance, there are several key reasons to leverage this feature:</p>
<h3 id="1-optimizing-resource-usage">1. <strong>Optimizing Resource Usage</strong></h3>
<p>By default, Distributed Firewall rules are applied to all ESXi hosts in the cluster, even if they are irrelevant to some workloads. Using the <code>&lt;Applied To&gt;</code> field allows you to:</p>
<ul>
<li><strong>Restrict rule enforcement</strong> to specific VMs, Security Groups, or segments.</li>
<li>Reduce unnecessary rule propagation across unrelated hosts.</li>
<li>Minimize the overhead of processing firewall rules.</li>
</ul>
<h3 id="2-enhancing-rule-clarity-and-management">2. <strong>Enhancing Rule Clarity and Management</strong></h3>
<p>When <code>&lt;Applied To&gt;</code> is used correctly, it provides clear boundaries for where rules are enforced:</p>
<ul>
<li>It helps avoid confusion about which VMs are affected by specific rules.</li>
<li>It simplifies troubleshooting by narrowing down the scope of rule application.</li>
<li>It prevents accidental rule application to unintended workloads.</li>
</ul>
<h3 id="3-improving-security-posture">3. <strong>Improving Security Posture</strong></h3>
<p>Restricting the scope of firewall rules reduces the attack surface:</p>
<ul>
<li>Only the VMs, segments, or groups explicitly defined in the <code>&lt;Applied To&gt;</code> field will be impacted by the rule.</li>
<li>This minimizes the risk of unintentionally exposing unrelated workloads to less restrictive rules.</li>
</ul>
<h3 id="4-avoiding-overlap-and-rule-conflicts">4. <strong>Avoiding Overlap and Rule Conflicts</strong></h3>
<p>In complex environments, rules can overlap or conflict, leading to unexpected behavior. By carefully defining <code>&lt;Applied To&gt;</code> fields:</p>
<ul>
<li>You ensure that rules are scoped to their intended targets, reducing the risk of conflicts.</li>
<li>You can isolate specific rule sets for testing or special cases without affecting unrelated traffic.</li>
</ul>
<h3 id="conclusion">Conclusion</h3>
<p>While the <strong><code>&lt;Applied To&gt;</code></strong> field might add an extra layer of configuration, it plays a vital role in optimizing NSX Distributed Firewall performance, clarity, and security. By carefully designing and applying <code>&lt;Applied To&gt;</code> settings at both the policy and rule levels, administrators can achieve a more efficient, secure, and manageable firewall implementation.
In addition, there is a limit to the number of firewall rules that can be applied to an ESX host and to a virtual NIC. The current limits may change with the NSX version and can be looked up in the Configmax tool from Broadcom.</p>
]]></content>
		</item>
		
		<item>
			<title>More performance trough NSX Edge TEP groups?</title>
			<link>https://sdn-warrior.org/posts/nsx-tep-groups/</link>
			<pubDate>Fri, 03 Jan 2025 12:00:00 +0100</pubDate>
			
			<guid>https://sdn-warrior.org/posts/nsx-tep-groups/</guid>
			<description><![CDATA[How to use Edge TEP groups in NSX]]></description>
			<content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>My esteemed colleague, Steven Schramm, recently published an excellent article titled <em><a href="https://sdn-techtalk.com/posts/multitep-ha/">Improving NSX Datacenter TEP Performance and Availability - Multi-TEP and TEP Group High Availability</a></em>. This inspired me to explore how TEP Groups influence performance in NSX, specifically focusing on how North/South traffic can benefit from their implementation.</p>
<h2 id="what-are-tep-groups-and-why-are-they-interesting">What Are TEP Groups and Why Are They Interesting?</h2>
<p>With NSX 4.2.1, TEP High Availability (HA) for Edge Transport Nodes was introduced. In addition to the HA feature, the load-sharing behavior was also modified.</p>
<p>Before NSX 4.2.1, each segment was bound to a single TEP interface. This limitation meant that North/South traffic could only utilize the maximum throughput of one physical adapter (ESXi where the Edge VM is realized). With TEP HA and the introduction of TEP Groups, this behavior has changed significantly.</p>
<p>It is worth noting that prior to TEP HA, a Multi-TEP implementation was already available. While this allowed for failover within the TEP network if a physical adapter lost its link, it did not address Layer 2 or Layer 3 issues. For more details on this topic, I recommend reading Steven’s article.</p>
<p>However, TEP HA is not enabled by default and, as of today, can only be activated via the API.</p>
<h2 id="lab-setup">LAB Setup</h2>
<p>For this exploration, I am running NSX 4.2.1 on three Intel NUC Pro devices, each equipped with dual 2.5 Gigabit LAN adapters. My test VMs are pinned to different hosts using DRS rules to ensure separation and accurate testing conditions.</p>
<p>Multi-TEP is configured in the setup, but TEP HA has not yet been enabled.</p>
<p>The test environment includes four Alpine Linux VMs, each connected to the same segment.</p>
<ul>
<li><strong>Alpine1</strong>: <code>10.10.20.10</code></li>
<li><strong>Alpine2</strong>: <code>10.10.20.20</code></li>
<li><strong>Alpine3</strong>: <code>10.10.20.30</code></li>
<li><strong>Alpine4</strong>: <code>10.10.20.40</code></li>
</ul>
<p>These VMs are distributed across two ESXi servers to simulate North/South traffic under real-world conditions. A third ESXi server hosts the <strong>NSX Edge VM</strong>, responsible for North/South traffic.</p>
<p>To evaluate performance, my iPerf target is located on a separate server with a <strong>10 Gb/s connection</strong>, ensuring that the network backbone does not introduce any bottlenecks. This setup provides a robust environment to test TEP HA and its impact on North/South traffic.</p>
<h2 id="baseline-tests-northsouth-capacity">Baseline Tests: North/South Capacity</h2>
<p>To measure the maximum North/South capacity of the setup, I ran iPerf tests simultaneously on all four Alpine VMs. Each VM generated traffic towards the iPerf target server with a 10 Gb/s connection. Below are the individual results:</p>
<ul>
<li><strong>Alpine1</strong>: 554 Mbps</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl">[ ID] Interval Transfer Bitrate Retr 
</span></span><span class="line"><span class="cl">[ 5] 0.00-100.00 sec 6.45 GBytes 554 Mbits/sec 727 sender 
</span></span><span class="line"><span class="cl">[ 5] 0.00-100.00 sec 6.45 GBytes 554 Mbits/sec receiver
</span></span></code></pre></div><ul>
<li><strong>Alpine2</strong>: 807 Mbps</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl">[ ID] Interval Transfer Bitrate Retr 
</span></span><span class="line"><span class="cl">[ 5] 0.00-100.00 sec 9.40 GBytes 807 Mbits/sec 1713 sender 
</span></span><span class="line"><span class="cl">[ 5] 0.00-100.00 sec 9.39 GBytes 807 Mbits/sec receiver
</span></span></code></pre></div><ul>
<li><strong>Alpine3</strong>: 465 Mbps</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl">[ ID] Interval Transfer Bitrate Retr 
</span></span><span class="line"><span class="cl">[ 5] 0.00-100.00 sec 5.42 GBytes 465 Mbits/sec 1196 sender 
</span></span><span class="line"><span class="cl">[ 5] 0.00-100.00 sec 5.41 GBytes 466 Mbits/sec receiver
</span></span></code></pre></div><ul>
<li><strong>Alpine4</strong>: 529 Mbps</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl">[ ID] Interval Transfer Bitrate Retr 
</span></span><span class="line"><span class="cl">[ 5] 0.00-100.00 sec 6.16 GBytes 529 Mbits/sec 1010 sender 
</span></span><span class="line"><span class="cl">[ 5] 0.00-100.01 sec 6.16 GBytes 529 Mbits/sec receiver
</span></span></code></pre></div><h3 id="total-throughput">Total Throughput</h3>
<p>The combined total throughput across all four VMs was <strong>2.355 Gbps</strong>, indicating the maximum North/South capacity under the current configuration.</p>
<h2 id="validating-physical-nic-utilization">Validating Physical NIC Utilization</h2>
<p>To monitor the utilization of the Edge VM, we can use <code>esxtop</code> on the ESXi server. By pressing <strong>&ldquo;N&rdquo;</strong>, we can examine the network statistics for the physical NICs (<code>vmnic0</code> and <code>vmnic1</code>) as well as the interfaces of the Edge VM.</p>
<p>My Edge VM is configured with four Fastpath interfaces:</p>
<ul>
<li><strong>fp0-fp1</strong>: Used for TEP traffic.</li>
<li><strong>fp2-fp3</strong>: Used for BGP uplinks.</li>
</ul>
<p>Additionally, <code>esxtop</code> displays the mapping of the Edge VM&rsquo;s interfaces to the respective physical NICs (<code>vmnic</code>). This allows us to verify how traffic is distributed across the available resources and ensures that both TEP and BGP traffic are leveraging the correct network paths.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl">  67108895 1054173:edge04.lab.home.eth3       vmnic0 DvsPortset-0 &lt;--(fp2)         
</span></span><span class="line"><span class="cl">  67108897 1054173:edge04.lab.home.eth1       vmnic0 DvsPortset-0 &lt;--(fp0)
</span></span><span class="line"><span class="cl">  67108898 1054173:edge04.lab.home.eth0       vmnic1 DvsPortset-0 &lt;--(MGMT)
</span></span><span class="line"><span class="cl">  67108899 1054173:edge04.lab.home.eth2       vmnic1 DvsPortset-0 &lt;--(fp1)
</span></span><span class="line"><span class="cl">  67108900 1054173:edge04.lab.home.eth4       vmnic1 DvsPortset-0 &lt;--(fp3)   
</span></span></code></pre></div><p>In addition to monitoring this in <code>esxtop</code>, I can observe the activity on my switches. By checking the switch port bandwith statistics, I can determine which physical adapter is actively handling the iPerf traffic and which one is idle. This provides an additional layer of validation for the distribution of traffic across the available pNICs.</p>
<h2 id="configuring-multi-tep-ha">Configuring Multi-TEP HA</h2>
<p>The process of enabling Multi-TEP High Availability (HA) is straightforward. It begins with creating a TEP HA Host Switch Profile. This is done through a simple API call using the <code>PUT</code> method to the following endpoint:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl">PUT https://&lt;nsx-policy-manager&gt;/policy/api/v1/infra/host-switch-profiles/nsxvtepha
</span></span></code></pre></div><h3 id="json-payload">JSON Payload</h3>
<p>The following JSON payload needs to be provided in the API request:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="nt">&#34;enabled&#34;</span><span class="p">:</span> <span class="s2">&#34;true&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="nt">&#34;failover_timeout&#34;</span><span class="p">:</span> <span class="s2">&#34;5&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="nt">&#34;auto_recovery&#34;</span><span class="p">:</span> <span class="s2">&#34;true&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="nt">&#34;auto_recovery_initial_wait&#34;</span><span class="p">:</span> <span class="s2">&#34;300&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="nt">&#34;auto_recovery_max_backoff&#34;</span><span class="p">:</span> <span class="s2">&#34;86400&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="nt">&#34;resource_type&#34;</span><span class="p">:</span> <span class="s2">&#34;PolicyVtepHAHostSwitchProfile&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="nt">&#34;display_name&#34;</span><span class="p">:</span> <span class="s2">&#34;nsxvtepha&#34;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>Key Parameters:</p>
<ul>
<li>enabled: Enables TEP HA functionality (true or false).</li>
<li>failover_timeout: Specifies the timeout (in seconds) for failover to occur.</li>
<li>auto_recovery: Enables automatic recovery of TEPs after a failure.</li>
<li>auto_recovery_initial_wait: Time (in seconds) before initiating the first recovery attempt.</li>
<li>auto_recovery_max_backoff: Maximum backoff time (in seconds) for recovery attempts.</li>
<li>display_name: A human-readable name for the profile.</li>
</ul>
<p>This API call creates the TEP HA Host Switch Profile, which can then be applied to the desired transport nodes to enable Multi-TEP HA functionality.</p>
<h2 id="assigning-the-tep-ha-profile">Assigning the TEP HA Profile</h2>
<p>To enable the Multi-TEP HA feature, the created TEP HA profile must be assigned to a <strong>Transport Node Profile</strong>. This assignment ensures that the specified hosts will have the Multi-TEP HA feature enabled.</p>
<p>Steps to Assign the TEP HA Profile:</p>
<ol>
<li><strong>Gather the Transport Node Profile ID</strong>:
Retrieve the ID of the transport node profile that you want to map the TEP HA profile to. Without this ID, you cannot complete the assignment.</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl">GET https://&lt;nsx-policy-manager&gt;/policy/api/v1/infra/host-transport-node-profiles/
</span></span></code></pre></div><ol start="2">
<li><strong>Assign the TEP HA Profile</strong>:
Use the API to update the transport node profile by linking it with the TEP HA profile. The request must specify the IDs of both the transport node profile and the TEP HA profile.</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl">PUT https://&lt;nsx-policy-manager&gt;/policy/api/v1/infra/host-transport-node-profiles/&lt;tnp-id&gt; 
</span></span></code></pre></div><p>Add the following entry to the transport node profile to link it with the TEP HA profile:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="nt">&#34;key&#34;</span><span class="p">:</span> <span class="s2">&#34;VtepHAHostSwitchProfile&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="nt">&#34;value&#34;</span><span class="p">:</span> <span class="s2">&#34;/infra/host-switch-profiles/nsxvtepha&#34;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>The full transport node profile looks like this:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="cl">  <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;host_switch_spec&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="nt">&#34;host_switches&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">        <span class="p">{</span>
</span></span><span class="line"><span class="cl">          <span class="nt">&#34;host_switch_name&#34;</span><span class="p">:</span> <span class="s2">&#34;NSX_vCompute3&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="nt">&#34;host_switch_id&#34;</span><span class="p">:</span> <span class="s2">&#34;50 27 cc 64 fe fc 4b 00-b1 af 91 5d 11 78 b9 06&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="nt">&#34;host_switch_type&#34;</span><span class="p">:</span> <span class="s2">&#34;VDS&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="nt">&#34;host_switch_mode&#34;</span><span class="p">:</span> <span class="s2">&#34;STANDARD&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="nt">&#34;ecmp_mode&#34;</span><span class="p">:</span> <span class="s2">&#34;L3&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="nt">&#34;host_switch_profile_ids&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">            <span class="p">{</span>
</span></span><span class="line"><span class="cl">              <span class="nt">&#34;key&#34;</span><span class="p">:</span> <span class="s2">&#34;UplinkHostSwitchProfile&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">              <span class="nt">&#34;value&#34;</span><span class="p">:</span> <span class="s2">&#34;/infra/host-switch-profiles/HostUplink&#34;</span>
</span></span><span class="line"><span class="cl">            <span class="p">},</span>
</span></span><span class="line"><span class="cl">            <span class="p">{</span>
</span></span><span class="line"><span class="cl">              <span class="nt">&#34;key&#34;</span><span class="p">:</span> <span class="s2">&#34;VtepHAHostSwitchProfile&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">              <span class="nt">&#34;value&#34;</span><span class="p">:</span> <span class="s2">&#34;/infra/host-switch-profiles/nsxvtepha&#34;</span>
</span></span><span class="line"><span class="cl">            <span class="p">}</span>
</span></span><span class="line"><span class="cl">          <span class="p">],</span>
</span></span><span class="line"><span class="cl">          <span class="nt">&#34;uplinks&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">            <span class="p">{</span>
</span></span><span class="line"><span class="cl">              <span class="nt">&#34;vds_uplink_name&#34;</span><span class="p">:</span> <span class="s2">&#34;Uplink 1&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">              <span class="nt">&#34;uplink_name&#34;</span><span class="p">:</span> <span class="s2">&#34;Uplink1&#34;</span>
</span></span><span class="line"><span class="cl">            <span class="p">},</span>
</span></span><span class="line"><span class="cl">            <span class="p">{</span>
</span></span><span class="line"><span class="cl">              <span class="nt">&#34;vds_uplink_name&#34;</span><span class="p">:</span> <span class="s2">&#34;Uplink 2&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">              <span class="nt">&#34;uplink_name&#34;</span><span class="p">:</span> <span class="s2">&#34;Uplink2&#34;</span>
</span></span><span class="line"><span class="cl">            <span class="p">}</span>
</span></span><span class="line"><span class="cl">          <span class="p">],</span>
</span></span><span class="line"><span class="cl">          <span class="nt">&#34;is_migrate_pnics&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="nt">&#34;ip_assignment_spec&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="nt">&#34;ip_pool_id&#34;</span><span class="p">:</span> <span class="s2">&#34;/infra/ip-pools/tep&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="nt">&#34;resource_type&#34;</span><span class="p">:</span> <span class="s2">&#34;StaticIpPoolSpec&#34;</span>
</span></span><span class="line"><span class="cl">          <span class="p">},</span>
</span></span><span class="line"><span class="cl">          <span class="nt">&#34;cpu_config&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">          <span class="p">],</span>
</span></span><span class="line"><span class="cl">          <span class="nt">&#34;transport_zone_endpoints&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">            <span class="p">{</span>
</span></span><span class="line"><span class="cl">              <span class="nt">&#34;transport_zone_id&#34;</span><span class="p">:</span> <span class="s2">&#34;/infra/sites/default/enforcement-points/default/transport-zones/OVERLAYTZ&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">              <span class="nt">&#34;transport_zone_profile_ids&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">              <span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="p">},</span>
</span></span><span class="line"><span class="cl">            <span class="p">{</span>
</span></span><span class="line"><span class="cl">              <span class="nt">&#34;transport_zone_id&#34;</span><span class="p">:</span> <span class="s2">&#34;/infra/sites/default/enforcement-points/default/transport-zones/MVLAN&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">              <span class="nt">&#34;transport_zone_profile_ids&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">              <span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="p">}</span>
</span></span><span class="line"><span class="cl">          <span class="p">],</span>
</span></span><span class="line"><span class="cl">          <span class="nt">&#34;not_ready&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="nt">&#34;portgroup_transport_zone_id&#34;</span><span class="p">:</span> <span class="s2">&#34;/infra/sites/default/enforcement-points/default/transport-zones/eb370bd3-db11-319c-98ec-585e402bf98c&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">      <span class="p">],</span>
</span></span><span class="line"><span class="cl">      <span class="nt">&#34;resource_type&#34;</span><span class="p">:</span> <span class="s2">&#34;StandardHostSwitchSpec&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="p">},</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;ignore_overridden_hosts&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;resource_type&#34;</span><span class="p">:</span> <span class="s2">&#34;PolicyHostTransportNodeProfile&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;id&#34;</span><span class="p">:</span> <span class="s2">&#34;45104efd-72bf-4d69-bc24-87d45b03b402&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;display_name&#34;</span><span class="p">:</span> <span class="s2">&#34;HostTNP&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;path&#34;</span><span class="p">:</span> <span class="s2">&#34;/infra/host-transport-node-profiles/45104efd-72bf-4d69-bc24-87d45b03b402&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;relative_path&#34;</span><span class="p">:</span> <span class="s2">&#34;45104efd-72bf-4d69-bc24-87d45b03b402&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;parent_path&#34;</span><span class="p">:</span> <span class="s2">&#34;/infra&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;remote_path&#34;</span><span class="p">:</span> <span class="s2">&#34;&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;unique_id&#34;</span><span class="p">:</span> <span class="s2">&#34;45104efd-72bf-4d69-bc24-87d45b03b402&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;realization_id&#34;</span><span class="p">:</span> <span class="s2">&#34;45104efd-72bf-4d69-bc24-87d45b03b402&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;owner_id&#34;</span><span class="p">:</span> <span class="s2">&#34;1ec3eeb1-8da7-457d-bebe-a8b2b47df7de&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;marked_for_delete&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;overridden&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;_system_owned&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;_protection&#34;</span><span class="p">:</span> <span class="s2">&#34;NOT_PROTECTED&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;_create_time&#34;</span><span class="p">:</span> <span class="mi">1723569644857</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;_create_user&#34;</span><span class="p">:</span> <span class="s2">&#34;admin&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;_last_modified_time&#34;</span><span class="p">:</span> <span class="mi">1732706346575</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;_last_modified_user&#34;</span><span class="p">:</span> <span class="s2">&#34;admin&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;_revision&#34;</span><span class="p">:</span> <span class="mi">3</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span></code></pre></div><h3 id="important-notes">Important Notes</h3>
<ul>
<li>Ensure the transport node profile is correctly assigned to the desired hosts to enable Multi-TEP HA.</li>
<li>Any misconfiguration or omission of the &ldquo;key&rdquo;: &ldquo;VtepHAHostSwitchProfile&rdquo; entry will result in the inability to activate the TEP HA functionality.</li>
<li>The value field must match the path of the created TEP HA profile.</li>
</ul>
<p>This process is crucial for leveraging the full capabilities of Multi-TEP HA in NSX environments.</p>
<h2 id="enabling-edge-tep-groups">Enabling Edge TEP Groups</h2>
<p>To enable the TEP Group feature on Edge nodes, the global connectivity configuration must be updated. This is achieved by modifying the <code>tep_group_config</code> parameter via an API call.</p>
<p>Use the following API request to enable the TEP Group feature on Edge nodes:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl">PUT https://&lt;NSX manager&gt;/policy/api/v1/infra/connectivity-global-config
</span></span></code></pre></div><p>JSON Payload</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="nt">&#34;tep_group_config&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="nt">&#34;enable_tep_grouping_on_edge&#34;</span><span class="p">:</span> <span class="kc">true</span>
</span></span><span class="line"><span class="cl">    <span class="p">},</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;resource_type&#34;</span><span class="p">:</span> <span class="s2">&#34;GlobalConfig&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span>
</span></span></code></pre></div><p>The full global config looks like this:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;fips&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="nt">&#34;lb_fips_enabled&#34;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">		<span class="nt">&#34;tls_fips_enabled&#34;</span><span class="p">:</span> <span class="kc">false</span>
</span></span><span class="line"><span class="cl">	<span class="p">},</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;l3_forwarding_mode&#34;</span><span class="p">:</span> <span class="s2">&#34;IPV4_ONLY&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;uplink_mtu_threshold&#34;</span><span class="p">:</span> <span class="mi">9000</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;vdr_mac&#34;</span><span class="p">:</span> <span class="s2">&#34;02:50:56:56:44:52&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;vdr_mac_nested&#34;</span><span class="p">:</span> <span class="s2">&#34;02:50:56:56:44:53&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;allow_changing_vdr_mac_in_use&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;arp_limit_per_gateway&#34;</span><span class="p">:</span> <span class="mi">50000</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;external_gateway_bfd&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="nt">&#34;bfd_profile_path&#34;</span><span class="p">:</span> <span class="s2">&#34;/infra/bfd-profiles/default-external-gw-bfd-profile&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">		<span class="nt">&#34;enable&#34;</span><span class="p">:</span> <span class="kc">true</span>
</span></span><span class="line"><span class="cl">	<span class="p">},</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;lb_ecmp&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;remote_tunnel_physical_mtu&#34;</span><span class="p">:</span> <span class="mi">1700</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;physical_uplink_mtu&#34;</span><span class="p">:</span> <span class="mi">9000</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;global_replication_mode_enabled&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;is_inherited&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;site_infos&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">	<span class="p">],</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;tep_group_config&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="nt">&#34;enable_tep_grouping_on_edge&#34;</span><span class="p">:</span> <span class="kc">true</span>
</span></span><span class="line"><span class="cl">	<span class="p">},</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;resource_type&#34;</span><span class="p">:</span> <span class="s2">&#34;GlobalConfig&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;id&#34;</span><span class="p">:</span> <span class="s2">&#34;global-config&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;display_name&#34;</span><span class="p">:</span> <span class="s2">&#34;default&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;path&#34;</span><span class="p">:</span> <span class="s2">&#34;/infra/global-config&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;relative_path&#34;</span><span class="p">:</span> <span class="s2">&#34;global-config&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;parent_path&#34;</span><span class="p">:</span> <span class="s2">&#34;/infra&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;remote_path&#34;</span><span class="p">:</span> <span class="s2">&#34;&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;unique_id&#34;</span><span class="p">:</span> <span class="s2">&#34;071c1408-8d73-42ea-b2ad-b85cc43c96b2&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;realization_id&#34;</span><span class="p">:</span> <span class="s2">&#34;071c1408-8d73-42ea-b2ad-b85cc43c96b2&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;owner_id&#34;</span><span class="p">:</span> <span class="s2">&#34;1ec3eeb1-8da7-457d-bebe-a8b2b47df7de&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;marked_for_delete&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;overridden&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;_system_owned&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;_protection&#34;</span><span class="p">:</span> <span class="s2">&#34;NOT_PROTECTED&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;_create_time&#34;</span><span class="p">:</span> <span class="mi">1723479213559</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;_create_user&#34;</span><span class="p">:</span> <span class="s2">&#34;system&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;_last_modified_time&#34;</span><span class="p">:</span> <span class="mi">1735854980053</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;_last_modified_user&#34;</span><span class="p">:</span> <span class="s2">&#34;admin&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;_revision&#34;</span><span class="p">:</span> <span class="mi">5</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><h2 id="verifying-changes-checking-edge-node-tep-groups">Verifying Changes: Checking Edge Node TEP Groups</h2>
<p>To ensure that the changes to enable TEP Groups are effective, you can verify the configuration directly on an Edge Node using SSH. The following command provides an overview of logical switches and their associated TEP Groups:</p>
<h3 id="command">Command</h3>
<p>Log in to the Edge Node via SSH and execute:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">get logical-switches
</span></span></code></pre></div><p>Sample Output:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl">UUID                                   VNI          ENCAP    TEP_GROUP    NAME                                               GLOBAL_VNI(FED)
</span></span><span class="line"><span class="cl">7f7e8af0-299e-4354-a143-a6a3689db228   74753        GENEVE   293888       transit-rl-aa5420e0-3d2b-4ff7-b00e-5f234c2f7413                
</span></span><span class="line"><span class="cl">0abeab93-66ef-4b41-87b6-64164b450e8d   67587        GENEVE   293888       transit-bp-T1                                                  
</span></span><span class="line"><span class="cl">09243099-ebb7-41ae-bcf4-10e0b833cc24   68609        GENEVE   293888       inter-sr-routing-bp-T0-ECMP                                    
</span></span><span class="line"><span class="cl">6261cda0-558f-4a57-838c-d47c95945c31   71680        GENEVE   293888       T1-dhcp-ls
</span></span></code></pre></div><p>Key Parameters to Verify:</p>
<ul>
<li>TEP_GROUP: The column should display a valid TEP Group ID (e.g., 293888) for all logical switches.</li>
<li>Logical Switch Details: Ensure that all expected logical switches are listed, along with their VNI and encapsulation type (e.g., GENEVE).</li>
</ul>
<p>If the TEP_GROUP column shows values for the logical switches, it confirms that the TEP Group feature is active and functioning as expected. This verification ensures that your configuration changes are effective across the Edge Nodes.</p>
<h2 id="performance-tests-with-tep-groups">Performance Tests with TEP Groups</h2>
<p>To evaluate the impact of TEP Groups on performance, I ran simultaneous iPerf tests on all four Alpine VMs. Below are the individual results:</p>
<ul>
<li><strong>Alpine1</strong>: 1.14 Gbits/sec</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl">[ ID] Interval Transfer Bitrate Retr 
</span></span><span class="line"><span class="cl">[ 5] 0.00-100.00 sec 13.3 GBytes 1.14 Gbits/sec 669 sender 
</span></span><span class="line"><span class="cl">[ 5] 0.00-100.00 sec 13.3 GBytes 1.14 Gbits/sec receiver
</span></span></code></pre></div><ul>
<li><strong>Alpine2</strong>: 1.08 Gbits/sec</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl">[ ID] Interval Transfer Bitrate Retr 
</span></span><span class="line"><span class="cl">[ 5] 0.00-100.00 sec 12.6 GBytes 1.08 Gbits/sec 774 sender 
</span></span><span class="line"><span class="cl">[ 5] 0.00-100.00 sec 12.6 GBytes 1.08 Gbits/sec receiver
</span></span></code></pre></div><ul>
<li><strong>Alpine3</strong>: 1.10 Gbits/sec</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl">[ ID] Interval Transfer Bitrate Retr 
</span></span><span class="line"><span class="cl">[ 5] 0.00-100.00 sec 12.8 GBytes 1.10 Gbits/sec 1002 sender 
</span></span><span class="line"><span class="cl">[ 5] 0.00-100.00 sec 12.8 GBytes 1.10 Gbits/sec receiver
</span></span></code></pre></div><ul>
<li><strong>Alpine4</strong>: 1.11 Gbits/sec</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl">[ ID] Interval Transfer Bitrate Retr 
</span></span><span class="line"><span class="cl">[ 5] 0.00-100.00 sec 12.9 GBytes 1.11 Gbits/sec 990 sender 
</span></span><span class="line"><span class="cl">[ 5] 0.00-100.00 sec 12.9 GBytes 1.11 Gbits/sec receiver
</span></span></code></pre></div><h3 id="total-throughput-1">Total Throughput</h3>
<p>The combined total throughput across all four VMs was <strong>4.43 Gbps</strong>, showing a significant improvement over the baseline tests without TEP Groups. This demonstrates the enhanced traffic distribution and performance benefits enabled by the Multi-TEP HA feature.</p>
<h2 id="cross-verification-on-the-switch">Cross-Verification on the Switch</h2>
<p>To further validate the results, I checked the physical interfaces of the ESXi server hosting the Edge VM directly on the switch. The switch statistics confirm that both physical interfaces are actively utilized during the iPerf tests.</p>
<h3 id="observations">Observations</h3>
<ul>
<li>Both physical interfaces (<code>vmnic0</code> and <code>vmnic1</code>) show significant traffic, indicating effective utilization and load balancing.</li>
<li>This behavior aligns with the expected performance of the TEP Groups feature, ensuring that traffic is distributed across multiple interfaces for maximum throughput.</li>
</ul>

<figure><picture>
          <source srcset="/nsx-tep-groups/switch_hu15845385026729381105.webp" type="image/webp">
          <source srcset="/nsx-tep-groups/switch_hu11297771637104314496.jpg" type="image/jpeg">
          <img src="/nsx-tep-groups/switch_hu15845385026729381105.webp"alt="Switch Port View"  width="1958"  height="184" />
        </picture><figcaption>
            <p>Switch port view</p>
          </figcaption></figure>
<p>The screenshot demonstrates how the Multi-TEP HA configuration efficiently balances the load across both physical NICs, validating the setup and confirming the improvements in traffic handling.</p>
<h2 id="final-thoughts">Final Thoughts</h2>
<p>TEP Groups can be easily integrated into any environment with a Multi-TEP setup without requiring significant modifications. The adjustments are minimal and pose a low risk to production environments.</p>
<p>In addition to the noticeable performance improvements, TEP Groups also provide significantly better High Availability (HA) handling. The performance gains are particularly impactful in environments with fewer NSX segments, where the previous load distribution method was less effective.</p>
<p>Moreover, TEP Groups can deliver higher performance for segments with high traffic loads, especially those previously constrained by the physical uplink&rsquo;s capacity. This makes TEP Groups a valuable enhancement for optimizing both performance and reliability in NSX deployments.</p>
<h2 id="further-resources">Further Resources</h2>
<p>For more details and in-depth explanations about Multi-TEP High Availability and TEP Groups, refer to the following resources:</p>
<ul>
<li><a href="https://sdn-techtalk.com/posts/multitep-ha/">Improving NSX Datacenter TEP Performance and Availability - Multi-TEP and TEP Group High Availability</a></li>
<li><a href="https://techdocs.broadcom.com/us/en/vmware-cis/nsx/vmware-nsx/4-2/administration-guide/host-switches/multi-tep-high-availability.html">VMware NSX Administration Guide: Multi-TEP High Availability</a></li>
</ul>
]]></content>
		</item>
		
		<item>
			<title>Redeploy an NSX Edge VM Appliance</title>
			<link>https://sdn-warrior.org/posts/nsx-edge-redeploy/</link>
			<pubDate>Mon, 30 Dec 2024 15:00:46 +0100</pubDate>
			
			<guid>https://sdn-warrior.org/posts/nsx-edge-redeploy/</guid>
			<description><![CDATA[Quicktip: Redeploy NSX Edge with API]]></description>
			<content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>There are situations where you might need to redeploy an NSX Edge node. This could happen if an Edge VM becomes non-functional, or if it needs to be relocated within the datacenter—for instance, to a different datastore or compute resource. You might also redeploy to move the node to another network. Of course, the specific reasons for redeployment depend on your enviroment.</p>
<p>It’s important to note that redeployment applies exclusively to existing NSX Edge nodes and can only be performed with an NSX Edge VM appliance.</p>
<h2 id="prerequisites">Prerequisites</h2>
<p>Before redeploying an NSX Edge node, keep the following in mind:</p>
<ul>
<li>
<p>While certain configurations of the NSX Edge transport node payload can be changed, <strong>do not modify</strong> the following settings on the existing NSX Edge node being replaced:</p>
<ul>
<li>Failure domain</li>
<li>Transport node connectivity</li>
<li>Physical NIC configuration</li>
<li>Logical routers</li>
<li>Load balancer allocations</li>
</ul>
</li>
<li>
<p>If the existing NSX Edge node is a physical server or was manually deployed via the vSphere Client, ensure that its connectivity to NSX Manager is down. If connectivity remains active, NSX will prevent the replacement of the existing node with a new one.</p>
</li>
<li>
<p><strong>Autodeployed NSX Edge nodes</strong> will retain hardware version 13. However, starting with NSX 4.0.1.1, redeploying an NSX Edge VM automatically upgrades the new VM to a hardware version compatible with the ESXi host version. For a list of compatible VM hardware versions, refer to VMware KB article <a href="https://kb.vmware.com/s/article/2007240">2007240</a>.</p>
</li>
</ul>
<h2 id="procedure">Procedure</h2>
<p>To redeploy an NSX Edge node, follow these steps:</p>
<ol>
<li>
<p><strong>Check the NSX Edge node</strong></p>
<ul>
<li>Open an SSH session and connect to the NSX Edge console.</li>
<li>Verify the logical routers configured on the NSX Edge node by running the following command in the CLI console:
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">get logical-routers
</span></span></code></pre></div></li>
<li>Power off the NSX Edge node.</li>
</ul>
</li>
<li>
<p><strong>Verify disconnection from NSX Manager:</strong></p>
<ul>
<li>
<p>Use the API to confirm the NSX Edge node is disconnected:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl">GET /api/v1/transport-nodes/&lt;edgenode&gt;/state
</span></span></code></pre></div><p>The <code>node_deployment_state</code> should display:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl">&#34;node_deployment_state&#34;: {
</span></span><span class="line"><span class="cl">    &#34;state&#34;: &#34;MPA_Disconnected&#34;
</span></span><span class="line"><span class="cl">}
</span></span></code></pre></div><p>A state of <code>MPA_Disconnected</code> indicates that you can proceed with redeployment.</p>
</li>
<li>
<p><strong>Important:</strong> If the <code>node_deployment_state</code> is <code>Node Ready</code>, NSX Manager will block the redeployment and display error <strong>78006</strong>: <em>Manager connectivity to Edge node must be down</em>.</p>
</li>
<li>
<p>Alternatively, check the connectivity state from the Edge Transport Node page in the NSX UI. A disconnected NSX Edge node will show the system message:</p>
<pre tabindex="0"><code>Configuration Error: Edge VM MPA Connectivity is down
</code></pre></li>
</ul>
</li>
<li>
<p><strong>For an auto-deployed NSX Edge node:</strong></p>
<ul>
<li>Use the following API command to retrieve the payload of the transport node:
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl">GET /&lt;NSX-Manager-IPaddress&gt;/api/v1/transport-nodes/&lt;edgenode&gt;
</span></span></code></pre></div></li>
<li>Save the output payload for later use. (Output example)</li>
</ul>
</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="cl">    <span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;node_id&#34;</span><span class="p">:</span> <span class="s2">&#34;607064c6-dd8d-4576-a2d9-2a73abff38aa&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;host_switch_spec&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="nt">&#34;host_switches&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">			<span class="p">{</span>
</span></span><span class="line"><span class="cl">				<span class="nt">&#34;host_switch_name&#34;</span><span class="p">:</span> <span class="s2">&#34;nsxHostSwitch&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">				<span class="nt">&#34;host_switch_id&#34;</span><span class="p">:</span> <span class="s2">&#34;c8e3cfaf-9837-4ee2-8a6f-9055927e6009&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">				<span class="nt">&#34;host_switch_type&#34;</span><span class="p">:</span> <span class="s2">&#34;NVDS&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">				<span class="nt">&#34;host_switch_mode&#34;</span><span class="p">:</span> <span class="s2">&#34;STANDARD&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">				<span class="nt">&#34;ecmp_mode&#34;</span><span class="p">:</span> <span class="s2">&#34;L3&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">				<span class="nt">&#34;host_switch_profile_ids&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">					<span class="p">{</span>
</span></span><span class="line"><span class="cl">						<span class="nt">&#34;key&#34;</span><span class="p">:</span> <span class="s2">&#34;UplinkHostSwitchProfile&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">						<span class="nt">&#34;value&#34;</span><span class="p">:</span> <span class="s2">&#34;1c653cda-9c95-414f-9b97-3d8f7cb192d6&#34;</span>
</span></span><span class="line"><span class="cl">					<span class="p">}</span>
</span></span><span class="line"><span class="cl">				<span class="p">],</span>
</span></span><span class="line"><span class="cl">				<span class="nt">&#34;pnics&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">					<span class="p">{</span>
</span></span><span class="line"><span class="cl">						<span class="nt">&#34;device_name&#34;</span><span class="p">:</span> <span class="s2">&#34;fp-eth0&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">						<span class="nt">&#34;uplink_name&#34;</span><span class="p">:</span> <span class="s2">&#34;Uplink1&#34;</span>
</span></span><span class="line"><span class="cl">					<span class="p">},</span>
</span></span><span class="line"><span class="cl">					<span class="p">{</span>
</span></span><span class="line"><span class="cl">						<span class="nt">&#34;device_name&#34;</span><span class="p">:</span> <span class="s2">&#34;fp-eth1&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">						<span class="nt">&#34;uplink_name&#34;</span><span class="p">:</span> <span class="s2">&#34;Uplink2&#34;</span>
</span></span><span class="line"><span class="cl">					<span class="p">}</span>
</span></span><span class="line"><span class="cl">				<span class="p">],</span>
</span></span><span class="line"><span class="cl">				<span class="nt">&#34;is_migrate_pnics&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">				<span class="nt">&#34;ip_assignment_spec&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">					<span class="nt">&#34;ip_pool_id&#34;</span><span class="p">:</span> <span class="s2">&#34;00743a1f-a1a8-46b8-96a7-e0ebe58d7feb&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">					<span class="nt">&#34;resource_type&#34;</span><span class="p">:</span> <span class="s2">&#34;StaticIpPoolSpec&#34;</span>
</span></span><span class="line"><span class="cl">				<span class="p">},</span>
</span></span><span class="line"><span class="cl">				<span class="nt">&#34;cpu_config&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">				<span class="p">],</span>
</span></span><span class="line"><span class="cl">				<span class="nt">&#34;transport_zone_endpoints&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">					<span class="p">{</span>
</span></span><span class="line"><span class="cl">						<span class="nt">&#34;transport_zone_id&#34;</span><span class="p">:</span> <span class="s2">&#34;1b3a2f36-bfd1-443e-a0f6-4de01abc963e&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">						<span class="nt">&#34;transport_zone_profile_ids&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">							<span class="p">{</span>
</span></span><span class="line"><span class="cl">								<span class="nt">&#34;resource_type&#34;</span><span class="p">:</span> <span class="s2">&#34;BfdHealthMonitoringProfile&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">								<span class="nt">&#34;profile_id&#34;</span><span class="p">:</span> <span class="s2">&#34;52035bb3-ab02-4a08-9884-18631312e50a&#34;</span>
</span></span><span class="line"><span class="cl">							<span class="p">}</span>
</span></span><span class="line"><span class="cl">						<span class="p">]</span>
</span></span><span class="line"><span class="cl">					<span class="p">},</span>
</span></span><span class="line"><span class="cl">					<span class="p">{</span>
</span></span><span class="line"><span class="cl">						<span class="nt">&#34;transport_zone_id&#34;</span><span class="p">:</span> <span class="s2">&#34;a95c914d-748d-497c-94ab-10d4647daeba&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">						<span class="nt">&#34;transport_zone_profile_ids&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">							<span class="p">{</span>
</span></span><span class="line"><span class="cl">								<span class="nt">&#34;resource_type&#34;</span><span class="p">:</span> <span class="s2">&#34;BfdHealthMonitoringProfile&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">								<span class="nt">&#34;profile_id&#34;</span><span class="p">:</span> <span class="s2">&#34;52035bb3-ab02-4a08-9884-18631312e50a&#34;</span>
</span></span><span class="line"><span class="cl">							<span class="p">}</span>
</span></span><span class="line"><span class="cl">						<span class="p">]</span>
</span></span><span class="line"><span class="cl">					<span class="p">}</span>
</span></span><span class="line"><span class="cl">				<span class="p">],</span>
</span></span><span class="line"><span class="cl">				<span class="nt">&#34;pnics_uninstall_migration&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">				<span class="p">],</span>
</span></span><span class="line"><span class="cl">				<span class="nt">&#34;vmk_uninstall_migration&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">				<span class="p">],</span>
</span></span><span class="line"><span class="cl">				<span class="nt">&#34;not_ready&#34;</span><span class="p">:</span> <span class="kc">false</span>
</span></span><span class="line"><span class="cl">			<span class="p">}</span>
</span></span><span class="line"><span class="cl">		<span class="p">],</span>
</span></span><span class="line"><span class="cl">		<span class="nt">&#34;resource_type&#34;</span><span class="p">:</span> <span class="s2">&#34;StandardHostSwitchSpec&#34;</span>
</span></span><span class="line"><span class="cl">	<span class="p">},</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;maintenance_mode&#34;</span><span class="p">:</span> <span class="s2">&#34;DISABLED&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;node_deployment_info&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="nt">&#34;deployment_type&#34;</span><span class="p">:</span> <span class="s2">&#34;VIRTUAL_MACHINE&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">		<span class="nt">&#34;deployment_config&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="nt">&#34;vm_deployment_config&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">				<span class="nt">&#34;vc_id&#34;</span><span class="p">:</span> <span class="s2">&#34;0adeeac2-42dc-4d5a-a4c4-1890b1174a4e&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">				<span class="nt">&#34;compute_id&#34;</span><span class="p">:</span> <span class="s2">&#34;domain-c18&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">				<span class="nt">&#34;storage_id&#34;</span><span class="p">:</span> <span class="s2">&#34;datastore-30&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">				<span class="nt">&#34;management_network_id&#34;</span><span class="p">:</span> <span class="s2">&#34;dvportgroup-29&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">				<span class="nt">&#34;ipv4_assignment_enabled&#34;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">				<span class="nt">&#34;ipv4_assignment_disabled&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">				<span class="nt">&#34;management_port_subnets&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">					<span class="p">{</span>
</span></span><span class="line"><span class="cl">						<span class="nt">&#34;ip_addresses&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">							<span class="s2">&#34;192.168.12.13&#34;</span>
</span></span><span class="line"><span class="cl">						<span class="p">],</span>
</span></span><span class="line"><span class="cl">						<span class="nt">&#34;prefix_length&#34;</span><span class="p">:</span> <span class="mi">24</span>
</span></span><span class="line"><span class="cl">					<span class="p">}</span>
</span></span><span class="line"><span class="cl">				<span class="p">],</span>
</span></span><span class="line"><span class="cl">				<span class="nt">&#34;default_gateway_addresses&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">					<span class="s2">&#34;192.168.12.1&#34;</span>
</span></span><span class="line"><span class="cl">				<span class="p">],</span>
</span></span><span class="line"><span class="cl">				<span class="nt">&#34;data_network_ids&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">					<span class="s2">&#34;dvportgroup-1001&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">					<span class="s2">&#34;dvportgroup-1002&#34;</span>
</span></span><span class="line"><span class="cl">				<span class="p">],</span>
</span></span><span class="line"><span class="cl">				<span class="nt">&#34;reservation_info&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">					<span class="nt">&#34;memory_reservation&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">						<span class="nt">&#34;reservation_percentage&#34;</span><span class="p">:</span> <span class="mi">100</span>
</span></span><span class="line"><span class="cl">					<span class="p">},</span>
</span></span><span class="line"><span class="cl">					<span class="nt">&#34;cpu_reservation&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">						<span class="nt">&#34;reservation_in_shares&#34;</span><span class="p">:</span> <span class="s2">&#34;HIGH_PRIORITY&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">						<span class="nt">&#34;reservation_in_mhz&#34;</span><span class="p">:</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">					<span class="p">}</span>
</span></span><span class="line"><span class="cl">				<span class="p">},</span>
</span></span><span class="line"><span class="cl">				<span class="nt">&#34;resource_allocation&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">					<span class="nt">&#34;cpu_count&#34;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">					<span class="nt">&#34;memory_allocation_in_mb&#34;</span><span class="p">:</span> <span class="mi">8192</span>
</span></span><span class="line"><span class="cl">				<span class="p">},</span>
</span></span><span class="line"><span class="cl">				<span class="nt">&#34;placement_type&#34;</span><span class="p">:</span> <span class="s2">&#34;VsphereDeploymentConfig&#34;</span>
</span></span><span class="line"><span class="cl">			<span class="p">},</span>
</span></span><span class="line"><span class="cl">			<span class="nt">&#34;form_factor&#34;</span><span class="p">:</span> <span class="s2">&#34;MEDIUM&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">			<span class="nt">&#34;node_user_settings&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">				<span class="nt">&#34;cli_username&#34;</span><span class="p">:</span> <span class="s2">&#34;admin&#34;</span>
</span></span><span class="line"><span class="cl">			<span class="p">}</span>
</span></span><span class="line"><span class="cl">		<span class="p">},</span>
</span></span><span class="line"><span class="cl">		<span class="nt">&#34;node_settings&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="nt">&#34;hostname&#34;</span><span class="p">:</span> <span class="s2">&#34;edge01-nsx.lab.home&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">			<span class="nt">&#34;search_domains&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">				<span class="s2">&#34;lab.home&#34;</span>
</span></span><span class="line"><span class="cl">			<span class="p">],</span>
</span></span><span class="line"><span class="cl">			<span class="nt">&#34;ntp_servers&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">				<span class="s2">&#34;192.168.12.1&#34;</span>
</span></span><span class="line"><span class="cl">			<span class="p">],</span>
</span></span><span class="line"><span class="cl">			<span class="nt">&#34;dns_servers&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">				<span class="s2">&#34;192.168.11.2&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">				<span class="s2">&#34;192.168.100.254&#34;</span>
</span></span><span class="line"><span class="cl">			<span class="p">],</span>
</span></span><span class="line"><span class="cl">			<span class="nt">&#34;enable_ssh&#34;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">			<span class="nt">&#34;allow_ssh_root_login&#34;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">			<span class="nt">&#34;enable_upt_mode&#34;</span><span class="p">:</span> <span class="kc">false</span>
</span></span><span class="line"><span class="cl">		<span class="p">},</span>
</span></span><span class="line"><span class="cl">		<span class="nt">&#34;resource_type&#34;</span><span class="p">:</span> <span class="s2">&#34;EdgeNode&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">		<span class="nt">&#34;external_id&#34;</span><span class="p">:</span> <span class="s2">&#34;607064c6-dd8d-4576-a2d9-2a73abff38aa&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">		<span class="nt">&#34;ip_addresses&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">			<span class="s2">&#34;192.168.12.13&#34;</span>
</span></span><span class="line"><span class="cl">		<span class="p">],</span>
</span></span><span class="line"><span class="cl">		<span class="nt">&#34;id&#34;</span><span class="p">:</span> <span class="s2">&#34;607064c6-dd8d-4576-a2d9-2a73abff38aa&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">		<span class="nt">&#34;display_name&#34;</span><span class="p">:</span> <span class="s2">&#34;edge01-nsx.lab.home&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">		<span class="nt">&#34;tags&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">		<span class="p">],</span>
</span></span><span class="line"><span class="cl">		<span class="nt">&#34;_revision&#34;</span><span class="p">:</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">	<span class="p">},</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;is_overridden&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;failure_domain_id&#34;</span><span class="p">:</span> <span class="s2">&#34;4fc1e3b0-1cd4-4339-86c8-f76baddbaafb&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;resource_type&#34;</span><span class="p">:</span> <span class="s2">&#34;TransportNode&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;id&#34;</span><span class="p">:</span> <span class="s2">&#34;607064c6-dd8d-4576-a2d9-2a73abff38aa&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;display_name&#34;</span><span class="p">:</span> <span class="s2">&#34;edge01-nsx.lab.home&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;tags&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">	<span class="p">],</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;_system_owned&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;_protection&#34;</span><span class="p">:</span> <span class="s2">&#34;NOT_PROTECTED&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;_create_time&#34;</span><span class="p">:</span> <span class="mi">1735544264171</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;_create_user&#34;</span><span class="p">:</span> <span class="s2">&#34;admin&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;_last_modified_time&#34;</span><span class="p">:</span> <span class="mi">1735546418835</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;_last_modified_user&#34;</span><span class="p">:</span> <span class="s2">&#34;admin&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">	<span class="nt">&#34;_revision&#34;</span><span class="p">:</span> <span class="mi">2</span> 
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span></code></pre></div><ol start="4">
<li><strong>Run the redeploy API command:</strong></li>
</ol>
<p>Prepare the payload:</p>
<p>Paste the payload retrieved earlier in the body of the redeploy API.
Verify the deployment_config section contains details about the target:
Compute manager
Datastore
Network
Ensure these values align with those defined in the node_settings section.</p>
<p>NSX Manager will use the information in the deployment_config section to redeploy the NSX Edge node to the specified location and resources.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl">   POST /api/v1/transport-nodes/&lt;transport-node-id&gt;?action=redeploy
</span></span></code></pre></div><h2 id="verifying-redeployment">Verifying Redeployment</h2>
<p>After successfully executing the POST command, the <code>revision_id</code> will increment by one. This indicates that the command was successfully sent to the NSX Manager.</p>
<ol>
<li>
<p><strong>Confirm the redeployment status in vCenter:</strong></p>
<ul>
<li>Log in to vCenter and navigate to the relevant cluster or host where the NSX Edge node is being redeployed.</li>
<li>Check the task and events log for deployment activity related to the NSX Edge VM.</li>
</ul>
</li>
<li>
<p><strong>Validate the NSX Edge node redeployment:</strong></p>
<ul>
<li>Verify that the NSX Edge VM is being newly deployed in the specified location, as defined in the <code>deployment_config</code> section of the API payload.</li>
</ul>
</li>
</ol>
<p>Once the redeployment process is complete, ensure the NSX Edge node is functioning correctly in your NSX environment.</p>

    <aside class="admonition info">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-info">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="16" x2="12" y2="12"></line>
      <line x1="12" y1="8" x2="12.01" y2="8"></line>
   </svg></div><b>Info</b>
        </div>
        <div class="admonition-content">After the NSX Manager successfully deploys the new NSX Edge node, it <strong>power on the Edge VM automatically</strong>.</div>
    </aside>
]]></content>
		</item>
		
		<item>
			<title>Ansible VLAN deployment with MikroTik</title>
			<link>https://sdn-warrior.org/posts/vlan-automation/</link>
			<pubDate>Tue, 24 Dec 2024 13:00:00 +0100</pubDate>
			
			<guid>https://sdn-warrior.org/posts/vlan-automation/</guid>
			<description><![CDATA[Ansible VLAN deployment with MikroTik]]></description>
			<content type="html"><![CDATA[<h2 id="yet-another-ansible-post">Yet Another Ansible Post</h2>
<p>As the year comes to a close, I can&rsquo;t help but reflect on the progress I&rsquo;ve made with Ansible in the past few weeks. Looking back, there&rsquo;s a lot to be satisfied with. After automating my IPAM system and the startup/shutdown process for my lab, I decided to tackle a long-standing annoyance: deploying VLANs for new lab environments.</p>
<h2 id="goals">Goals</h2>
<p>The main goal is to have a single file that describes all the required VLANs. This file should also allow me to delete or reuse VLANs as needed. Each VLAN must be deployed across three switches and configured as a tagged VLAN on specific ports.</p>
<p>Additionally, there are the peculiarities of MikroTik hardware to consider. When creating new VLANs on my Top-of-Rack (ToR) switch, MikroTik recommends disabling the L3 Hardware Offloading feature beforehand. In the past, I’ve encountered strange issues when this wasn’t done prior to creating new networks. Therefore, the automation should also handle disabling and re-enabling this feature as part of the process.</p>
<h2 id="future-goals">Future Goals</h2>
<p>In the future, I plan to expand this setup further, integrating it with Ansible Tower or ArgoCD to enable management through pipelines.</p>
<p>A pipeline is essentially an automated workflow that takes a defined input—such as a configuration file or a code repository—and processes it through several steps to achieve a desired outcome. For example, in this context, a pipeline could validate my VLAN configuration, deploy it to the target switches, and handle any post-deployment tasks automatically.</p>
<p>My locally hosted Gitea instance will serve as the <strong>Source of Truth</strong>, housing the configuration files and acting as the central repository for all changes. This ensures consistency, version control, and a clear audit trail for every modification.</p>
<h2 id="what-ive-done">What I&rsquo;ve Done</h2>
<p>For this implementation, I approached things a bit differently compared to my last two Ansible projects—after all, learning is part of the process!</p>
<h3 id="project-structure">Project Structure</h3>
<p>I structured the project as follows:</p>
<ul>
<li><strong>INI File</strong>: Stores the credentials for the three switches.</li>
<li><strong>ansible.cfg</strong>: Manages the inventory file and SSH settings.</li>
<li><strong>Playbook</strong>: A YAML file that runs without additional parameters.</li>
<li><strong>Directories</strong>:
<ul>
<li><code>group_vars/</code>: Contains global variables shared across all devices.
<ul>
<li><code>all.yml</code>: Includes all VLANs that should exist, along with descriptions.</li>
</ul>
</li>
<li><code>host_vars/</code>: Contains per-device variables.
<ul>
<li>Each switch has its own YAML file defining interfaces and bridges.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Here’s the project structure visualized:</p>
<pre tabindex="0"><code>mikrotik/
├── ansible.cfg          # Configuration file for project Ansible settings
├── inventory.ini        # Inventory file with switch credentials
├── vlan_esx.yml         # Main playbook
├── group_vars/
│   └── all.yml          # Global VLAN definitions and descriptions
└── host_vars/
    ├── 192.168.0.1.yml      # Variables for Switch 1 (interfaces, bridges)
    ├── 192.168.0.5.yml      # Variables for Switch 2 (interfaces, bridges)
    └── 192.168.0.7.yml      # Variables for Switch 3 (interfaces, bridges)
</code></pre><h3 id="advantages-of-this-structure">Advantages of This Structure</h3>
<p>One major advantage of this structure is its scalability. Adding another MikroTik switch is straightforward: I only need to create a new <code>host_var</code> file for the switch and update the <code>inventory.ini</code> file with its credentials.</p>
<p>Additionally, since the playbook runs without requiring extra parameters, it simplifies the GitOps approach I plan to implement later. This means the entire process becomes more streamlined and easily automatable through pipelines, reducing complexity and potential for errors.</p>
<h2 id="project-files">Project Files</h2>
<p>Here are the files I created as part of this project:</p>
<h3 id="ansiblecfg">ansible.cfg</h3>
<p>This file configures Ansible with the necessary inventory and SSH settings.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-ini" data-lang="ini"><span class="line"><span class="cl"><span class="k">[defaults]</span>
</span></span><span class="line"><span class="cl"><span class="na">host_key_checking</span> <span class="o">=</span> <span class="s">False</span>
</span></span><span class="line"><span class="cl"><span class="na">transport</span> <span class="o">=</span> <span class="s">ssh</span>
</span></span><span class="line"><span class="cl"><span class="na">inventory</span> <span class="o">=</span> <span class="s">mikrotik.ini</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">[ssh_connection]</span>
</span></span><span class="line"><span class="cl"><span class="na">ssh_type</span> <span class="o">=</span> <span class="s">libssh</span>
</span></span><span class="line"><span class="cl"><span class="na">timeout</span> <span class="o">=</span> <span class="s">60</span>
</span></span></code></pre></div><h3 id="inventoryini">inventory.ini</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-ini" data-lang="ini"><span class="line"><span class="cl"><span class="k">[mikrotik]</span>
</span></span><span class="line"><span class="cl"><span class="na">192.168.0.1 ansible_user</span><span class="o">=</span><span class="s">admin ansible_password=&#34;xxx&#34; ansible_connection=network_cli ansible_network_os=community.network.routeros</span>
</span></span><span class="line"><span class="cl"><span class="na">192.168.0.5 ansible_user</span><span class="o">=</span><span class="s">admin ansible_password=&#34;xxx&#34; ansible_connection=network_cli ansible_network_os=community.network.routeros </span>
</span></span><span class="line"><span class="cl"><span class="na">192.168.0.7 ansible_user</span><span class="o">=</span><span class="s">admin ansible_password=&#34;xxx&#34; ansible_connection=network_cli ansible_network_os=community.network.routeros</span>
</span></span></code></pre></div><h3 id="vlan_esxyml">vlan_esx.yml</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nn">---</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Manage VLANs with delete option</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">hosts</span><span class="p">:</span><span class="w"> </span><span class="l">mikrotik</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">gather_facts</span><span class="p">:</span><span class="w"> </span><span class="kc">no</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">collections</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="l">community.network</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">tasks</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Disable L3 HW Offloading</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">community.network.routeros_command</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">commands</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span>- <span class="s2">&#34;/interface ethernet switch set [find name=switch1] l3-hw-offloading=no&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">when</span><span class="p">:</span><span class="w"> </span><span class="l">ansible_host == &#39;192.168.0.1&#39;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">register</span><span class="p">:</span><span class="w"> </span><span class="l">hw_offload_result</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Delete VLANs marked for deletion</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">community.network.routeros_command</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">commands</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span>- <span class="s2">&#34;/interface bridge vlan remove [find vlan-ids={{ item.id }}]&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">with_items</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;{{ vlans }}&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">when</span><span class="p">:</span><span class="w"> </span><span class="l">item.delete | bool</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Configure VLANs on the bridge</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">community.network.routeros_command</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">commands</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span>- <span class="s2">&#34;/interface bridge vlan remove [find vlan-ids={{ item.id }}]&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span>- <span class="s2">&#34;/interface bridge vlan add bridge={{ bridge }} vlan-ids={{ item.id }} tagged={{ interfaces | join(&#39;,&#39;) }}&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">with_items</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;{{ vlans }}&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">when</span><span class="p">:</span><span class="w"> </span><span class="l">not item.delete | bool</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Set description for each VLAN</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">community.network.routeros_command</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">commands</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span>- <span class="s2">&#34;/interface bridge vlan comment [find vlan-ids={{ item.id }}] comment=\&#34;{{ item.description }}\&#34;&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">with_items</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;{{ vlans }}&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">when</span><span class="p">:</span><span class="w"> </span><span class="l">not item.delete | bool</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Enable L3 HW Offloading</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">community.network.routeros_command</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">commands</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span>- <span class="s2">&#34;/interface ethernet switch set [find name=switch1] l3-hw-offloading=yes&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">when</span><span class="p">:</span><span class="w"> </span><span class="l">ansible_host == &#39;192.168.0.1&#39;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">register</span><span class="p">:</span><span class="w"> </span><span class="l">hw_offload_result</span><span class="w">
</span></span></span></code></pre></div><h3 id="group_varsallyml">group_vars/all.yml</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">vlans</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">id</span><span class="p">:</span><span class="w"> </span><span class="m">4</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">description</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;vMotion&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">delete</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">id</span><span class="p">:</span><span class="w"> </span><span class="m">12</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">description</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;ESXi MGMT&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">delete</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">id</span><span class="p">:</span><span class="w"> </span><span class="m">14</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">description</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;NSXB Host Tep&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">delete</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">id</span><span class="p">:</span><span class="w"> </span><span class="m">15</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">description</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;NSXB Edge Tep&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">delete</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">id</span><span class="p">:</span><span class="w"> </span><span class="m">20</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">description</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;K3s&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">delete</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">id</span><span class="p">:</span><span class="w"> </span><span class="m">31</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">description</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;NSXB Uplink1&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">delete</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">id</span><span class="p">:</span><span class="w"> </span><span class="m">41</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">description</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;NSXB Uplink2&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">delete</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">id</span><span class="p">:</span><span class="w"> </span><span class="m">50</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">description</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;RTEP NSX Federation&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">delete</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">id</span><span class="p">:</span><span class="w"> </span><span class="m">69</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">description</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;vSAN&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">delete</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">id</span><span class="p">:</span><span class="w"> </span><span class="m">200</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">description</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;VCF VM MGMT&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">delete</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">id</span><span class="p">:</span><span class="w"> </span><span class="m">201</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">description</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;VCF MGMT&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">delete</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">id</span><span class="p">:</span><span class="w"> </span><span class="m">202</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">description</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;VCF vSAN&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">delete</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">id</span><span class="p">:</span><span class="w"> </span><span class="m">203</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">description</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;VCF vSAN&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">delete</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">id</span><span class="p">:</span><span class="w"> </span><span class="m">204</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">description</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;VCF HostTEP&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">delete</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">id</span><span class="p">:</span><span class="w"> </span><span class="m">205</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">description</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;VCF EdgeTEP&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">delete</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">id</span><span class="p">:</span><span class="w"> </span><span class="m">206</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">description</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34; &#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">delete</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">id</span><span class="p">:</span><span class="w"> </span><span class="m">207</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">description</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34; &#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">delete</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">id</span><span class="p">:</span><span class="w"> </span><span class="m">208</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">description</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34; &#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">delete</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">id</span><span class="p">:</span><span class="w"> </span><span class="m">209</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">description</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34; &#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">delete</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">id</span><span class="p">:</span><span class="w"> </span><span class="m">211</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">description</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34; &#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">delete</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">id</span><span class="p">:</span><span class="w"> </span><span class="m">212</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">description</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34; &#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">delete</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></span></span></code></pre></div><h3 id="host_varsswitch1yml">host_vars/switch1.yml</h3>
<p>Each file defines the specific interfaces and bridges for a switch. Example for 192.168.0.5.yml</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">interfaces</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="l">10_bonding_SWA02</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="l">00_bonding_CoreRouter</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="l">01_ether1_ESX01_1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="l">01_ether2_ESX01_2</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="l">02_qsfpplus1-1_ESX02_1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="l">03_qsfpplus1-2_ESX03_1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="l">04_qsfpplus1-3_ESX04_1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="l">05_qsfpplus1-4_ESX05_1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="l">07_ether3_ESX07_1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="l">07_ether4_ESX07_2</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="l">08_ether5_ESX08_1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="l">08_ether6_ESX08_2 </span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="l">09_ether7_ESX09_1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="l">09_ether8_ESX09_2</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">bridge</span><span class="p">:</span><span class="w"> </span><span class="l">bridge</span><span class="w">
</span></span></span></code></pre></div><h2 id="what-does-the-playbook-do">What Does the Playbook Do?</h2>
<p>This playbook is designed to manage VLANs on MikroTik switches, including the ability to delete or configure VLANs. It uses the <code>community.network</code> collection and skips fact-gathering since it only executes specific commands on the devices.</p>
<h3 id="step-by-step-breakdown">Step-by-Step Breakdown</h3>
<ol>
<li>
<p><strong>Disable L3 Hardware Offloading</strong><br>
The playbook begins by disabling the L3 Hardware Offloading feature on the switch named <code>Tor Switch</code>. This step is necessary because MikroTik recommends turning off this feature before making VLAN configuration changes. The command is executed only if the host IP is <code>192.168.0.1</code>. The result is stored in the variable <code>hw_offload_result</code>.</p>
</li>
<li>
<p><strong>Delete VLANs Marked for Deletion</strong><br>
VLANs that are marked for deletion in the <code>vlans</code> variable (<code>delete: true</code>) are removed. The playbook iterates through the list of VLANs and executes the removal command for each VLAN marked as deleted.</p>
</li>
<li>
<p><strong>Configure VLANs</strong><br>
For VLANs that are not marked for deletion, the playbook configures them as follows:</p>
<ul>
<li>Removes any existing VLAN with the same ID to avoid conflicts.</li>
<li>Adds the VLAN to the specified bridge and assigns it to the interfaces defined as <code>tagged</code>. This ensures a clean and consistent configuration.</li>
</ul>
</li>
<li>
<p><strong>Set VLAN Descriptions</strong><br>
The playbook adds a description to each VLAN that is not marked for deletion. It uses the <code>comment</code> function in MikroTik and sets the description based on the variables provided.</p>
</li>
<li>
<p><strong>Enable L3 Hardware Offloading</strong><br>
Finally, the playbook re-enables the L3 Hardware Offloading feature on <code>ToR Switch</code>, but only if the host IP is <code>192.168.0.1</code>. The result of this step is also stored in the <code>hw_offload_result</code> variable.</p>
</li>
</ol>
<h2 id="summary">Summary</h2>
<p>This playbook automates the entire VLAN management process:</p>
<ul>
<li>Disables the L3 offloading feature when required.</li>
<li>Deletes VLANs marked for removal.</li>
<li>Configures new VLANs, ensuring no conflicts.</li>
<li>Sets descriptions for the VLANs.</li>
<li>Re-enables the L3 offloading feature after the configuration.</li>
</ul>
<p>The structure ensures reliability and consistency, handling edge cases such as existing VLANs and hardware offloading quirks automatically.</p>
<h2 id="whats-left-to-do">What&rsquo;s Left to Do</h2>
<p>There are several improvements I plan to make to this playbook in the future:</p>
<ol>
<li>
<p><strong>Clean Up Variables</strong><br>
Currently, there are some unused variables in the playbook that I need to clean up to keep the codebase tidy and maintainable.</p>
</li>
<li>
<p><strong>Enhanced Logic for VLAN Checks</strong><br>
I want to extend the logic to verify if a VLAN already matches the desired target configuration. This would prevent unnecessary deletion and re-creation of VLANs, reducing downtime and ensuring a smoother operation.</p>
</li>
<li>
<p><strong>Improved Error Handling</strong><br>
Better error handling is a priority to ensure the playbook gracefully recovers from unexpected issues, such as failed commands or unreachable devices.</p>
</li>
<li>
<p><strong>Pipeline Integration</strong><br>
The ultimate goal is to integrate the playbook into a pipeline, enabling automated execution through tools like Ansible Tower or ArgoCD. This would streamline the entire process and align it with a GitOps approach.</p>
</li>
<li>
<p><strong>Distributed Switch Integration</strong><br>
It’s also conceivable to extend the functionality by adding new VLANs directly to the distributed switch in my vSphere environment. However, this would be handled in a separate playbook to maintain modularity. The pipeline would then orchestrate both playbooks to ensure a seamless configuration process.</p>
</li>
</ol>
<p>By addressing these points, the project will become more robust, scalable, and aligned with modern automation practices.</p>
]]></content>
		</item>
		
		<item>
			<title>IPAM Automation with NetBox, Ansible, and Microsoft Windows DNS Server</title>
			<link>https://sdn-warrior.org/posts/ipam-automation/</link>
			<pubDate>Fri, 20 Dec 2024 02:00:04 +0100</pubDate>
			
			<guid>https://sdn-warrior.org/posts/ipam-automation/</guid>
			<description><![CDATA[IPAM Automation with Netbox and Ansible]]></description>
			<content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Managing IP addresses and DNS records manually can be a daunting task, especially in dynamic IT environments. This blog post demonstrates how to leverage NetBox, Ansible, and Microsoft Windows DNS Server to automate IP Address Management (IPAM) and DNS record updates, making your infrastructure more efficient and reliable.</p>
<h2 id="why-automate-ipam-and-dns">Why Automate IPAM and DNS?</h2>
<ul>
<li>Consistency: Automation minimizes human errors and ensures uniformity.</li>
<li>Efficiency: Automating repetitive tasks saves time and allows teams to focus on strategic activities.</li>
<li>Scalability: As networks grow, automated solutions adapt more easily than manual processes.</li>
</ul>
<h2 id="my-goal">My goal</h2>
<ul>
<li>Get a free IP address is dynamically fetched from a defined subnet in NetBox.</li>
<li>The IP address is immediately assigned to the specified FQDN in NetBox.</li>
<li>A corresponding Host A record is created in your Windows DNS Server.</li>
</ul>
<h2 id="prerequisites">Prerequisites</h2>
<p>Before diving into the implementation, ensure the following:</p>
<ul>
<li>A functional NetBox instance configured with appropriate IPAM data.</li>
<li>A Microsoft Windows DNS Server with administrative access.</li>
<li>Ansible installed and configured on a control node.</li>
<li>API access credentials for NetBox.</li>
<li>pywinrm Python module</li>
<li>PowerShell Remoting</li>
</ul>
<h2 id="ansible-project">Ansible Project</h2>
<p>For this automation project, I structured my workflow into multiple steps to keep it organized and modular. I use an ansible.cfg file to integrate and manage my inventory. At the core of the setup is a master playbook, which orchestrates the entire automation process.</p>
<p>To simplify and separate concerns, I divided the tasks into two sub-playbooks:</p>
<p>NetBox playbook: Handles all interactions with NetBox, such as fetching available IPs or updating DNS-related metadata.
DNS playbook: Focuses on managing DNS records on my Microsoft Windows DNS Server.
This approach not only makes the automation workflow easier to manage but also allows me to test and modify individual components independently while maintaining a clear overview of the entire process through the master playbook.</p>
<h2 id="getting-started">Getting Started</h2>
<p>To begin, I will list the files and their roles in this automation project. While these files are currently stored in my local Gitea instance, I’m considering creating a public Git repository for future projects to make them more accessible and easier to share.</p>
<h3 id="inventoryyml">inventory.yml</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">all</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">hosts</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">dnsserver.lab.home</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">ansible_host</span><span class="p">:</span><span class="w"> </span><span class="l">dc.lab.home </span><span class="w"> </span><span class="c"># IP-Adresse or Hostname of Windows-DNS-Servers</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">ansible_user</span><span class="p">:</span><span class="w"> </span><span class="l">administrator </span><span class="w"> </span><span class="c"># Username </span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">ansible_password</span><span class="p">:</span><span class="w"> </span><span class="l">xxx </span><span class="w"> </span><span class="c"># Password</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">ansible_connection</span><span class="p">:</span><span class="w"> </span><span class="l">winrm </span><span class="w"> </span><span class="c"># connection</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">ansible_winrm_transport</span><span class="p">:</span><span class="w"> </span><span class="l">basic </span><span class="w"> </span><span class="c"># auth</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">ansible_winr_server_cert_validation</span><span class="p">:</span><span class="w"> </span><span class="l">ignore</span><span class="w"> </span><span class="c">#don&#39;t check the certificate</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">ansible_port</span><span class="p">:</span><span class="w"> </span><span class="m">5986</span><span class="w"> </span><span class="c">#winrm https port</span><span class="w">
</span></span></span></code></pre></div><h3 id="ansiblecfg">ansible.cfg</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-ini" data-lang="ini"><span class="line"><span class="cl"><span class="k">[defaults]</span>
</span></span><span class="line"><span class="cl"><span class="na">inventory</span> <span class="o">=</span> <span class="s">inventory.yml</span>
</span></span></code></pre></div><h3 id="register_ipyml">register_ip.yml</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nn">---</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Validate input variables</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">fail</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">msg</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;You must provide &#39;netbox_token&#39;, &#39;prefix&#39;, and &#39;dns_name&#39; as extra-vars.&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">when</span><span class="p">:</span><span class="w"> </span><span class="l">netbox_token == &#34;&#34; or prefix == &#34;&#34; or dns_name == &#34;&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Get the prefix ID from NetBox</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">uri</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">url</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;{{ netbox_url }}/api/ipam/prefixes/?prefix={{ prefix }}&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">method</span><span class="p">:</span><span class="w"> </span><span class="l">GET</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">headers</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">Authorization</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;Token {{ netbox_token }}&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">Accept</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;application/json&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">return_content</span><span class="p">:</span><span class="w"> </span><span class="kc">yes</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">register</span><span class="p">:</span><span class="w"> </span><span class="l">prefix_data</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Fail if the prefix does not exist</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">fail</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">msg</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;Prefix {{ prefix }} does not exist in NetBox.&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">when</span><span class="p">:</span><span class="w"> </span><span class="l">prefix_data.json.results | length == 0</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Get available IPs in the prefix</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">uri</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">url</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;{{ netbox_url }}/api/ipam/prefixes/{{ prefix_data.json.results[0].id }}/available-ips/&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">method</span><span class="p">:</span><span class="w"> </span><span class="l">GET</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">headers</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">Authorization</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;Token {{ netbox_token }}&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">Accept</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;application/json&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">return_content</span><span class="p">:</span><span class="w"> </span><span class="kc">yes</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">register</span><span class="p">:</span><span class="w"> </span><span class="l">available_ips</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Fail if no available IPs are found</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">fail</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">msg</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;No available IPs found in prefix {{ prefix }}.&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">when</span><span class="p">:</span><span class="w"> </span><span class="l">available_ips.json | length == 0</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Assign the first available IP</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">uri</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">url</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;{{ netbox_url }}/api/ipam/ip-addresses/&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">method</span><span class="p">:</span><span class="w"> </span><span class="l">POST</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">headers</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">Authorization</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;Token {{ netbox_token }}&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">Accept</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;application/json&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">Content-Type</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;application/json&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">body</span><span class="p">:</span><span class="w"> </span><span class="p">&gt;</span><span class="sd">
</span></span></span><span class="line"><span class="cl"><span class="sd">      {
</span></span></span><span class="line"><span class="cl"><span class="sd">        &#34;address&#34;: &#34;{{ available_ips.json[0].address }}&#34;,
</span></span></span><span class="line"><span class="cl"><span class="sd">        &#34;status&#34;: &#34;active&#34;,
</span></span></span><span class="line"><span class="cl"><span class="sd">        &#34;description&#34;: &#34;Created by Ansible&#34;,
</span></span></span><span class="line"><span class="cl"><span class="sd">        &#34;dns_name&#34;: &#34;{{ dns_name }}&#34;
</span></span></span><span class="line"><span class="cl"><span class="sd">      }</span><span class="w">      
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">body_format</span><span class="p">:</span><span class="w"> </span><span class="l">json</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">status_code</span><span class="p">:</span><span class="w"> </span><span class="m">201</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">return_content</span><span class="p">:</span><span class="w"> </span><span class="kc">yes</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">register</span><span class="p">:</span><span class="w"> </span><span class="l">ip_assignment</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Extract host and zone from DNS name</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">set_fact</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">dns_host</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;{{ dns_name.split(&#39;.&#39;)[0] }}&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">dns_zone</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;{{ dns_name.split(&#39;.&#39;, 1)[1] }}&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">assigned_ip</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;{{ ip_assignment.json.address.split(&#39;/&#39;)[0] }}&#34;</span><span class="w">
</span></span></span></code></pre></div><h3 id="add_dns_recordyml">add_dns_record.yml</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nn">---</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Add DNS A Record</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">win_shell</span><span class="p">:</span><span class="w"> </span><span class="p">|</span><span class="sd">
</span></span></span><span class="line"><span class="cl"><span class="sd">    Add-DnsServerResourceRecordA -Name &#34;{{ zdns_host }}&#34; -ZoneName &#34;{{ zdns_zone }}&#34; -IPv4Address &#34;{{ zassigned_ip }}&#34;</span><span class="w">    
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">args</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">executable</span><span class="p">:</span><span class="w"> </span><span class="l">powershell</span><span class="w">
</span></span></span></code></pre></div><h3 id="mp_dnsyml-my-masterplaybook">mp_dns.yml (my masterplaybook)</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nn">---</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Register IP in NetBox</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">hosts</span><span class="p">:</span><span class="w"> </span><span class="l">localhost</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">gather_facts</span><span class="p">:</span><span class="w"> </span><span class="kc">no</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">vars</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">prefix</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;{{ prefix }}&#34;</span><span class="w">  </span><span class="c"># variables</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">dns_name</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;{{ dns_name }}&#34;</span><span class="w">  </span><span class="c"># variables</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">netbox_url</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;http://netbox.lab.home&#34;</span><span class="w">  </span><span class="c">#NetBox-URL</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">netbox_token</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;xxx&#34;</span><span class="w">  </span><span class="c"># Ntebox API token</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">tasks</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Run NetBox IP Registration Playbook</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">include_tasks</span><span class="p">:</span><span class="w"> </span><span class="l">register_ip.yml</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">vars</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">prefix</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;{{ prefix }}&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">dns_name</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;{{ dns_name }}&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        
</span></span></span><span class="line"><span class="cl"><span class="w"> 
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Add DNS A Record</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">hosts</span><span class="p">:</span><span class="w"> </span><span class="l">dnsserver.lab.home</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">gather_facts</span><span class="p">:</span><span class="w"> </span><span class="kc">no</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">vars</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">ansible_winrm_server_cert_validation</span><span class="p">:</span><span class="w"> </span><span class="l">ignore</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">zassigned_ip</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;{{ hostvars[&#39;localhost&#39;][&#39;sip&#39;] }}&#34;</span><span class="w"> </span><span class="c"># variables</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">zdns_host</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;{{ hostvars[&#39;localhost&#39;][&#39;sdns&#39;] }}&#34;</span><span class="w">   </span><span class="c"># variables</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">zdns_zone</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;{{ hostvars[&#39;localhost&#39;][&#39;szone&#39;] }}&#34;</span><span class="w">  </span><span class="c"># variables</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">tasks</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Include DNS Record Playbook</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">include_tasks</span><span class="p">:</span><span class="w"> </span><span class="l">add_dns_record.yml</span><span class="w">
</span></span></span></code></pre></div><h2 id="how-the-playbooks-work">How the Playbooks Work</h2>
<p>The process is coordinated by a master playbook (mp_dns.yml) and relies on sub-playbooks for discrete tasks.</p>
<h3 id="master-playbook-mp_dnsyml">Master Playbook (mp_dns.yml)</h3>
<p>The master playbook serves as the central control file. It performs the following steps:</p>
<p>Registers an IP Address in NetBox: This step invokes the register_ip.yml sub-playbook to allocate an available IP address within a specified prefix and associate it with the given DNS name in NetBox.</p>
<ul>
<li>
<p>Sets Facts:
After obtaining the IP address and DNS details from NetBox, it uses set_fact to store these values in variables (sip, sdns, szone) for use in the next task.</p>
</li>
<li>
<p>Adds a DNS A Record:
The second phase connects to the DNS server and calls the add_dns_record.yml sub-playbook to create a DNS A record using the information retrieved from NetBox.</p>
</li>
</ul>
<h3 id="sub-playbook-register_ipyml">Sub-Playbook: register_ip.yml</h3>
<p>This playbook interacts with NetBox&rsquo;s API to:</p>
<ul>
<li>Validate input variables like the NetBox token, prefix, and DNS name.</li>
<li>Retrieve the prefix and find available IPs.</li>
<li>Assign the first available IP to the provided DNS name and register it in NetBox.</li>
</ul>
<p>The playbook sends a POST request to the NetBox API to assign an available IP address to the provided DNS name. The response is returned in JSON format and parsed to extract the necessary variables for the DNS record creation.</p>
<p>The JSON response is parsed to extract key values:</p>
<ul>
<li>dns_host and dns_zone are derived by splitting the FQDN.</li>
<li>assigned_ip captures the raw IP address, omitting the CIDR notation.</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="w">  </span><span class="nt">dns_host</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;{{ dns_name.split(&#39;.&#39;)[0] }}&#34;</span><span class="w">  </span><span class="c"># Extracts the hostname (e.g., &#34;myhost&#34; from &#34;myhost.lab.local&#34;)</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">dns_zone</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;{{ dns_name.split(&#39;.&#39;, 1)[1] }}&#34;</span><span class="w">  </span><span class="c"># Extracts the zone (e.g., &#34;lab.local&#34;)</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">assigned_ip</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;{{ ip_assignment.json.address.split(&#39;/&#39;)[0] }}&#34;</span><span class="w">  </span><span class="c"># Removes the subnet mask (e.g., &#34;192.168.1.10/24&#34; to &#34;192.168.1.10&#34;)</span><span class="w">
</span></span></span></code></pre></div><p>This parsing ensures the required details are extracted for creating the DNS record in subsequent tasks, linking NetBox&rsquo;s IP allocation to the DNS configuration seamlessly.</p>
<h3 id="sub-playbook-add_dns_recordyml">Sub-Playbook: add_dns_record.yml</h3>
<p>This playbook uses PowerShell (win_shell) to execute the Add-DnsServerResourceRecordA cmdlet on the Windows DNS server. It creates a DNS A record with the assigned IP, host, and zone.</p>
<h3 id="why-use-host_vars">Why Use host_vars?</h3>
<p>hostvars is a built-in Ansible variable that provides access to variables from other hosts in the inventory. This is particularly useful when you need to share or reference facts or variables gathered from one host on another host.
The NetBox-related tasks (e.g., registering IP addresses and extracting DNS details) are performed on localhost since they interact with external APIs and don’t require remote server execution.
Variables like sip, sdns, and szone are set as facts on localhost during the first phase of the playbook execution.
The <em><strong>hostvars[&rsquo;localhost&rsquo;]</strong></em> construct is used to retrieve these facts and make them available to the subsequent tasks running on the DNS server (dnsserver.lab.home).</p>
<h3 id="variable-assignments">Variable Assignments:</h3>
<ul>
<li>zassigned_ip: This retrieves the IP address (sip) assigned to the host from the NetBox interaction on localhost.</li>
<li>zdns_host: This extracts the host portion of the DNS name (sdns) derived from the FQDN split.</li>
<li>zdns_zone: This fetches the DNS zone (szone), also derived from the FQDN split.</li>
</ul>
<p>This approach ensures that:</p>
<ul>
<li>Data derived or computed in one phase (NetBox-related tasks) is seamlessly passed to the next phase (DNS-related tasks).</li>
<li>sThe DNS playbook (add_dns_record.yml) running on the DNS server has access to the correct IP, host, and zone information without redundant processing.</li>
</ul>

    <aside class="admonition info">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-info">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="16" x2="12" y2="12"></line>
      <line x1="12" y1="8" x2="12.01" y2="8"></line>
   </svg></div><b>Info</b>
        </div>
        <div class="admonition-content"><p>One of the biggest challenges I faced in this project was understanding why I couldn’t directly use the variables returned from NetBox in the DNS-related tasks. I initially tried to pass these variables directly, but the playbook failed because the DNS tasks were executed on a different host (dnsserver.lab.home) than the one that retrieved the data (localhost).</p>
<p>The solution involved using hostvars to reference the facts set on localhost. This took the most time to figure out, as I didn’t immediately realize that variables gathered on one host are not automatically accessible on another. Once I understood how hostvars works, everything started to fall into place.</p>
</div>
    </aside>
<h2 id="ok-enough-code-and-explanations-lets-see-it-in-action">Ok, Enough Code and Explanations, Let’s See It in Action</h2>
<p>Starting the playbook:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl">ansible-playbook mp_dns.yml -e &#34;prefix=192.168.2.0/24 dns_name=hello-world.lab.home&#34;
</span></span></code></pre></div>
<figure><a href="output.png"><picture>
          <source srcset="/ipam-automation/output_hu17735082299749069346.webp" type="image/webp">
          <source srcset="/ipam-automation/output_hu2134898282273478624.jpg" type="image/jpeg">
          <img src="/ipam-automation/output_hu17735082299749069346.webp"alt="Ansible output"  width="1003"  height="610" />
        </picture></a><figcaption>
            <p>Ansible output (click to enlarge)</p>
          </figcaption></figure>

<figure><a href="netbox.png"><picture>
          <source srcset="/ipam-automation/netbox_hu16384549009619117197.webp" type="image/webp">
          <source srcset="/ipam-automation/netbox_hu10090990778334734330.jpg" type="image/jpeg">
          <img src="/ipam-automation/netbox_hu16384549009619117197.webp"alt="Netbox"  width="1447"  height="489" />
        </picture></a><figcaption>
            <p>Netbox (click to enlarge)</p>
          </figcaption></figure>

<figure><a href="dns.png"><picture>
          <source srcset="/ipam-automation/dns_hu12209689906728532248.webp" type="image/webp">
          <source srcset="/ipam-automation/dns_hu11255499409238053450.jpg" type="image/jpeg">
          <img src="/ipam-automation/dns_hu12209689906728532248.webp"alt="DNS"  width="404"  height="455" />
        </picture></a><figcaption>
            <p>DNS (click to enlarge)</p>
          </figcaption></figure>
<h2 id="conclusion">Conclusion</h2>
<p>This project represents just the first step toward a fully automated IPAM and DNS management workflow. While the current solution works well in my lab environment, there is plenty of room for improvement and expansion.</p>
<p>Key Takeaways:</p>
<ul>
<li>
<p>Modular Design: Starting with a modular playbook structure ensures flexibility for future enhancements and easier debugging.</p>
</li>
<li>
<p>Lab vs. Production: This setup is tailored for a lab environment. For production systems, avoid using highly privileged accounts like the local administrator on the DNS server. A more secure approach with role-based access control (RBAC) should be implemented in future iterations.</p>
</li>
<li>
<p>Continuous Improvement: I acknowledge that the playbook is not perfect. Over time, I plan to refine and optimize it, addressing any current shortcomings and making it more robust for complex workflows.</p>
</li>
</ul>
<p><em><strong>Automation is a journey</strong></em>, and I’m excited to see how this project evolves. Stay tuned for updates and new features in future versions!</p>
<h2 id="update--automatic-ptr-creation">Update:  automatic PTR creation</h2>
<p>Here’s a quick update to my blog: With the adjusted code, you can automatically create a PTR record when adding a Host A record. Note: The Reverse Lookup Zone must already exist.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nn">---</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Add DNS A Record</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">win_shell</span><span class="p">:</span><span class="w"> </span><span class="p">|</span><span class="sd">
</span></span></span><span class="line"><span class="cl"><span class="sd">    $ip = &#34;{{ zassigned_ip }}&#34;
</span></span></span><span class="line"><span class="cl"><span class="sd">    $hostname = &#34;{{ zdns_host }}.{{ zdns_zone }}&#34;
</span></span></span><span class="line"><span class="cl"><span class="sd">    $reverseZone = (&#34;{0}.{1}.{2}.in-addr.arpa&#34; -f $ip.Split(&#34;.&#34;)[2], $ip.Split(&#34;.&#34;)[1], $ip.Split(&#34;.&#34;)[0])
</span></span></span><span class="line"><span class="cl"><span class="sd">    Add-DnsServerResourceRecordA -Name &#34;{{ zdns_host }}&#34; -ZoneName &#34;{{ zdns_zone }}&#34; -IPv4Address &#34;{{ zassigned_ip }}&#34;
</span></span></span><span class="line"><span class="cl"><span class="sd">    Add-DnsServerResourceRecordPtr -ZoneName $reverseZone -Name ($ip.Split(&#34;.&#34;)[3]) -PtrDomainName &#34;$hostname.$zoneName&#34; </span><span class="w">    
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">args</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">executable</span><span class="p">:</span><span class="w"> </span><span class="l">powershell</span><span class="w">
</span></span></span></code></pre></div>]]></content>
		</item>
		
		<item>
			<title>From Zero to Automation: How I Used ChatGPT to Create My First Ansible Playbook</title>
			<link>https://sdn-warrior.org/posts/first-steps-ansible/</link>
			<pubDate>Tue, 17 Dec 2024 22:36:18 +0100</pubDate>
			
			<guid>https://sdn-warrior.org/posts/first-steps-ansible/</guid>
			<description><![CDATA[How I Used ChatGPT to Create My First Ansible Playbook]]></description>
			<content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>I recently decided to automate the startup and shutdown of my lab environments—both standard and nested labs. While the idea sounded simple, it quickly turned into an interesting challenge. Having never written an Ansible Playbook before, I turned to ChatGPT for help.</p>
<h2 id="why-chatgpt">Why ChatGPT?</h2>
<p>Let’s be honest: starting with Ansible can feel overwhelming, especially if you&rsquo;re new to it. My last experience with something remotely similar was years ago, working with PowerShell scripts or even earlier with .NET 3 (yes, I’m &ldquo;that old&rdquo;).</p>
<p>The task itself seemed straightforward at first:</p>
<ul>
<li>Write a playbook to power VMs on and off in a controlled manner.</li>
<li>Integrate both my standard lab and nested lab (e.g., my VCF setup with its own vCenter).</li>
</ul>
<p>However, the challenge revealed itself quickly:</p>
<ul>
<li>Controlling VMs via my main vCenter is relatively easy.</li>
<li>But what about nested labs where each nested setup has its own vCenter?</li>
</ul>
<p>This is where ChatGPT became a game changer.</p>
<h2 id="the-approach">The Approach</h2>
<h3 id="starting-from-zero">Starting from Zero</h3>
<p>I described my setup and goals to ChatGPT:</p>
<p>Automate VM startup/shutdown.
Handle dependencies like nested vCenters that control their own VMs.
ChatGPT provided a clear starting point, explaining how to structure an Ansible playbook. Step by step, it introduced me to tasks, loops, and the required VMware modules.</p>
<h3 id="iterating-through-challenges">Iterating Through Challenges</h3>
<p>The major challenge was managing nested environments:</p>
<p>Powering on the parent vCenter first.
Waiting until it’s responsive.
Then triggering the startup sequence for the nested VMs managed by that vCenter.
Through multiple iterations, ChatGPT helped refine the logic.</p>
<h3 id="not-always-smooth-sailing">Not Always Smooth Sailing</h3>
<p>To be honest, ChatGPT’s suggestions weren’t always perfect. More than once, I found myself in a dead end. I had to point out repeatedly that the same solution, presented for the third time, simply didn’t work. This is the reality of working with AI: it doesn’t replace expertise, but it certainly accelerates the process.</p>
<p>While ChatGPT couldn’t solve everything on its own, it significantly simplified finding the right solution. Instead of starting from scratch or digging through documentation for hours, I could focus on testing and refining the playbook.</p>
<h2 id="current-progress-what-i-achieved-in-two-evenings">Current Progress: What I Achieved in Two Evenings</h2>
<p>After a couple of evenings, with a few hours of experimenting and iterating with ChatGPT, I managed to create four modular Ansible playbooks. These playbooks are designed to handle two key scenarios for starting and stopping VMs:</p>
<p>Two Playbooks for Environments with vCenter</p>
<p>These playbooks are for my standard (non-nested) lab environments, where I can rely on vCenter to manage the VMs.
With vCenter in place, controlling VMs is relatively straightforward, as vCenter provides a central interface to handle power states.
Two Playbooks for Environments without vCenter</p>
<p>These playbooks handle environments where no vCenter is available, such as nested labs or standalone ESXi hosts.
In nested labs, the challenge arises because VMs and their dependencies are controlled individually, without the convenience of a central management interface.
By separating the logic into modular playbooks, I ensured flexibility and reusability across my different lab setups. Whether I’m dealing with my regular homelab VMs or complex nested environments like my VCF setup, I can now efficiently start and stop VMs with a single command.</p>
<h3 id="inventory-files-the-backbone-of-the-setup">Inventory Files: The Backbone of the Setup</h3>
<p>To make the playbooks flexible and reusable, I created inventory YAML files for each lab. Out of habit, I named them something like vcfvm_vars.yml or vcfesx_vars.yml. These files act as the variable storage for each lab environment.</p>
<p>There are two types of inventory files:</p>
<p>For Nested VMs:</p>
<p>Includes variables specific to nested lab setups, such as nested vCenter credentials, VM names, and their dependencies.
For Non-Nested VMs:</p>
<p>Stores details for standard VMs managed directly via the main vCenter.</p>
<h3 id="nested-vcf-example-controlled-boot-and-shutdown">Nested VCF Example: Controlled Boot and Shutdown</h3>
<p>In my VCF setup, which is fully nested, the playbook must follow a strict sequence:</p>
<p>Startup:</p>
<p>Start the nested ESXi hosts first.
Wait for their availability.
Then start the nested management VMs, such as NSX Manager, SDDC Manager, and vCenter.
Shutdown:</p>
<p>Stop the management VMs first.
Once the management layer is powered down, shut down the nested ESXi hosts.
This controlled sequence ensures the nested environment behaves predictably.</p>
<h3 id="inventory-file-for-esxi-hosts">Inventory File for ESXi Hosts</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl"># vcfesx_vars.yml
</span></span><span class="line"><span class="cl">vcenter_hostname: &#34;vcsa.lab.home&#34;
</span></span><span class="line"><span class="cl">vcenter_username: &#34;administrator@vsphere.local&#34;
</span></span><span class="line"><span class="cl">vcenter_password: &#34;your_pw&#34;
</span></span><span class="line"><span class="cl">vcenter_datacenter: &#34;Homelab&#34;
</span></span><span class="line"><span class="cl">validate_certs: false
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">vm_names:
</span></span><span class="line"><span class="cl">  - &#34;sfo01-m01-esx01&#34;
</span></span><span class="line"><span class="cl">  - &#34;sfo01-m01-esx02&#34;
</span></span><span class="line"><span class="cl">  - &#34;sfo01-m01-esx03&#34;
</span></span></code></pre></div><h3 id="inventory-file-for-nested-vms">Inventory File for Nested VMs</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl"># vcfvm_vars.yml
</span></span><span class="line"><span class="cl">validate_certs: false
</span></span><span class="line"><span class="cl">esxi_hosts:
</span></span><span class="line"><span class="cl">  - &#34;sfo01-m01-esx01.lab.home&#34;
</span></span><span class="line"><span class="cl">  - &#34;sfo01-m01-esx02.lab.home&#34;
</span></span><span class="line"><span class="cl">  - &#34;sfo01-m01-esx03.lab.home&#34;
</span></span><span class="line"><span class="cl">esxi_username: &#34;root&#34;
</span></span><span class="line"><span class="cl">esxi_password: &#34;your_pw&#34;
</span></span><span class="line"><span class="cl">esxi_datacenter: &#34;sfo-m01-dc01&#34;
</span></span><span class="line"><span class="cl">vm_names:
</span></span><span class="line"><span class="cl">  - &#34;vcfvcsa&#34;
</span></span><span class="line"><span class="cl">  - &#34;vcfnsx01a&#34;
</span></span><span class="line"><span class="cl">  - &#34;vcf01&#34;
</span></span></code></pre></div><h3 id="power-on-playbook-for-non-nested-vms">Power-On Playbook for Non-Nested VMs</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl">---
</span></span><span class="line"><span class="cl">- name: Start specific VMs in vCenter
</span></span><span class="line"><span class="cl">  hosts: localhost
</span></span><span class="line"><span class="cl">  gather_facts: no
</span></span><span class="line"><span class="cl">  collections:
</span></span><span class="line"><span class="cl">    - community.vmware
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  tasks:
</span></span><span class="line"><span class="cl">    - name: Load variables from file
</span></span><span class="line"><span class="cl">      include_vars: &#34;{{ vars_file }}&#34;
</span></span><span class="line"><span class="cl">    - name: Connect to vCenter and start VMs
</span></span><span class="line"><span class="cl">      community.vmware.vmware_guest_powerstate:
</span></span><span class="line"><span class="cl">        hostname: &#34;{{ vcenter_hostname }}&#34;
</span></span><span class="line"><span class="cl">        username: &#34;{{ vcenter_username }}&#34;
</span></span><span class="line"><span class="cl">        password: &#34;{{ vcenter_password }}&#34;
</span></span><span class="line"><span class="cl">        validate_certs: &#34;{{ validate_certs }}&#34;
</span></span><span class="line"><span class="cl">        name: &#34;{{ item }}&#34;
</span></span><span class="line"><span class="cl">        state: powered-on
</span></span><span class="line"><span class="cl">      loop: &#34;{{ vm_names }}&#34;
</span></span><span class="line"><span class="cl">      register: power_state_result
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    - name: Display power state result
</span></span><span class="line"><span class="cl">      debug:
</span></span><span class="line"><span class="cl">        msg: &#34;VM {{ item.item }} wurde erfolgreich gestartet.&#34;
</span></span><span class="line"><span class="cl">      when: item.instance.hw_power_status == &#34;poweredOn&#34;
</span></span><span class="line"><span class="cl">      loop: &#34;{{ power_state_result.results }}&#34;
</span></span><span class="line"><span class="cl">      loop_control:
</span></span><span class="line"><span class="cl">        label: &#34;{{ item.item }}&#34;
</span></span></code></pre></div><ol>
<li>
<p><em><strong>include_vars:</strong></em> Loads a variable file, such as vcfvm_vars.yml, which makes the playbook modular and reusable.</p>
</li>
<li>
<p><em><strong>community.vmware.vmware_guest_powerstate:</strong></em> Uses the <em><strong>vmware_guest_powerstate</strong></em> module to control the power state of VMs in a vCenter-managed environment.</p>
</li>
<li>
<p><em><strong>The state:</strong></em> powered-on option ensures VMs are powered on.</p>
</li>
<li>
<p><em><strong>register: power_state_result:</strong></em> Captures the result of the task execution for each VM, including its power state.</p>
</li>
<li>
<p><em><strong>debug with when:</strong></em> Checks the power state of each VM and displays a success message if the VM was successfully powered on.</p>
</li>
</ol>
<h3 id="power-on-playbook-for-nested-vms-on-multiple-esxi-hosts">Power-On Playbook for Nested VMs on Multiple ESXi Hosts</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl">---
</span></span><span class="line"><span class="cl">- name: Power on multiple VMs on multiple ESXi hosts
</span></span><span class="line"><span class="cl">  hosts: localhost
</span></span><span class="line"><span class="cl">  gather_facts: no
</span></span><span class="line"><span class="cl">  collections:
</span></span><span class="line"><span class="cl">    - community.vmware
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  tasks:
</span></span><span class="line"><span class="cl">    - name: Load variables from file
</span></span><span class="line"><span class="cl">      include_vars: &#34;{{ vars_file }}&#34;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    - name: Get VM power status for each VM on each ESXi host
</span></span><span class="line"><span class="cl">      community.vmware.vmware_guest_info:
</span></span><span class="line"><span class="cl">        hostname: &#34;{{ item.0 }}&#34;
</span></span><span class="line"><span class="cl">        username: &#34;{{ esxi_username }}&#34;
</span></span><span class="line"><span class="cl">        password: &#34;{{ esxi_password }}&#34;
</span></span><span class="line"><span class="cl">        datacenter: &#34;{{ esxi_datacenter }}&#34;
</span></span><span class="line"><span class="cl">        validate_certs: &#34;{{ validate_certs }}&#34;
</span></span><span class="line"><span class="cl">        name: &#34;{{ item.1 }}&#34;
</span></span><span class="line"><span class="cl">      with_nested:
</span></span><span class="line"><span class="cl">        - &#34;{{ esxi_hosts }}&#34;
</span></span><span class="line"><span class="cl">        - &#34;{{ vm_names }}&#34;
</span></span><span class="line"><span class="cl">      register: vm_info_results
</span></span><span class="line"><span class="cl">      ignore_errors: true
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    - name: Filter VMs that are poweredOff
</span></span><span class="line"><span class="cl">      set_fact:
</span></span><span class="line"><span class="cl">        powered_off_vms: &#34;{{ vm_info_results.results | selectattr(&#39;failed&#39;, &#39;equalto&#39;, false)
</span></span><span class="line"><span class="cl">                           | selectattr(&#39;instance.hw_power_status&#39;, &#39;equalto&#39;, &#39;poweredOff&#39;) }}&#34;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    - name: Power on VMs if they are poweredOff
</span></span><span class="line"><span class="cl">      community.vmware.vmware_guest_powerstate:
</span></span><span class="line"><span class="cl">        hostname: &#34;{{ item.item.0 }}&#34;
</span></span><span class="line"><span class="cl">        username: &#34;{{ esxi_username }}&#34;
</span></span><span class="line"><span class="cl">        password: &#34;{{ esxi_password }}&#34;
</span></span><span class="line"><span class="cl">        datacenter: &#34;{{ esxi_datacenter }}&#34;
</span></span><span class="line"><span class="cl">        validate_certs: &#34;{{ validate_certs }}&#34;
</span></span><span class="line"><span class="cl">        name: &#34;{{ item.item.1 }}&#34;
</span></span><span class="line"><span class="cl">        state: powered-on
</span></span><span class="line"><span class="cl">      loop: &#34;{{ powered_off_vms }}&#34;
</span></span><span class="line"><span class="cl">      loop_control:
</span></span><span class="line"><span class="cl">        label: &#34;Host: {{ item.item.0 }} | VM: {{ item.item.1 }}&#34;
</span></span><span class="line"><span class="cl">      register: poweron_results
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    - name: Wait for VMs to be powered on
</span></span><span class="line"><span class="cl">      community.vmware.vmware_guest_info:
</span></span><span class="line"><span class="cl">        hostname: &#34;{{ item.item.item.0 }}&#34;
</span></span><span class="line"><span class="cl">        username: &#34;{{ esxi_username }}&#34;
</span></span><span class="line"><span class="cl">        password: &#34;{{ esxi_password }}&#34;
</span></span><span class="line"><span class="cl">        datacenter: &#34;{{ esxi_datacenter }}&#34;
</span></span><span class="line"><span class="cl">        validate_certs: &#34;{{ validate_certs }}&#34;
</span></span><span class="line"><span class="cl">        name: &#34;{{ item.item.item.1 }}&#34;
</span></span><span class="line"><span class="cl">      loop: &#34;{{ poweron_results.results }}&#34;
</span></span><span class="line"><span class="cl">      loop_control:
</span></span><span class="line"><span class="cl">        label: &#34;Host: {{ item.item.item.0 }} | VM: {{ item.item.item.1 }}&#34;
</span></span><span class="line"><span class="cl">      register: vm_status
</span></span><span class="line"><span class="cl">      until: vm_status.instance.hw_power_status == &#34;poweredOn&#34;
</span></span><span class="line"><span class="cl">      retries: 20
</span></span><span class="line"><span class="cl">      delay: 15
</span></span><span class="line"><span class="cl">      when: item.failed == false
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    - name: Display power on result
</span></span><span class="line"><span class="cl">      debug:
</span></span><span class="line"><span class="cl">        msg: &#34;VM {{ item.item.item.1 }} on Host {{ item.item.item.0 }} has been successfully powered on.&#34;
</span></span><span class="line"><span class="cl">      loop: &#34;{{ poweron_results.results }}&#34;
</span></span><span class="line"><span class="cl">      loop_control:
</span></span><span class="line"><span class="cl">        label: &#34;Host: {{ item.item.item.0 }} | VM: {{ item.item.item.1 }}&#34;
</span></span></code></pre></div><ol>
<li>
<p><em><strong>vmware_guest_info</strong></em> Retrieves the power state of each VM on each ESXi host.</p>
</li>
<li>
<p><em><strong>set_fact</strong></em> Filters out only those VMs that are powered off.</p>
</li>
<li>
<p><em><strong>vmware_guest_powerstate</strong></em> Powers on each VM that is in a &ldquo;poweredOff&rdquo; state.</p>
</li>
<li>
<p><em><strong>wait_for with retries</strong></em> Ensures that the VMs are fully powered on before proceeding.</p>
</li>
<li>
<p><em><strong>debug</strong></em> Displays a confirmation message for each successfully powered-on VM.</p>
</li>
</ol>
<h3 id="master-playbook">Master Playbook</h3>
<p>to orchestrate the two Power-On playbooks in the correct order. I kept your current 60-second pause timer as a placeholder for checking ESXi server readiness but structured everything neatly for clarity. A 60-second pause ensures that the ESXi hosts have enough time to initialize. Why a Pause? Without an active feedback mechanism to confirm the ESXi servers are ready, this static wait acts as a temporary workaround and will replaced later.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl">---
</span></span><span class="line"><span class="cl">- name: Power on Nested ESXi Hosts
</span></span><span class="line"><span class="cl">  import_playbook: poweron_vcsa.yml
</span></span><span class="line"><span class="cl">  vars:
</span></span><span class="line"><span class="cl">    vars_file: &#34;vcfesx_vars.yml&#34;
</span></span><span class="line"><span class="cl">  # Executes the playbook to power on the nested ESXi hosts.
</span></span><span class="line"><span class="cl">  # Variables specific to ESXi servers are loaded from &#34;vcfesx_vars.yml&#34;.
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">- name: Wait for 60 seconds before powering on nested VMs
</span></span><span class="line"><span class="cl">  hosts: localhost
</span></span><span class="line"><span class="cl">  gather_facts: no
</span></span><span class="line"><span class="cl">  tasks:
</span></span><span class="line"><span class="cl">    - name: Pause for 60 seconds
</span></span><span class="line"><span class="cl">      pause:
</span></span><span class="line"><span class="cl">        seconds: 60
</span></span><span class="line"><span class="cl">      # A static wait time to ensure ESXi hosts are ready.
</span></span><span class="line"><span class="cl">      # This will be improved in the future with dynamic checks.
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">- name: Power on Nested Management VMs
</span></span><span class="line"><span class="cl">  import_playbook: poweron_esx.yml
</span></span><span class="line"><span class="cl">  vars:
</span></span><span class="line"><span class="cl">    vars_file: &#34;vcfvm_vars.yml&#34;
</span></span><span class="line"><span class="cl">  # Executes the playbook to power on nested VMs like NSX Manager, SDDC Manager, and vCenter.
</span></span><span class="line"><span class="cl">  # Variables specific to management VMs are loaded from &#34;vcfvm_vars.yml&#34;.
</span></span></code></pre></div><h3 id="starting-the-vms-via-the-ansible-master-playbook">Starting the VMs via the Ansible Master Playbook</h3>
<p>Starting my VCF nested lab has never been easier. With the Ansible Master Playbook, it’s as simple as running a single command on my Ansible server:</p>
<pre tabindex="0"><code>ansible-playbook mp_poweron_vcf.yml
</code></pre><p>Within approximately 5-10 minutes (depending on the overall load on my lab), the entire VCF environment is up and ready to use—without any further manual intervention.</p>
<p>The beauty of this setup lies in its flexibility:</p>
<p>New labs can be easily added by simply creating a new inventory file and a customized master playbook.
The core logic remains untouched, making it a scalable and modular solution for automating additional environments.
This approach not only saves time but also ensures consistency when starting up complex nested labs like my VCF setup.</p>

<figure><a href="ansible.png"><picture>
          <source srcset="/first-steps-ansible/ansible_hu6172869207777496753.webp" type="image/webp">
          <source srcset="/first-steps-ansible/ansible_hu10846971240981565405.jpg" type="image/jpeg">
          <img src="/first-steps-ansible/ansible_hu6172869207777496753.webp"alt="Ansible Log"  width="1718"  height="1056" />
        </picture></a><figcaption>
            <p>Ansible Output (click to enlarge)</p>
          </figcaption></figure>
<p>The log output of my Ansible playbook contains failed messages during the task: Get VM power status for each VM on each ESXi host
These failures occur because each ESXi host is queried for specific VMs (like vcf01) that may not exist on that particular host. This is both normal and expected behavior.</p>
<p>Why?
Due to DRS (Distributed Resource Scheduler), I can never be certain which nested ESXi host a particular VM was last running on. By iterating through all ESXi hosts, the playbook ensures that the power status of every VM is eventually retrieved, regardless of where it was previously located.</p>
<h3 id="shutdown-playbook-graceful-power-off-of-vms">Shutdown Playbook: Graceful Power-Off of VMs</h3>
<p>The shutdown process follows the same principles as the power-on playbook but in reverse order. Instead of starting VMs, it ensures a graceful shutdown while verifying their power state. I won&rsquo;t describe every task in detail, but here’s a quick overview:</p>
<p>Logic Similar to Power-On:</p>
<ul>
<li>VMs are iterated across multiple ESXi hosts.</li>
<li>Only VMs that are currently powered on are gracefully shut down.</li>
</ul>
<p>Graceful Shutdown with Validation:</p>
<ul>
<li>VMs are shut down using shutdown-guest to trigger the guest OS shutdown process.</li>
<li>A retry loop with retries: 20 and delay: 15 ensures that the playbook actively checks until the VMs reach the poweredOff state.</li>
</ul>
<p>Harmless Errors Handled:</p>
<ul>
<li>As with the power-on playbook, the ignore_errors: true directive handles expected failures gracefully (e.g., querying for VMs on ESXi hosts where they are not located).</li>
</ul>
<h3 id="shutdown-nested-vms">Shutdown Nested VMs</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl">- name: Graceful shutdown of multiple VMs on multiple ESXi hosts
</span></span><span class="line"><span class="cl">  hosts: localhost
</span></span><span class="line"><span class="cl">  gather_facts: no
</span></span><span class="line"><span class="cl">  collections:
</span></span><span class="line"><span class="cl">    - community.vmware
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  tasks:
</span></span><span class="line"><span class="cl">    - name: Load variables from file
</span></span><span class="line"><span class="cl">      include_vars: &#34;{{ vars_file }}&#34;
</span></span><span class="line"><span class="cl">      
</span></span><span class="line"><span class="cl">    - name: Get VM power status for each VM on each ESXi host
</span></span><span class="line"><span class="cl">      community.vmware.vmware_guest_info:
</span></span><span class="line"><span class="cl">        hostname: &#34;{{ item.0 }}&#34;
</span></span><span class="line"><span class="cl">        username: &#34;{{ esxi_username }}&#34;
</span></span><span class="line"><span class="cl">        password: &#34;{{ esxi_password }}&#34;
</span></span><span class="line"><span class="cl">        datacenter: &#34;{{ esxi_datacenter }}&#34;
</span></span><span class="line"><span class="cl">        validate_certs: &#34;{{ validate_certs }}&#34;
</span></span><span class="line"><span class="cl">        name: &#34;{{ item.1 }}&#34;
</span></span><span class="line"><span class="cl">      with_nested:
</span></span><span class="line"><span class="cl">        - &#34;{{ esxi_hosts }}&#34;
</span></span><span class="line"><span class="cl">        - &#34;{{ vm_names }}&#34;
</span></span><span class="line"><span class="cl">      register: vm_info_results
</span></span><span class="line"><span class="cl">      ignore_errors: true
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    - name: Filter VMs that are poweredOn
</span></span><span class="line"><span class="cl">      set_fact:
</span></span><span class="line"><span class="cl">        powered_on_vms: &#34;{{ vm_info_results.results | selectattr(&#39;failed&#39;, &#39;equalto&#39;, false)
</span></span><span class="line"><span class="cl">                           | selectattr(&#39;instance.hw_power_status&#39;, &#39;equalto&#39;, &#39;poweredOn&#39;) }}&#34;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    - name: Shut down VMs if they are poweredOn
</span></span><span class="line"><span class="cl">      community.vmware.vmware_guest_powerstate:
</span></span><span class="line"><span class="cl">        hostname: &#34;{{ item.item.0 }}&#34;
</span></span><span class="line"><span class="cl">        username: &#34;{{ esxi_username }}&#34;
</span></span><span class="line"><span class="cl">        password: &#34;{{ esxi_password }}&#34;
</span></span><span class="line"><span class="cl">        datacenter: &#34;{{ esxi_datacenter }}&#34;
</span></span><span class="line"><span class="cl">        validate_certs: &#34;{{ validate_certs }}&#34;
</span></span><span class="line"><span class="cl">        name: &#34;{{ item.item.1 }}&#34;
</span></span><span class="line"><span class="cl">        state: shutdown-guest
</span></span><span class="line"><span class="cl">        force: false
</span></span><span class="line"><span class="cl">      loop: &#34;{{ powered_on_vms }}&#34;
</span></span><span class="line"><span class="cl">      loop_control:
</span></span><span class="line"><span class="cl">        label: &#34;Host: {{ item.item.0 }} | VM: {{ item.item.1 }}&#34;
</span></span><span class="line"><span class="cl">      register: shutdown_results
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    - name: Wait for VMs to be powered off
</span></span><span class="line"><span class="cl">      community.vmware.vmware_guest_info:
</span></span><span class="line"><span class="cl">        hostname: &#34;{{ item.item.item.0 }}&#34;
</span></span><span class="line"><span class="cl">        username: &#34;{{ esxi_username }}&#34;
</span></span><span class="line"><span class="cl">        password: &#34;{{ esxi_password }}&#34;
</span></span><span class="line"><span class="cl">        datacenter: &#34;{{ esxi_datacenter }}&#34;
</span></span><span class="line"><span class="cl">        validate_certs: &#34;{{ validate_certs }}&#34;
</span></span><span class="line"><span class="cl">        name: &#34;{{ item.item.item.1 }}&#34;
</span></span><span class="line"><span class="cl">      loop: &#34;{{ shutdown_results.results }}&#34;
</span></span><span class="line"><span class="cl">      loop_control:
</span></span><span class="line"><span class="cl">        label: &#34;Host: {{ item.item.item.0 }} | VM: {{ item.item.item.1 }}&#34;
</span></span><span class="line"><span class="cl">      register: vm_status
</span></span><span class="line"><span class="cl">      until: vm_status.instance.hw_power_status == &#34;poweredOff&#34;
</span></span><span class="line"><span class="cl">      retries: 20
</span></span><span class="line"><span class="cl">      delay: 15
</span></span><span class="line"><span class="cl">      when: item.failed == false
</span></span></code></pre></div><h3 id="shutdown-playbook-for-virtual-esxi-servers-using-vcenter">Shutdown Playbook for Virtual ESXi Servers Using vCenter</h3>
<p>This playbook is very similar to the nested VM shutdown playbook, but since I can rely on the vCenter, I don’t need to iterate through all ESXi servers. This simplifies the process and improves efficiency.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl">---
</span></span><span class="line"><span class="cl">- name: Graceful shutdown of specific VMs if powered on
</span></span><span class="line"><span class="cl">  hosts: localhost
</span></span><span class="line"><span class="cl">  gather_facts: no
</span></span><span class="line"><span class="cl">  collections:
</span></span><span class="line"><span class="cl">    - community.vmware
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  tasks:
</span></span><span class="line"><span class="cl">    - name: Load variables from file
</span></span><span class="line"><span class="cl">      include_vars: &#34;{{ vars_file }}&#34;
</span></span><span class="line"><span class="cl">    - name: Get VM information
</span></span><span class="line"><span class="cl">      community.vmware.vmware_guest_info:
</span></span><span class="line"><span class="cl">        hostname: &#34;{{ vcenter_hostname }}&#34;
</span></span><span class="line"><span class="cl">        username: &#34;{{ vcenter_username }}&#34;
</span></span><span class="line"><span class="cl">        password: &#34;{{ vcenter_password }}&#34;
</span></span><span class="line"><span class="cl">        datacenter: &#34;{{ vcenter_datacenter }}&#34;
</span></span><span class="line"><span class="cl">        validate_certs: &#34;{{ validate_certs }}&#34;
</span></span><span class="line"><span class="cl">        name: &#34;{{ item }}&#34;
</span></span><span class="line"><span class="cl">      loop: &#34;{{ vm_names }}&#34;
</span></span><span class="line"><span class="cl">      register: vm_info_results
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    - name: Shut down VMs gracefully if powered on
</span></span><span class="line"><span class="cl">      community.vmware.vmware_guest_powerstate:
</span></span><span class="line"><span class="cl">        hostname: &#34;{{ vcenter_hostname }}&#34;
</span></span><span class="line"><span class="cl">        username: &#34;{{ vcenter_username }}&#34;
</span></span><span class="line"><span class="cl">        password: &#34;{{ vcenter_password }}&#34;
</span></span><span class="line"><span class="cl">        validate_certs: &#34;{{ validate_certs }}&#34;
</span></span><span class="line"><span class="cl">        name: &#34;{{ item.item }}&#34;
</span></span><span class="line"><span class="cl">        state: shutdown-guest
</span></span><span class="line"><span class="cl">        force: false
</span></span><span class="line"><span class="cl">      when: item.instance.hw_power_status == &#34;poweredOn&#34;
</span></span><span class="line"><span class="cl">      loop: &#34;{{ vm_info_results.results }}&#34;
</span></span><span class="line"><span class="cl">      register: shutdown_results
</span></span><span class="line"><span class="cl">      loop_control:
</span></span><span class="line"><span class="cl">        label: &#34;{{ item.item }}&#34;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    - name: Wait for VMs to be powered off
</span></span><span class="line"><span class="cl">      community.vmware.vmware_guest_info:
</span></span><span class="line"><span class="cl">        hostname: &#34;{{ vcenter_hostname }}&#34;
</span></span><span class="line"><span class="cl">        username: &#34;{{ vcenter_username }}&#34;
</span></span><span class="line"><span class="cl">        password: &#34;{{ vcenter_password }}&#34;
</span></span><span class="line"><span class="cl">        datacenter: &#34;{{ vcenter_datacenter }}&#34;
</span></span><span class="line"><span class="cl">        validate_certs: &#34;{{ validate_certs }}&#34;
</span></span><span class="line"><span class="cl">        name: &#34;{{ item.item }}&#34;
</span></span><span class="line"><span class="cl">      register: vm_status
</span></span><span class="line"><span class="cl">      until: vm_status.instance.hw_power_status == &#34;poweredOff&#34;
</span></span><span class="line"><span class="cl">      retries: 20
</span></span><span class="line"><span class="cl">      delay: 15
</span></span><span class="line"><span class="cl">      loop: &#34;{{ vm_info_results.results }}&#34;
</span></span><span class="line"><span class="cl">      when: item.instance.hw_power_status == &#34;poweredOn&#34;
</span></span><span class="line"><span class="cl">      loop_control:
</span></span><span class="line"><span class="cl">        label: &#34;{{ item.item }}&#34;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    - name: Display shutdown result
</span></span><span class="line"><span class="cl">      debug:
</span></span><span class="line"><span class="cl">        msg: &#34;VM {{ item.item }} ist erfolgreich heruntergefahren oder war bereits ausgeschaltet.&#34;
</span></span><span class="line"><span class="cl">      loop: &#34;{{ vm_info_results.results }}&#34;
</span></span><span class="line"><span class="cl">      loop_control:
</span></span><span class="line"><span class="cl">        label: &#34;{{ item.item }}&#34;
</span></span></code></pre></div><p>Use of vCenter:</p>
<ul>
<li>
<p>The playbook uses vCenter directly to manage the shutdown process, which avoids manually iterating through all ESXi hosts.
Graceful Shutdown:</p>
</li>
<li>
<p>The shutdown-guest option triggers a clean shutdown of the guest operating system running on the virtual ESXi servers.
Dynamic Verification:</p>
</li>
<li>
<p>The playbook dynamically filters the powered-on ESXi VMs and waits until their power state is confirmed as poweredOff.
Efficiency:</p>
</li>
<li>
<p>By leveraging vCenter and a loop with retries, the process is both clean and efficient.</p>
</li>
</ul>
<h3 id="master-shutdown-playbook">Master Shutdown Playbook</h3>
<p>To orchestrate the shutdown of the nested VCF lab and its virtual ESXi servers, we’ll create a master playbook similar to the Power-On master playbook. The inventory files remain the same as those used for the Power-On process, ensuring consistency and avoiding duplication.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl">- name: Poweroff Nested VMs   
</span></span><span class="line"><span class="cl">  import_playbook: shutdown_esx.yml
</span></span><span class="line"><span class="cl">  vars:
</span></span><span class="line"><span class="cl">    vars_file: &#34;vcfvm_vars.yml&#34;
</span></span><span class="line"><span class="cl">- name: Poweroff Nested ESXi
</span></span><span class="line"><span class="cl">  import_playbook: shutdown_vcsa.yml
</span></span><span class="line"><span class="cl">  vars:
</span></span><span class="line"><span class="cl">    vars_file: &#34;vcfesx_vars.yml&#34;
</span></span></code></pre></div><p>Unlike the Power-On master playbook, the shutdown process does not require a pause or workaround. This is because during the shutdown, we can actively check if the respective VMs have already powered off using a loop. This makes the process cleaner and more efficient.</p>

    <aside class="admonition attention">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" class="feather feather-link" width="24" height="24" viewBox="0 0 24 24"
      fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
      <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
      <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
   </svg></div><b>Attention</b>
        </div>
        <div class="admonition-content"><p>The playbooks presented in this article were generated with the help of AI and subsequently adjusted to work in my specific environment. While they function as intended for my use case, I strongly recommend exercising caution and thoroughly testing these playbooks in your own environment before implementing or relying on them.</p>
<p>Automation can be powerful, but every infrastructure is unique—always test in a controlled setting first!</p>
</div>
    </aside>
<h2 id="conclusion-is-chatgpt-useful-for-ansible">Conclusion: Is ChatGPT Useful for Ansible?</h2>
<p>From my perspective, the answer is both yes and no.</p>
<p>ChatGPT gave me a solid starting point and explained a lot of the foundational concepts, which was extremely helpful as a beginner with Ansible. However, it wasn’t perfect—there were several significant errors in the generated playbooks, and more than once, the AI proposed the same incorrect solution repeatedly.</p>
<p>Despite these challenges, I still found the process enjoyable. With some manual corrections and adjustments, I was able to create playbooks that worked for my specific environment. Within just a few hours, I achieved a usable result—something that would have taken considerably longer without ChatGPT&rsquo;s assistance.</p>
<p>Ultimately, while ChatGPT cannot replace expertise or thorough testing, it’s a powerful tool to accelerate development and simplify learning, especially when working with automation tools like Ansible.</p>
]]></content>
		</item>
		
		<item>
			<title>Homelab V5</title>
			<link>https://sdn-warrior.org/posts/homelab-v5/</link>
			<pubDate>Sat, 14 Dec 2024 02:00:26 +0100</pubDate>
			
			<guid>https://sdn-warrior.org/posts/homelab-v5/</guid>
			<description><![CDATA[My Homelab Journey: From Unraid Beginnings to Version 5]]></description>
			<content type="html"><![CDATA[<h2 id="my-homelab-journey-from-unraid-beginnings-to-version-5">My Homelab Journey: From Unraid Beginnings to Version 5</h2>
<p>Building and optimizing a homelab has always been a passion of mine. Since its inception, my homelab has gone through several iterations, constantly evolving to meet my goals of achieving maximum performance while minimizing power consumption, noise, and physical space requirements. Here is a snapshot of my journey, culminating in the current Version 5 of my homelab.</p>
<h2 id="the-beginning-unraid-with-custom-hardware">The Beginning: Unraid with Custom Hardware</h2>
<p>My homelab journey began with a custom-built Unraid server featuring an Intel i3 11th Generation processor and 64 GB of RAM. This setup acted as an all-in-one solution for storage, virtualization, and container workloads. I even conducted simple nested vSphere tests on this server during its early days. Today, the server is still in use as a storage and Docker host, although I have replaced the underlying hardware four times to keep up with evolving requirements.</p>

<figure><picture>
          <source srcset="/labv5/unraid_hu12871636074369548549.webp" type="image/webp">
          <source srcset="/labv5/unraid_hu3133507235213664422.jpeg" type="image/jpeg">
          <img src="/labv5/unraid_hu12871636074369548549.webp"alt="Homelab v1"  width="960"  height="1280" />
        </picture><figcaption>
            <p>My first Homelab</p>
          </figcaption></figure>
<p>The rack that I used from Version 1 through Version 4 of my homelab housed only 2 switches, a Pi3 and an old HP Elitedesk Client in V1, but it had to be replaced to accommodate the changes in Version 5.</p>
<h2 id="evolution-to-version-5">Evolution to Version 5</h2>
<p>Over the years, I continuously refined and upgraded the homelab. With my role at Evoila GmbH, my expectations for both myself and my homelab grew significantly. It quickly became clear that I needed different hardware to meet these new demands, especially as I aimed to conduct more extensive labs with NSX.</p>
<p>To start, I added a simple 3-node NUC cluster using 11th Generation Intel i5 processors. Additionally, I replaced the switches in my setup with multiple multispeed switches from Zyxel and QNAP. At the time, there were limited options on the market for 2.5 Gbps switches with management capabilities, resulting in a somewhat heterogeneous configuration.</p>
<p>Each iteration brought new hardware, better software configurations, and more ambitious goals. Over time, more technologies found their way into my lab, including a Fortinet FortiGate F40, BGP routing, and 10G switches. These advancements eventually culminated in Version 4 of my lab. However, as the lab grew, the rack ran out of space for further development, prompting the need for a complete rebuild, which led to the creation of Version 5. Now, in its fifth version, the lab has transformed into a powerful and efficient setup comprising.</p>
<h2 id="my-lab-philosophy">My Lab Philosophy</h2>
<p>My primary goal has always been to achieve the best possible performance with minimal power consumption, noise, and space requirements. To this end, I have standardized my homelab on Intel’s 13th Generation CPUs, which strike a great balance between power efficiency and computational capability.</p>
<h2 id="lab-overview">Lab Overview</h2>
<p>In Lab Version 5, I have three clusters:</p>
<h3 id="management-cluster">Management Cluster:</h3>
<p>This cluster is powered by an Intel NUC i3 13th Generation, which serves as the always-on management node. The ESXi server in this cluster hosts several key VMs:</p>
<ul>
<li>A Windows 11 VM with tools like Hugo, Go, and GitHub for managing this blog.</li>
<li>LogInsight and FortiAnalyzer.</li>
<li>A vCenter server.</li>
<li>An mDNS Repeater for Smart Home integration.</li>
<li>Homebridge for managing smart devices.</li>
<li>NetBox for network documentation.</li>
<li>A Veeam server for backups.</li>
</ul>
<p>Additionally, two VMs on my Unraid server contribute to the management cluster:</p>
<ul>
<li>A Root CA.</li>
<li>A Domain Controller, which primarily supports the labs.</li>
</ul>
<p>The Unraid server also runs several containers, including:</p>
<ul>
<li>DNS servers.</li>
<li>An Excalidraw instance.</li>
<li>Various other tools.</li>
</ul>
<p>For redundancy, I run a backup DNS server on a Raspberry Pi 3 to ensure DNS functionality during storage maintenance. I use AdGuard Home as my primary DNS server, which blocks ads and forwards DNS queries to my lab.home domain managed by the Active Directory server.</p>
<h3 id="compute-cluster">Compute Cluster:</h3>
<p>My compute cluster consists of three Intel NUCs of the 13th Generation, each equipped with 64 GB of RAM and a 2TB NVMe drive. Due to the P/E core architecture, these NUCs do not support Hyperthreading but offer 12 cores (4P + 8E). Based on my experience, the performance with E cores enabled is better than using 4 P cores with Hyperthreading. Each NUC features dual 2.5G network interfaces and is connected to my iSCSI storage. This cluster runs standard nested labs, such as my NSX lab and AVI load balancer labs. The performance is sufficient for many labs, making it a reliable and frequently used part of my setup.</p>
<p>The Compute Cluster in Lab Version 5 has the following total resources:</p>
<ul>
<li>192 GB RAM across 3 NUCs</li>
<li>6 TB NVMe storage (2 TB per NUC)</li>
<li>8 TB shared storage</li>
<li>36 CPU cores (12 cores per NUC)</li>
</ul>
<h3 id="performance-cluster">Performance Cluster:</h3>
<p>My performance cluster consists of four MinisForum MS-01 units, each featuring an Intel i9 processor with 14 cores (6P + 8E), 64 GB of physical RAM, and a 400% memory tiering configuration. Each MS-01 includes 2 TB of local NVMe storage and an additional 1 TB PCIe4 NVMe drive for memory tiering. With onboard dual 10GbE networking, the MS-01 units are ideal for demanding labs, such as a complete VCF deployment including an HCX proof of concept where I live-migrated VMs between my NSX lab and the VCF lab. The MS-01 units are also used for vSAN labs. Additionally, Intel vPro support allows for efficient remote management. This cluster provides:</p>
<ul>
<li>56 CPU cores (14 cores per MS-01).</li>
<li>1280 GB of RAM (64 GB physical per unit with memory tiering).</li>
<li>8 TB of local NVMe storage (2 TB per unit).</li>
<li>4 TB of NVMe storage for memory tiering (1 TB per unit).</li>
<li>8 TB shared Storage</li>
</ul>
<h3 id="network">Network</h3>
<p>My network consists of multiple MikroTik switches. The centerpiece is my ToR (Top of Rack) switch, the MikroTik CRS309, which can route at line speed thanks to hardware offloading. This switch hosts all lab-relevant gateways and networks, ensuring they don&rsquo;t need to be routed through my Fortinet FortiGate F40. The servers themselves are connected to two access switches: the NUCs via dual 2.5Gbps connections, and the MS-01 units via 10Gbps connections per switch.
I also have a service router (RB5009) that establishes a VPN tunnel to a fellow homelabber. Through this connection, I can utilize his Kubernetes resources, and we&rsquo;ve even tested the NSX Application Platform (NAPP) together. <a href="https://marschall.systems/">Visit marschall.systems</a></p>

<figure><a href="plan.png"><picture>
          <source srcset="/labv5/plan_hu4235498138432559503.webp" type="image/webp">
          <source srcset="/labv5/plan_hu7818108223675192909.jpg" type="image/jpeg">
          <img src="/labv5/plan_hu4235498138432559503.webp"alt="Network setup"  width="6206"  height="3968" />
        </picture></a><figcaption>
            <p>Network setup (click to enlarge)</p>
          </figcaption></figure>
<p>My network employs dynamic routing, with eBGP as the primary protocol peering all critical components. My BGP NSX lab peers directly with my ToR switch, ensuring high efficiency and seamless integration with the rest of the network. For my labs, I utilize both OSPF and BGP. My OSPF lab runs on two virtual ArubaCX switches and a VyOS router, which has an IP in my standard client network and provides internet access to the OSPF lab via NAT.</p>

<figure><a href="bgp.png"><picture>
          <source srcset="/labv5/bgp_hu13323852698658684218.webp" type="image/webp">
          <source srcset="/labv5/bgp_hu5788182371385841057.jpg" type="image/jpeg">
          <img src="/labv5/bgp_hu13323852698658684218.webp"alt="Network bgp setup"  width="3111"  height="2917" />
        </picture></a><figcaption>
            <p>BGP setup (click to enlarge)</p>
          </figcaption></figure>
<h3 id="storage">Storage</h3>
<p>As my primary storage solution, I use my Unraid server. After implementing several iSCSI optimizations <a href="https://sdn-warrior.org/posts/iscsi-tuning/">(my iSCSI Blog post)</a> and installing the iSCSI Target plugin <a href="https://sdn-warrior.org/posts/unraid-storage/">(my Unraid Blog post)</a>, the server provides a performant iSCSI storage capable of achieving around 2000 MB/s for both read and write operations.</p>

    <aside class="admonition info">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-info">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="16" x2="12" y2="12"></line>
      <line x1="12" y1="8" x2="12.01" y2="8"></line>
   </svg></div><b>Info</b>
        </div>
        <div class="admonition-content">An iSCSI Target in Linux refers to a service or daemon that enables a Linux server to present storage devices over the network using the iSCSI protocol. This allows other machines, known as iSCSI Initiators, to connect to and use these storage devices as if they were local drives.</div>
    </aside>
<h2 id="firewall">Firewall</h2>
<p>As my firewall, I use a Fortinet FortiGate F40, which I’ve had for two years. I am fortunate to have access to an NFR/LAB license through my employer at an affordable price. The FortiGate F40 handles both firewalling and IDS/IPS functionality. Additionally, I operate it in a dual-stack configuration and leverage its SD-WAN feature to load balance two WAN connections: 5G and DSL.</p>
<h2 id="lessons-learned-and-future-goals">Lessons Learned and Future Goals</h2>
<ul>
<li>
<p>Performance vs. Efficiency: Achieving the right balance between performance and efficiency requires meticulous planning and experimentation. Each hardware choice and configuration tweak contributes to the overall success of the setup.</p>
</li>
<li>
<p>Automation: In the future, I plan to incorporate more automation into my homelab. To achieve this, I have started experimenting with Terraform to streamline deployments and configurations.</p>
</li>
<li>
<p>Scaling Smartly: As my lab has grown, managing power, cooling, and network configurations has become increasingly important.</p>
</li>
<li>
<p>Continuous Improvement: My homelab is a perpetual work in progress. With each iteration, I discover new ways to optimize and expand its capabilities.</p>
</li>
</ul>
<h2 id="current-setup-bill-of-materials-bom">Current Setup: Bill of Materials (BOM)</h2>
<table>
  <thead>
      <tr>
          <th>Quantity</th>
          <th>Component</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Server</strong></td>
          <td></td>
      </tr>
      <tr>
          <td>4</td>
          <td>Minisforum MS-01 i9 13.Gen</td>
      </tr>
      <tr>
          <td>3</td>
          <td>Asus NUC Pro i7 13.Gen</td>
      </tr>
      <tr>
          <td>1</td>
          <td>Asus NUC Pro i3 13.Gen</td>
      </tr>
      <tr>
          <td><strong>Network</strong></td>
          <td></td>
      </tr>
      <tr>
          <td>2</td>
          <td>Mikrotik CRS309-1G-8S+IN</td>
      </tr>
      <tr>
          <td>1</td>
          <td>MikroTik L009UiGS-RM</td>
      </tr>
      <tr>
          <td>1</td>
          <td>Mikrotik CRS326-4C +20G+2Q</td>
      </tr>
      <tr>
          <td>1</td>
          <td>Mikrotik RB5009UG+S+IN</td>
      </tr>
      <tr>
          <td><strong>Storage</strong></td>
          <td></td>
      </tr>
      <tr>
          <td>1</td>
          <td>Intel NUC Extreme i7 11.Gen</td>
      </tr>
      <tr>
          <td><strong>USV</strong></td>
          <td></td>
      </tr>
      <tr>
          <td>1</td>
          <td>APC Back-UPS Pro 1300VA BR1300MI</td>
      </tr>
      <tr>
          <td><strong>Firewall</strong></td>
          <td></td>
      </tr>
      <tr>
          <td>1</td>
          <td>Fortinet Fortigate F40</td>
      </tr>
      <tr>
          <td><strong>Other</strong></td>
          <td></td>
      </tr>
      <tr>
          <td>1</td>
          <td>21U Rack</td>
      </tr>
      <tr>
          <td>1</td>
          <td>DAC Cable / Ethernet Cable</td>
      </tr>
      <tr>
          <td>2</td>
          <td>Cable Management</td>
      </tr>
      <tr>
          <td>2</td>
          <td>Rack Mount MS-01</td>
      </tr>
      <tr>
          <td>1</td>
          <td>Rack Mount NUC</td>
      </tr>
      <tr>
          <td>2</td>
          <td>Air Vent</td>
      </tr>
      <tr>
          <td>4</td>
          <td>Rack PSU</td>
      </tr>
  </tbody>
</table>
<p>You can also find the detailed BOM with prices <a href="https://docs.google.com/spreadsheets/d/1XK32KJWiLBMlKLlPSKNDwBaX2mg4fmGFN2aEA3SIsEc/edit?pli=1&amp;gid=0#gid=0">here</a>, which I update regularly to reflect any changes in my setup.</p>
<h2 id="final-result">Final result</h2>
<p>
<figure><picture>
          <source srcset="/labv5/rackmount_hu1092622698464279345.webp" type="image/webp">
          <source srcset="/labv5/rackmount_hu15529659375378009796.jpg" type="image/jpeg">
          <img src="/labv5/rackmount_hu1092622698464279345.webp"alt="Rackmount"  width="1600"  height="1200" />
        </picture><figcaption>
            <p>Rackmount MS-01</p>
          </figcaption></figure>

<figure><picture>
          <source srcset="/labv5/rackmount2_hu12242692340557408737.webp" type="image/webp">
          <source srcset="/labv5/rackmount2_hu16189033617681113906.jpg" type="image/jpeg">
          <img src="/labv5/rackmount2_hu12242692340557408737.webp"alt="Rackmount2"  width="2250"  height="1500" />
        </picture><figcaption>
            <p>Rackmount MS-01</p>
          </figcaption></figure>

<figure><picture>
          <source srcset="/labv5/cable_hu10763548189357929237.webp" type="image/webp">
          <source srcset="/labv5/cable_hu12300581878864859744.jpg" type="image/jpeg">
          <img src="/labv5/cable_hu10763548189357929237.webp"alt="Rack"  width="1500"  height="1000" />
        </picture><figcaption>
            <p>Rack view top</p>
          </figcaption></figure>

<figure><picture>
          <source srcset="/labv5/rack_hu9852508572301551615.webp" type="image/webp">
          <source srcset="/labv5/rack_hu10464460824911337438.jpg" type="image/jpeg">
          <img src="/labv5/rack_hu9852508572301551615.webp"alt="Rack"  width="1200"  height="1600" />
        </picture><figcaption>
            <p>Rack view</p>
          </figcaption></figure></p>
]]></content>
		</item>
		
		<item>
			<title>How to use QoS in NSX</title>
			<link>https://sdn-warrior.org/posts/nsx-qos/</link>
			<pubDate>Tue, 10 Dec 2024 02:00:40 +0100</pubDate>
			
			<guid>https://sdn-warrior.org/posts/nsx-qos/</guid>
			<description><![CDATA[How to use QoS in NSX]]></description>
			<content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Quality of Service (QoS) is a critical aspect of network performance management, especially in complex environments where NSX is deployed. NSX provides powerful QoS capabilities at both the gateway and segment levels, enabling fine-grained control over traffic prioritization and bandwidth allocation. However, understanding the differences between these two levels of QoS implementation is essential for optimizing network performance.</p>
<p>In this article, we’ll delve into how QoS functions on the gateway versus the segment in NSX, explore their respective use cases, and provide insights into selecting the right approach for your network needs. Whether you&rsquo;re managing inter-tenant traffic or fine-tuning internal traffic flows, mastering these distinctions will empower you to make informed decisions and maximize the efficiency of your NSX deployment.</p>
<h2 id="qos-on-an-nsx-segment">QoS on an NSX Segment</h2>
<p>Quality of Service (QoS) on an NSX segment focuses on managing traffic flows within a specific segment, providing comprehensive control over bandwidth and traffic priorities. Unlike gateway-level QoS, which typically manages north-south traffic at the boundary of the network, segment-level QoS applies to all traffic associated with virtual machines (VMs) on the segment, regardless of direction.</p>
<p>To implement QoS at this level, you must first create a Segment Profile, which defines the QoS policies. This includes settings such as ingress and egress traffic shaping, bandwidth guarantees, and DSCP marking. Once configured, this profile is attached to the segment, ensuring that the specified QoS policies are applied consistently to all VMs on that segment.</p>

<figure><a href="qos1.png"><picture>
          <source srcset="/nsx-qos/qos1_hu9376800852532490140.webp" type="image/webp">
          <source srcset="/nsx-qos/qos1_hu16011686681575812025.jpg" type="image/jpeg">
          <img src="/nsx-qos/qos1_hu9376800852532490140.webp"alt="QoS Profile"  width="1157"  height="466" />
        </picture></a><figcaption>
            <p>QoS Profile</p>
          </figcaption></figure>

    <aside class="admonition info">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-info">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="16" x2="12" y2="12"></line>
      <line x1="12" y1="8" x2="12.01" y2="8"></line>
   </svg></div><b>Info</b>
        </div>
        <div class="admonition-content">A crucial aspect of segment-level QoS is that it impacts all traffic originating from or destined for VMs on the segment. For example, if a Segment Profile specifies a guaranteed bandwidth of 30 Mbps, each VM on that segment will have this guarantee for all traffic, whether it is east-west within the same data center or north-south to external networks.</div>
    </aside>
<h2 id="explanation-of-segment-qos-profile-parameters-in-nsx">Explanation of Segment QoS Profile Parameters in NSX</h2>
<h3 id="mode"><strong>Mode</strong></h3>
<p>Defines how DSCP (Differentiated Services Code Point) values are handled for traffic originating from or destined for a logical port.</p>
<ul>
<li>
<p><strong>Trusted Mode</strong>:</p>
<ul>
<li>The DSCP value from the <strong>inner packet header</strong> (original header) is copied to the <strong>outer IP header</strong> (tunnel header) for IP/IPv6 traffic.</li>
<li>For non-IP/IPv6 traffic, the default DSCP value (0) is used for the outer IP header.</li>
<li>Supported only on <strong>overlay-based logical ports</strong>.</li>
</ul>
</li>
<li>
<p><strong>Untrusted Mode</strong>:</p>
<ul>
<li>For <strong>overlay-based logical ports</strong>, the configured DSCP value is applied to the outer IP header, regardless of the inner packet type.</li>
<li>For <strong>VLAN-based logical ports</strong>, the configured DSCP value is applied to the IP/IPv6 packets&rsquo; outer IP header.</li>
<li>DSCP values can range from 0 to 63.</li>
</ul>
</li>
</ul>

    <aside class="admonition info">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-info">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="16" x2="12" y2="12"></line>
      <line x1="12" y1="8" x2="12.01" y2="8"></line>
   </svg></div><b>Info</b>
        </div>
        <div class="admonition-content">DSCP settings affect only tunneled traffic and do not apply to traffic within the same hypervisor.</div>
    </aside>
<h3 id="priority"><strong>Priority</strong></h3>
<p>Specifies the <strong>DSCP priority value</strong>, which determines the level of importance for packets. The DSCP priority values range from <strong>0 to 63</strong>, with higher values indicating higher priority traffic.</p>
<h3 id="class-of-service-cos"><strong>Class of Service (CoS)</strong></h3>
<p>Defines the <strong>CoS value</strong>, applicable to VLAN-based logical ports.</p>
<ul>
<li>CoS groups similar traffic types and assigns a service priority level for each type.</li>
<li>Lower-priority traffic may experience reduced throughput or be dropped to ensure better performance for higher-priority traffic.</li>
<li>CoS can also be configured for VLAN ID 0 packets.</li>
<li>The CoS value ranges from <strong>0 to 7</strong>, where <strong>0</strong> indicates best-effort service.</li>
</ul>
<h3 id="ingress">Ingress</h3>
<p>Configures traffic shaping for <strong>outbound traffic</strong> from the VM to the logical network.</p>
<ul>
<li><strong>Average Bandwidth</strong>: The average rate of outbound traffic to prevent network congestion.</li>
<li><strong>Peak Bandwidth</strong>: The maximum traffic rate allowed to support bursts.</li>
<li><strong>Burst Size</strong>: Defines the maximum data size for a traffic burst, calculated as:</li>
</ul>
$$
\frac{\text{Peak Bandwidth (in bits per second)} \times \text{Burst Duration (in seconds)}}{8} = \text{Burst Size (in Bytes)}
$$<p>For example, with an average bandwidth of 30 Mbps, a peak of 60 Mbps, and a burst duration of 0.1 seconds, the burst size would be:</p>
$$
\frac{{60000000} \text{(bits per second)} \times {0.1} \text{(seconds)}}{8} = 750000 \text{ bytes}
$$<p>Default value is 0, which disables rate limiting.</p>
<h3 id="ingress-broadcast">Ingress Broadcast</h3>
<p>Configures traffic shaping for broadcast traffic sent from the VM to the logical network.
Works similarly to the general ingress settings, allowing custom limits for average bandwidth, peak bandwidth, and burst size for broadcast traffic.
Default value is 0, which disables rate limiting for ingress broadcast traffic.</p>
<h3 id="egress">Egress</h3>
<p>Configures traffic shaping for inbound traffic from the logical network to the VM.
Allows setting limits on the average bandwidth, peak bandwidth, and burst size for inbound traffic.
Default value is 0, which disables rate limiting on egress traffic.
By configuring these parameters effectively, you can ensure traffic prioritization, manage congestion, and optimize bandwidth usage for both overlay-based and VLAN-based logical ports in NSX environments.</p>
<h2 id="test-scenario-evaluating-qos-at-the-segment-level">Test Scenario: Evaluating QoS at the segment level</h2>
<p>To demonstrate the differences between a setup with and without QoS, I have created a test environment consisting of two T1 routers, each connected to its own segment and hosting VMs for testing. Both T1 routers are connected to the same Tier-0 (T0) router, providing a shared Internet connection for testing north-south traffic scenarios.</p>

<figure><a href="topo.png"><picture>
          <source srcset="/nsx-qos/topo_hu6475854221281022681.webp" type="image/webp">
          <source srcset="/nsx-qos/topo_hu2349874414644969993.jpg" type="image/jpeg">
          <img src="/nsx-qos/topo_hu6475854221281022681.webp"alt="NSX Enviroment"  width="912"  height="936" />
        </picture></a><figcaption>
            <p>NSX Test Enviroment</p>
          </figcaption></figure>
<p>This test specifically focuses on <strong>QoS at the segment level</strong>, with the primary goal of limiting the VMs on the segment <code>LS-10.10.20.1</code> to a maximum bandwidth of <strong>30 Mbps</strong> using a QoS profile.</p>
<h3 id="t1-router-1-t1-bgp-no-qos"><strong>T1 Router 1: T1-BGP No QoS</strong></h3>
<ul>
<li><strong>Segment</strong>: <code>LS-10.10.10.1</code></li>
<li><strong>QoS Policy</strong>: None applied</li>
<li><strong>VM</strong>: <code>Alpine01</code> IP Adress <code>10.10.10.10</code>
<ul>
<li>Running an instance of iPerf to act as a traffic generator and receiver.</li>
</ul>
</li>
</ul>
<p>This router and segment represent a baseline configuration without any QoS policies, allowing for a comparison of unshaped and unprioritized traffic.</p>
<h3 id="t1-router-2-t1-bgp-qos"><strong>T1 Router 2: T1-BGP QoS</strong></h3>
<ul>
<li><strong>Segment</strong>: <code>LS-10.10.20.1</code></li>
<li><strong>QoS Policy</strong>: A custom QoS profile is applied to this segment, specifically configured to limit bandwidth to 30 Mbps for all associated VMs.</li>
<li><strong>VM</strong>: <code>Alpine02</code> IP Adresse <code>10.10.20.10</code>
<ul>
<li>Equipped with iPerf for traffic generation and reception.</li>
<li>Includes a browser for additional testing and validation purposes.</li>
</ul>
</li>
</ul>
<h3 id="purpose-of-the-test"><strong>Purpose of the Test</strong></h3>
<p>The primary goal of this test is to validate <strong>QoS at the segment level</strong>, focusing on the following:</p>
<ul>
<li>Verifying that VMs connected to <code>LS-10.10.20.1</code> are effectively limited to a bandwidth of 30 Mbps for egress traffic.</li>
<li>Demonstrating that the QoS profile, configured as ingress-only, limits traffic originating from <code>Alpine02</code> to other VMs or the Internet, while traffic from <code>Alpine01</code> to <code>Alpine02</code> remains unrestricted.</li>
<li>Comparing traffic behavior between a segment with and without an applied QoS profile.</li>
<li>Assessing performance consistency under traffic shaping policies.</li>
<li>Measuring the impact of the 30 Mbps limit on both east-west and north-south traffic.</li>
</ul>
<p>These behaviors will be demonstrated using iPerf measurements, highlighting the effectiveness and boundaries of the configured QoS profile.
By analyzing the test results, we can confirm the effectiveness of the QoS profile in limiting segment-level bandwidth and understand its implications for overall network performance.</p>
<h3 id="first-test-iperf-test-from-alpine02-to-alpine03">First Test: iPerf Test from Alpine02 to Alpine03</h3>
<h4 id="test-configuration"><strong>Test Configuration</strong></h4>
<ul>
<li><strong>Source</strong>: Alpine02 (<code>10.10.20.10</code>) connected to the segment <code>LS-10.10.20.1</code> with the QoS profile applied (ingress limited to 30 Mbps).</li>
<li><strong>Destination</strong>: Alpine03 (<code>10.10.10.10</code>) connected to the segment <code>LS-10.10.10.1</code> with no QoS policy applied.</li>
<li><strong>Tool</strong>: iPerf3</li>
<li><strong>Command</strong>: <code>iperf3 -c 10.10.10.10</code></li>
</ul>
<h4 id="result-summary"><strong>Result Summary</strong></h4>
<ul>
<li><strong>Average Sender Bitrate</strong>: 32.6 Mbps</li>
<li><strong>Average Receiver Bitrate</strong>: 30.2 Mbps</li>
<li><strong>Key Observation</strong>: The sender&rsquo;s bitrate fluctuates around the 30 Mbps mark, as expected due to the QoS ingress limitation applied to the LS-10.10.20.1 segment. Receiver bitrate is consistent with the QoS configuration, confirming that the profile effectively limits traffic from Alpine02 to Alpine03.</li>
</ul>
<pre tabindex="0"><code>alpine02:~# iperf3 -c 10.10.10.10
Connecting to host 10.10.10.10, port 5201
[  5] local 10.10.20.10 port 40468 connected to 10.10.10.10 port 5201
[ ID] Interval           Transfer     Bitrate         Retr  Cwnd
[  5]   0.00-1.00   sec  6.12 MBytes  51.3 Mbits/sec    0    522 KBytes       
[  5]   1.00-2.00   sec  4.25 MBytes  35.7 Mbits/sec    0    522 KBytes       
[  5]   2.00-3.00   sec  3.25 MBytes  27.3 Mbits/sec    0    522 KBytes       
[  5]   3.00-4.00   sec  3.12 MBytes  26.2 Mbits/sec    0    522 KBytes       
[  5]   4.00-5.00   sec  4.25 MBytes  35.7 Mbits/sec    0    522 KBytes       
[  5]   5.00-6.00   sec  3.12 MBytes  26.2 Mbits/sec    0    522 KBytes       
[  5]   6.00-7.00   sec  4.25 MBytes  35.6 Mbits/sec    0    522 KBytes       
[  5]   7.00-8.00   sec  3.12 MBytes  26.2 Mbits/sec    2    365 KBytes       
[  5]   8.00-9.00   sec  3.25 MBytes  27.3 Mbits/sec    0    365 KBytes       
[  5]   9.00-10.00  sec  4.12 MBytes  34.6 Mbits/sec    0    365 KBytes       
- - - - - - - - - - - - - - - - - - - - - - - - -
[ ID] Interval           Transfer     Bitrate         Retr
[  5]   0.00-10.00  sec  38.9 MBytes  32.6 Mbits/sec    2        sender
[  5]   0.00-10.00  sec  36.0 MBytes  30.2 Mbits/sec             receiver
</code></pre><h3 id="second-test-iperf-test-from-alpine01-to-alpine02">Second Test: iPerf Test from Alpine01 to Alpine02</h3>
<h4 id="test-configuration-1"><strong>Test Configuration</strong></h4>
<ul>
<li><strong>Source</strong>: Alpine01 (<code>10.10.10.10</code>) connected to the segment <code>LS-10.10.10.1</code> with no QoS policy applied.</li>
<li><strong>Destination</strong>: Alpine02 (<code>10.10.20.10</code>) connected to the segment <code>LS-10.10.20.1</code> with the QoS profile applied (ingress limited to 30 Mbps).</li>
<li><strong>Tool</strong>: iPerf3</li>
<li><strong>Command</strong>: <code>iperf3 -c 10.10.20.10</code></li>
</ul>
<h4 id="result-summary-1"><strong>Result Summary</strong></h4>
<ul>
<li><strong>Average Sender Bitrate</strong>: 2.25 Gbps</li>
<li><strong>Average Receiver Bitrate</strong>: 2.25 Gbps</li>
<li><strong>Key Observation</strong>: The traffic from Alpine01 to Alpine02 is not limited by the QoS profile, as expected. This confirms the QoS profile applies only to ingress traffic on the <code>LS-10.10.20.1</code> segment.</li>
</ul>
<pre tabindex="0"><code>alpine01:~# iperf3 -c 10.10.20.10
Connecting to host 10.10.20.10, port 5201
[  5] local 10.10.10.10 port 50482 connected to 10.10.20.10 port 5201
[ ID] Interval           Transfer     Bitrate         Retr  Cwnd
[  5]   0.00-1.00   sec   269 MBytes  2.25 Gbits/sec  150   2.08 MBytes       
[  5]   1.00-2.00   sec   268 MBytes  2.25 Gbits/sec    0   2.19 MBytes       
[  5]   2.00-3.00   sec   268 MBytes  2.25 Gbits/sec  172   1.21 MBytes       
[  5]   3.00-4.00   sec   267 MBytes  2.24 Gbits/sec  150    997 KBytes       
[  5]   4.00-5.00   sec   267 MBytes  2.24 Gbits/sec   11    673 KBytes       
[  5]   5.00-6.00   sec   268 MBytes  2.24 Gbits/sec    0    928 KBytes       
[  5]   6.00-7.00   sec   268 MBytes  2.25 Gbits/sec    0   1.10 MBytes       
[  5]   7.00-8.00   sec   268 MBytes  2.25 Gbits/sec    0   1.27 MBytes       
[  5]   8.00-9.00   sec   269 MBytes  2.25 Gbits/sec    0   1.42 MBytes       
[  5]   9.00-10.00  sec   268 MBytes  2.25 Gbits/sec    6   1.11 MBytes       
- - - - - - - - - - - - - - - - - - - - - - - - -
[ ID] Interval           Transfer     Bitrate         Retr
[  5]   0.00-10.00  sec  2.62 GBytes  2.25 Gbits/sec  489             sender
[  5]   0.00-10.01  sec  2.62 GBytes  2.25 Gbits/sec                  receiver

iperf Done.
</code></pre>
    <aside class="admonition attention">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" class="feather feather-link" width="24" height="24" viewBox="0 0 24 24"
      fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
      <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
      <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
   </svg></div><b>Note on QoS Profiles and Perspective</b>
        </div>
        <div class="admonition-content"><p>When working with QoS profiles on a segment, it is important to understand that the traffic shaping perspective is always <strong>from the segment&rsquo;s point of view</strong>. For example:</p>
<ul>
<li>A profile that shapes <strong>ingress traffic</strong> is applied to traffic <strong>entering the segment</strong>.</li>
<li>From the VM&rsquo;s perspective, this same traffic is considered <strong>egress traffic</strong> (leaving the VM).
This distinction can initially be confusing but is crucial for correctly interpreting and configuring QoS policies in NSX environments.</li>
</ul></div>
    </aside>
<h4 id="qos-profiles-apply-to-all-vms-on-the-segment"><strong>QoS Profiles Apply to All VMs on the Segment</strong></h4>
<p>QoS profiles applied at the segment level are effective for <strong>all VMs connected to that segment</strong>. In our example, this means:</p>
<ul>
<li>Every VM connected to the segment <code>LS-10.10.20.1</code> is limited to a maximum <strong>outgoing traffic rate of 30 Mbps</strong>.</li>
<li>The QoS profile ensures this bandwidth limit is enforced uniformly, regardless of the specific VM or traffic destination.</li>
</ul>
<p>This behavior highlights the segment-wide scope of QoS policies, making them a powerful tool for managing traffic flow consistently across all connected VMs.</p>
<h2 id="qos-on-t1-gateway-level">QoS on T1 Gateway Level</h2>
<p>To evaluate QoS at the Tier-1 Gateway level, the test conditions remain the same as in the segment-level QoS tests. However, the QoS profile will now be applied directly to the T1 Gateway. Before proceeding, the QoS profile is removed from the segment <code>LS-10.10.20.1</code>.</p>
<h2 id="qos-profile-configuration-on-t1-gateway"><strong>QoS Profile Configuration on T1 Gateway</strong></h2>
<p>For the T1 Gateway, the QoS profile is applied with the following characteristics:</p>
<ul>
<li><strong>Type</strong>: Ingress</li>
<li><strong>Committed Bandwidth</strong>: 30 Mbps</li>
<li><strong>Burst Size</strong>: Configured based on constraints (explained below).</li>
</ul>
<p>Unlike segment-level QoS, the T1 Gateway QoS profile allows only the configuration of <strong>Committed Bandwidth</strong> and <strong>Burst Size</strong> in bytes. The direction (Ingress or Egress) is explicitly specified when applying the profile to the gateway.</p>
<h3 id="limitations-of-gateway-qos-profiles"><strong>Limitations of Gateway QoS Profiles</strong></h3>
<ul>
<li><strong>Supported only on Tier-1 Gateways</strong>:
<ul>
<li>QoS profiles can only be applied to T1 Gateways, not to Tier-0 Gateways or any other components.</li>
</ul>
</li>
<li><strong>Applies only to North-South Traffic</strong>:
<ul>
<li>QoS policies on Tier-1 Gateways are limited to north-south traffic and do not affect overlay segments or service interfaces connected to the gateway.</li>
</ul>
</li>
<li><strong>Requires Active-Standby Mode</strong>:
<ul>
<li>The T1 Gateway must be in active-standby mode with an NSX Edge cluster for the QoS profile to function.</li>
</ul>
</li>
<li><strong>Not Supported for Distributed Routing</strong>:
<ul>
<li>Gateways configured for distributed routing cannot have QoS profiles applied.</li>
</ul>
</li>
</ul>
<h3 id="burst-size-calculation"><strong>Burst Size Calculation</strong></h3>
<p>The calculation of the <strong>Burst Size</strong> for a T1 Gateway is more complex due to additional constraints. The Burst Size must satisfy the following:</p>
<ol>
<li>
<p><strong>Token Refill per Interval</strong>:</p>
\[B \geq \frac{R \times 1000000 \times I}{1000 \times 8} \]<p>
Where:</p>
</li>
</ol>
<ul>
<li>\( B \): Burst Size in Bytes</li>
<li>\( R \): Committed Bandwidth in Mbps</li>
<li>\( I \): Refill Interval in milliseconds (e.g., 1 ms)</li>
</ul>
<ol start="2">
<li><strong>Minimum Refill Interval</strong>:
\[ B \geq \frac{R \times 1000000 \times 1}{1,000 \times 8} \]</li>
</ol>
<ul>
<li>The minimum interval \( I \) is 1 ms to account for dataplane CPU usage.</li>
</ul>
<ol start="3">
<li><strong>MTU Constraint</strong>:
\[ B \geq MTU \] of the Service Router (SR) port.</li>
</ol>
<ul>
<li>The Burst Size must accommodate at least one full MTU-size packet.</li>
</ul>
<p>The effective Burst Size must satisfy all three constraints. Therefore, the configured Burst Size is determined as:</p>
\[
B = \text{Max} \left( \frac{R \times 1000000 \times I}{1000 \times 8}, \frac{R \times 1000000 \times 1}{1000 \times 8}, MTU \right)
\]<h3 id="burst-size-calculation-example">Burst Size Calculation Example</h3>
<h4 id="parameters"><strong>Parameters</strong></h4>
<ul>
<li>\( R \): <strong>Committed Bandwidth</strong> = 30 Mbps</li>
<li>\( I \): <strong>Refill Interval</strong> = 1000 ms (1 second)</li>
<li>\( MTU \): <strong>Maximum Transmission Unit</strong> = 1500 bytes</li>
</ul>
<p>The Burst Size \( B \) must satisfy the following constraints:</p>
<h4 id="1-token-refill-per-interval"><strong>1. Token Refill per Interval</strong></h4>
\[
B \geq \frac{R \times 1000000 \times I}{1000 \times 8}
\]<p>
Substitute the values:
</p>
\[
B \geq \frac{30 \times 1000000 \times 1000}{1000 \times 8}
\]\[
B \geq \frac{30000000000}{8000}
\]\[
B \geq 3750000 \, \text{bytes}
\]<h4 id="2-minimum-refill-interval"><strong>2. Minimum Refill Interval</strong></h4>
\[
B \geq \frac{R \times 1000000 \times 1000}{1000 \times 8}
\]<p>
This calculation remains the same as in the previous case since \( I = 1000 \, \text{ms} \):
</p>
\[
B \geq 3750000 \, \text{bytes}
\]<h4 id="3-mtu-constraint"><strong>3. MTU Constraint</strong></h4>
\[
B \geq MTU
\]\[
B \geq 1500 \, \text{bytes}
\]<h4 id="final-burst-size"><strong>Final Burst Size</strong></h4>
<p>The Burst Size \( B \) must satisfy <strong>all three constraints</strong>, so:
</p>
\[
B = \text{Max}(3750000, 3750000, 1500)
\]\[
B = 3750000 \, \text{bytes}
\]<h3 id="result"><strong>Result</strong></h3>
<p>The minimum Burst Size required is <strong>3750000 bytes</strong> to satisfy all constraints with the given parameters.</p>

    <aside class="admonition info">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-info">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="16" x2="12" y2="12"></line>
      <line x1="12" y1="8" x2="12.01" y2="8"></line>
   </svg></div><b>Minimum Refill Interval</b>
        </div>
        <div class="admonition-content">Use the <code>get dataplane</code> command from the NSX Edge CLI to retrieve the time interval, Qos_wakeup_interval_ms. The default value for Qos_wakeup_interval_ms is 50ms. However, this value is automatically adjusted by the dataplane based on the QoS configuration. In my lab, the QoS_wakeup_interval is relatively high, which is partly due to my hardware and the fact that it is a nested lab. In production environments, this value is typically lower.</div>
    </aside>
<h3 id="implementation-for-this-test"><strong>Implementation for This Test</strong></h3>
<ul>
<li>The T1 Gateway is configured with a 30 Mbps Committed Bandwidth and a Burst Size that satisfies the constraints above.</li>
<li>The profile is applied in <strong>Ingress</strong> mode to test incoming north-south traffic through the T1 Gateway.</li>
</ul>

<figure><a href="t1qos.png"><picture>
          <source srcset="/nsx-qos/t1qos_hu14467825683727384270.webp" type="image/webp">
          <source srcset="/nsx-qos/t1qos_hu5352979924153472324.jpg" type="image/jpeg">
          <img src="/nsx-qos/t1qos_hu14467825683727384270.webp"alt="T1 Qos Profile"  width="1433"  height="821" />
        </picture></a><figcaption>
            <p>T1 Qos Profile</p>
          </figcaption></figure>
<p>This setup will help analyze how traffic shaping and rate limiting function at the T1 Gateway level compared to the segment-level QoS.</p>
<h2 id="first-test-iperf-test-from-alpine01-to-alpine02-t1-gateway-qos">First Test: iPerf Test from Alpine01 to Alpine02 (T1 Gateway QoS)</h2>
<h3 id="test-configuration-2"><strong>Test Configuration</strong></h3>
<ul>
<li><strong>Source</strong>: Alpine01 (<code>10.10.10.10</code>) connected to the segment <code>LS-10.10.10.1</code> with no QoS profile applied.</li>
<li><strong>Destination</strong>: Alpine02 (<code>10.10.20.10</code>) connected to the segment <code>LS-10.10.20.1</code>.</li>
<li><strong>QoS Profile</strong>: Applied to the T1 Gateway with:
<ul>
<li><strong>Type</strong>: Ingress</li>
<li><strong>Committed Bandwidth</strong>: 30 Mbps</li>
<li><strong>Burst Size</strong>: Configured according to the calculated constraints.</li>
</ul>
</li>
<li><strong>Tool</strong>: iPerf3</li>
<li><strong>Command</strong>: <code>iperf3 -c 10.10.20.10</code></li>
</ul>
<pre tabindex="0"><code>alpine01:~# iperf3 -c 10.10.20.10 
Connecting to host 10.10.20.10, port 5201
[  5] local 10.10.10.10 port 33292 connected to 10.10.20.10 port 5201
[ ID] Interval           Transfer     Bitrate         Retr  Cwnd
[  5]   0.00-1.00   sec  7.25 MBytes  60.8 Mbits/sec  283   5.66 KBytes       
[  5]   1.00-2.00   sec  3.25 MBytes  27.3 Mbits/sec  369   5.66 KBytes       
[  5]   2.00-3.00   sec  3.00 MBytes  25.2 Mbits/sec  233   52.3 KBytes       
[  5]   3.00-4.00   sec  3.75 MBytes  31.5 Mbits/sec  392   7.07 KBytes       
[  5]   4.00-5.00   sec  3.12 MBytes  26.2 Mbits/sec  284   41.0 KBytes       
[  5]   5.00-6.00   sec  3.88 MBytes  32.5 Mbits/sec  356   8.48 KBytes       
[  5]   6.00-7.00   sec  3.25 MBytes  27.3 Mbits/sec  270   7.07 KBytes       
[  5]   7.00-8.00   sec  3.62 MBytes  30.4 Mbits/sec  315    110 KBytes       
[  5]   8.00-9.00   sec  3.50 MBytes  29.4 Mbits/sec  407   7.07 KBytes       
[  5]   9.00-10.00  sec  3.00 MBytes  25.2 Mbits/sec  245   12.7 KBytes       
- - - - - - - - - - - - - - - - - - - - - - - - -
[ ID] Interval           Transfer     Bitrate         Retr
[  5]   0.00-10.00  sec  37.6 MBytes  31.6 Mbits/sec  3154             sender
[  5]   0.00-10.00  sec  37.1 MBytes  31.1 Mbits/sec                  receiver

iperf Done.
</code></pre><h3 id="result-summary-2">Result Summary</h3>
<ul>
<li>Average Sender Bitrate: 31.6 Mbps</li>
<li>Average Receiver Bitrate: 31.1 Mbps</li>
<li>Total Retransmissions: 3154</li>
</ul>
<h2 id="second-test-iperf-test-from-alpine02-to-alpine03-t1-gateway-qos">Second Test: iPerf Test from Alpine02 to Alpine03 (T1 Gateway QoS)</h2>
<h3 id="test-configuration-3"><strong>Test Configuration</strong></h3>
<ul>
<li><strong>Source</strong>: Alpine02 (<code>10.10.20.10</code>) connected to the segment <code>LS-10.10.20.1</code> with no QoS profile applied.</li>
<li><strong>Destination</strong>: Alpine03 (<code>10.10.10.10</code>) connected to the segment <code>LS-10.10.10.1</code> with no QoS profile applied.</li>
<li><strong>QoS Profile</strong>: Applied to the T1 Gateway in <strong>Ingress</strong> mode, limiting traffic to 30 Mbps for ingress traffic  from T0 to the T1 gateway.</li>
<li><strong>Tool</strong>: iPerf3</li>
<li><strong>Command</strong>: <code>iperf3 -c 10.10.10.10</code></li>
</ul>
<pre tabindex="0"><code>alpine02:~# iperf3 -c 10.10.10.10
Connecting to host 10.10.10.10, port 5201
[  5] local 10.10.20.10 port 50200 connected to 10.10.10.10 port 5201
[ ID] Interval           Transfer     Bitrate         Retr  Cwnd
[  5]   0.00-1.00   sec   249 MBytes  2.09 Gbits/sec    5    615 KBytes       
[  5]   1.00-2.00   sec   267 MBytes  2.24 Gbits/sec    0    885 KBytes       
[  5]   2.00-3.00   sec   159 MBytes  1.34 Gbits/sec   73    290 KBytes       
[  5]   3.00-4.00   sec   248 MBytes  2.08 Gbits/sec    0    677 KBytes       
[  5]   4.00-5.00   sec   261 MBytes  2.19 Gbits/sec    0    928 KBytes       
[  5]   5.00-6.00   sec   264 MBytes  2.22 Gbits/sec    1    792 KBytes       
[  5]   6.00-7.00   sec   260 MBytes  2.18 Gbits/sec   26    441 KBytes       
[  5]   7.00-8.00   sec   257 MBytes  2.16 Gbits/sec    0    766 KBytes       
[  5]   8.00-9.00   sec   262 MBytes  2.20 Gbits/sec    7    585 KBytes       
[  5]   9.00-10.00  sec   262 MBytes  2.19 Gbits/sec    0    858 KBytes       
- - - - - - - - - - - - - - - - - - - - - - - - -
[ ID] Interval           Transfer     Bitrate         Retr
[  5]   0.00-10.00  sec  2.43 GBytes  2.09 Gbits/sec  112             sender
[  5]   0.00-10.00  sec  2.43 GBytes  2.09 Gbits/sec                  receiver
</code></pre><h3 id="result-summary-3">Result Summary</h3>
<ul>
<li>Average Sender Bitrate: 2.09 Gbps</li>
<li>Average Receiver Bitrate: 2.09 Gbps</li>
<li>Total Retransmissions: 112</li>
</ul>
<h2 id="third-test-iperf-test-from-alpine01-to-alpine02-concurrent-traffic">Third Test: iPerf Test from Alpine01 to Alpine02 (Concurrent Traffic)</h2>
<h3 id="test-configuration-4"><strong>Test Configuration</strong></h3>
<ul>
<li><strong>Source</strong>: Alpine01 (<code>10.10.10.10</code>) connected to the segment <code>LS-10.10.10.1</code> with no QoS profile applied.</li>
<li><strong>Destination</strong>: Alpine02 (<code>10.10.20.10</code>) connected to the segment <code>LS-10.10.20.1</code>.</li>
<li><strong>Additional Traffic</strong>: A second VM, connected to the same T1 Gateway as Alpine02, is concurrently receiving data to simulate shared bandwidth conditions.</li>
<li><strong>QoS Profile</strong>: Applied to the T1 Gateway in <strong>Ingress</strong> mode, limiting traffic to 30 Mbps for ingress traffic to the gateway.</li>
<li><strong>Tool</strong>: iPerf3</li>
<li><strong>Command</strong>: <code>iperf3 -c 10.10.20.10</code></li>
</ul>
<pre tabindex="0"><code>alpine01:~# iperf3 -c 10.10.20.10 
Connecting to host 10.10.20.10, port 5201
[  5] local 10.10.10.10 port 40898 connected to 10.10.20.10 port 5201
[ ID] Interval           Transfer     Bitrate         Retr  Cwnd
[  5]   0.00-1.00   sec  2.88 MBytes  24.1 Mbits/sec  475   8.48 KBytes       
[  5]   1.00-2.00   sec  2.50 MBytes  21.0 Mbits/sec  212   53.7 KBytes       
[  5]   2.00-3.00   sec  2.12 MBytes  17.8 Mbits/sec  233   9.90 KBytes       
[  5]   3.00-4.00   sec  2.12 MBytes  17.8 Mbits/sec  145   1.41 KBytes       
[  5]   4.00-5.00   sec  2.50 MBytes  21.0 Mbits/sec  237   14.1 KBytes       
[  5]   5.00-6.00   sec  1.62 MBytes  13.6 Mbits/sec  145   22.6 KBytes       
[  5]   6.00-7.00   sec  3.12 MBytes  26.2 Mbits/sec  331   5.66 KBytes       
[  5]   7.00-8.00   sec  1.00 MBytes  8.39 Mbits/sec  155   7.07 KBytes       
[  5]   8.00-9.00   sec  2.62 MBytes  22.0 Mbits/sec  142   86.3 KBytes       
[  5]   9.00-10.00  sec  3.12 MBytes  26.2 Mbits/sec  349   22.6 KBytes       
- - - - - - - - - - - - - - - - - - - - - - - - -
[ ID] Interval           Transfer     Bitrate         Retr
[  5]   0.00-10.00  sec  23.6 MBytes  19.8 Mbits/sec  2424             sender
[  5]   0.00-10.00  sec  22.4 MBytes  18.8 Mbits/sec                  receiver
</code></pre><h3 id="result-summary-4">Result Summary</h3>
<ul>
<li>Average Sender Bitrate: 19.8 Mbps</li>
<li>Average Receiver Bitrate: 18.8 Mbps</li>
<li>Total Retransmissions: 2424</li>
<li>The QoS profile at the T1 Gateway effectively limits total ingress bandwidth to 30 Mbps. However, the concurrent traffic from the second VM reduces the available bandwidth for Alpine01.</li>
</ul>

    <aside class="admonition attention">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" class="feather feather-link" width="24" height="24" viewBox="0 0 24 24"
      fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
      <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
      <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
   </svg></div><b>Note on QoS Profiles and Perspective</b>
        </div>
        <div class="admonition-content"><p>For a QoS profile applied at the gateway level, the <strong>Ingress</strong> direction refers to traffic entering the Tier-1 (T1) Gateway from the Tier-0 (T0) Gateway. This means:</p>
<ul>
<li>From the perspective of a VM, this is indeed <strong>ingress traffic</strong>, as it refers to traffic arriving at the VM via the T1 Gateway.</li>
<li>The <strong>egress traffic</strong> of a VM (traffic leaving the VM) is not affected by an ingress QoS profile applied at the T1 Gateway.
This distinction ensures that the QoS profile at the gateway level is only applied to traffic coming from the T0 to the T1, without influencing outgoing traffic generated by the VM.</li>
</ul></div>
    </aside>
<h2 id="summary">Summary</h2>
<p>When working with QoS profiles in NSX, it is important to understand the key differences and use cases for <strong>Gateway QoS Profiles</strong> and <strong>Segment QoS Profiles</strong>:</p>
<h3 id="gateway-qos-profiles"><strong>Gateway QoS Profiles</strong></h3>
<ul>
<li><strong>Shared Bandwidth</strong>: The configured bandwidth applies to the <strong>total traffic</strong> for all VMs connected to the same T1 Gateway. This includes all segments attached to that gateway.</li>
<li><strong>Practical Use Case</strong>: Gateway QoS profiles are ideal for scenarios like test environments, where you want to limit the total available bandwidth across all VMs and segments.</li>
<li><strong>Traffic Direction</strong>: The QoS direction is critical:
<ul>
<li><strong>Ingress QoS</strong>: Limits traffic entering the T1 Gateway (from T0 to T1), affecting ingress traffic from the VM&rsquo;s perspective.</li>
<li><strong>Egress QoS</strong>: Limits traffic leaving the T1 Gateway (from T1 to T0), affecting egress traffic from the VM&rsquo;s perspective.</li>
</ul>
</li>
</ul>
<h3 id="segment-qos-profiles"><strong>Segment QoS Profiles</strong></h3>
<ul>
<li><strong>Individual Bandwidth Allocation</strong>: Each VM connected to the segment receives the bandwidth specified in the profile. VMs do <strong>not share</strong> the bandwidth; they each receive the assigned limit (assuming the total environment can provide the required bandwidth).</li>
<li><strong>Flexibility</strong>: Segment QoS profiles offer more granular control and can be used for more than just rate limiting. For example, they can prioritize or shape specific types of traffic.</li>
<li><strong>Traffic Direction</strong>: The direction in the profile (Ingress or Egress) must be carefully considered based on what you want to achieve.</li>
</ul>
<h3 id="key-considerations"><strong>Key Considerations</strong></h3>
<ul>
<li><strong>Shared vs. Dedicated Bandwidth</strong>: Use Gateway QoS Profiles when you want to manage total bandwidth collectively for all VMs. Use Segment QoS Profiles when you need to allocate specific bandwidth to individual VMs.</li>
<li><strong>Performance Impact</strong>: Avoid using Gateway QoS Profiles in scenarios requiring high performance and scalability. The active/standby limitation can create bottlenecks, making distributed T1 Gateways the better choice for performance-critical environments.</li>
</ul>
<p>By understanding these differences, you can effectively apply QoS profiles to achieve desired traffic shaping and bandwidth management goals in your NSX environment.</p>
]]></content>
		</item>
		
		<item>
			<title>NSX 4.2.1.1 Hotfix Update</title>
			<link>https://sdn-warrior.org/posts/nsx4_2_1_1/</link>
			<pubDate>Mon, 09 Dec 2024 14:55:00 +0100</pubDate>
			
			<guid>https://sdn-warrior.org/posts/nsx4_2_1_1/</guid>
			<description><![CDATA[short summary of the NSX update 4.2.1.1]]></description>
			<content type="html"><![CDATA[<p>The latest NSX update delivers a comprehensive set of fixes to enhance stability, performance, and security. Here’s a summary of the resolved issues and their impact:</p>
<h2 id="1-enhanced-stability-for-virtual-environments">1. Enhanced Stability for Virtual Environments</h2>
<ul>
<li>Loss of IP Bindings after VMotion (Issue 3453866): Addressed the removal of IP bindings and logical ports associated with VMs during vMotion events.</li>
<li>Critical ESXi Errors with UENS (Issue 3456283): Fixed intermittent PSODs caused by control priority filter lookups, ensuring smoother ESXi operations.</li>
<li>Portgroup Creation Issue (Issue 3458111): Resolved the creation of additional portgroups during full sync, preventing potential vCenter crashes.</li>
<li>Transport Zone Reference Issue (Issue 3454291): Fixed transport zone profile mismatches, restoring vMotion and service functionality.</li>
</ul>
<h2 id="2-improved-network-performance">2. Improved Network Performance</h2>
<ul>
<li>TCP Packet Drops in EDP (Issue 3457047): Resolved issues causing TCP connection drops when using Enhanced Datapath configurations.</li>
<li>Packet Reordering with LRO (Issue 3456533): Fixed packet reordering issues when HW Large Receive Offload is enabled, improving TCP throughput.</li>
<li>Reduced Traffic Performance with UENS and LRO (Issue 3456289): Addressed performance degradation in VSAN workloads.</li>
</ul>
<h2 id="3-robust-security-and-monitoring">3. Robust Security and Monitoring</h2>
<ul>
<li>NSX UI Alarm for Metrics Delivery Failure (Issue 3456663): Fixed authentication issues following certificate changes to restore metrics delivery.</li>
<li>IDPS and TLS Prevention (Issue 3458040): Enhanced malicious traffic prevention by resolving decryption issues with IDPS.</li>
<li>IDPS Events and Certificate Verification (Issue 3458038): Restored the flow of IDPS events to Security Intelligence by fixing Kafka channel errors.</li>
</ul>
<h2 id="4-stability-in-upgrades-and-configurations">4. Stability in Upgrades and Configurations</h2>
<ul>
<li>NSX Manager Slowness (Issue 3453882): Resolved slowness and instability in NSX Manager post-upgrade.</li>
<li>Edge Node IP Table Rules (Issue 3452795): Ensured proper application of IP table rules on Edge nodes.</li>
<li>NSX Configuration Realization (Issue 3452794): Fixed issues preventing configuration realization on Transport Nodes.</li>
</ul>
<h2 id="5-enhancements-in-distributed-firewall-and-flow-management">5. Enhancements in Distributed Firewall and Flow Management</h2>
<ul>
<li>DFW Rules During Upgrade (Issue 3450247): Mitigated periods where DFW rules were disabled during the upgrade process.</li>
<li>Flow Exporter Alarms (Issues 3429787, 3456644): Fixed alarms and restored flow export functionality for Security Intelligence.</li>
</ul>
<h2 id="6-overlay-and-connectivity-improvements">6. Overlay and Connectivity Improvements</h2>
<ul>
<li>Overlay Segment Connectivity (Issue 3450019): Addressed connectivity loss in Overlay Segments when Edge TEP groups were enabled.</li>
</ul>
<h2 id="conclusion">Conclusion</h2>
<p>This NSX update resolves critical issues to improve operational reliability, security, and performance in virtual environments. For a seamless experience, upgrading to this release is highly recommended. As always, thorough testing in a staging environment before deployment in production is advised.</p>
]]></content>
		</item>
		
		<item>
			<title>iSCSI Tuning</title>
			<link>https://sdn-warrior.org/posts/iscsi-tuning/</link>
			<pubDate>Sun, 08 Dec 2024 12:21:20 +0100</pubDate>
			
			<guid>https://sdn-warrior.org/posts/iscsi-tuning/</guid>
			<description><![CDATA[Optimizing iSCSI Performance in an Unraid Environment]]></description>
			<content type="html"><![CDATA[<p>In my setup, I use iSCSI in combination with Unraid to create a DIY block storage solution. Unraid, with its flexibility, serves as the foundation, and I utilize the Linux iSCSI implementation installed via a plugin to enable block-level storage.</p>
<p>For my setup, I use an Intel NUC of the 13th generation, equipped with two 2.5G network adapters. These provide the necessary connectivity for storage traffic. I configured two VMkernel (VMK) adapters specifically for iSCSI traffic, ensuring redundancy and optimized throughput.</p>
<p>To further enhance performance, I’ve implemented several optimizations, including fine-tuning settings on my ESXi servers.</p>
<h2 id="optimize-maxiosizekb">Optimize MaxIoSizeKB</h2>
<p>One such optimization involves adjusting the maximum I/O size for iSCSI traffic.</p>

    <aside class="admonition info">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-info">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="16" x2="12" y2="12"></line>
      <line x1="12" y1="8" x2="12.01" y2="8"></line>
   </svg></div><b>Info</b>
        </div>
        <div class="admonition-content">By default, VMware ESXi uses a <em><strong>MaxIoSizeKB</strong></em> value of 128 KB.</div>
    </aside>
<p>While this is sufficient for many setups, it may not be optimal for environments
like mine, where jumbo frames are enabled across the network. Larger packets perform better in such a configuration, reducing overhead and increasing throughput.</p>
<p>To take advantage of my network&rsquo;s capabilities, I increased the <em><strong>MaxIoSizeKB</strong></em> parameter to 512 KB</p>
<p>To configure this, I ran the following command on my ESXi host:</p>
<pre tabindex="0"><code>esxcli system settings advanced set -o /ISCSI/MaxIoSizeKB -i 512
</code></pre><p>This change allows the iSCSI initiator to send larger I/O requests, improving data transfer efficiency in my jumbo-frame-enabled network. With this configuration, I noticed a significant improvement in performance, as the network could handle larger blocks of data more effectively.</p>

    <aside class="admonition attention">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" class="feather feather-link" width="24" height="24" viewBox="0 0 24 24"
      fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
      <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
      <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
   </svg></div><b>Attention</b>
        </div>
        <div class="admonition-content">For this change to take effect, a host reboot is required. After restarting the ESXi server, the new value will be applied, enabling the iSCSI initiator to send larger I/O requests.</div>
    </aside>
<p>After the reboot, you can verify that the change has been successfully applied by running the following command:</p>
<pre tabindex="0"><code>esxcli system settings advanced list -o /ISCSI/MaxIoSizeKB
</code></pre><p>The output should look like this:</p>
<pre tabindex="0"><code>[root@esxnuc1:~] esxcli system settings advanced list -o /ISCSI/MaxIoSizeKB
   Path: /ISCSI/MaxIoSizeKB
   Type: integer
   Int Value: 512
   Default Int Value: 128
   Min Value: 128
   Max Value: 512
   String Value: 
   Default String Value: 
   Valid Characters: 
   Description: Maximum Software iSCSI I/O size (in KB) (REQUIRES REBOOT!)
   Host Specific: false
   Impact: reboot
</code></pre><h2 id="optimize-multipathing">Optimize multipathing</h2>
<p>To optimize performance, I configured Round Robin as the multipathing policy for my iSCSI volumes on the ESXi server. This ensures better load distribution and failover capabilities. The configuration can be applied via the ESXi CLI as follows:</p>
<ul>
<li>List all connected storage devices to identify the target naa or eui identifier:</li>
</ul>
<pre tabindex="0"><code>esxcli storage nmp device list
</code></pre><ul>
<li>Output</li>
</ul>
<pre tabindex="0"><code>naa.60014058f1117188efe49cb8b5de2273
   Device Display Name: LIO-ORG iSCSI Disk (naa.60014058f1117188efe49cb8b5de2273)
   Storage Array Type: VMW_SATP_ALUA
   Storage Array Type Device Config: {implicit_support=on; explicit_support=on; explicit_allow=on; alua_followover=on; action_OnRetryErrors=on; {TPG_id=0,TPG_state=AO}}
   Path Selection Policy: VMW_PSP_MRU
   Path Selection Policy Device Config: {policy=iops,iops=1000,bytes=10485760,useANO=0; lastPathIndex=0: NumIOsPending=0,numBytesPending=0}
   Path Selection Policy Device Custom Config: policy=iops;iops=1000;bytes=10485760;samplingCycles=16;latencyEvalTime=180000;useANO=0;
   Working Paths: vmhba64:C1:T0:L1, vmhba64:C0:T0:L1
   Is USB: false
</code></pre>
    <aside class="admonition info">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-info">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="16" x2="12" y2="12"></line>
      <line x1="12" y1="8" x2="12.01" y2="8"></line>
   </svg></div><b>Info</b>
        </div>
        <div class="admonition-content"><!-- raw HTML omitted --> The default multipath policy is &ldquo;Most Recently Used&rdquo; (VM_PSP_MRU).</div>
    </aside>
<ul>
<li>Set the multipathing policy for the desired iSCSI device to RoundRobin:</li>
</ul>
<pre tabindex="0"><code>esxcli storage nmp device set --device &lt;DeviceIdentifier&gt; --psp VMW_PSP_RR
</code></pre><ul>
<li>Verify that the policy has been successfully applied:</li>
</ul>
<pre tabindex="0"><code>esxcli storage nmp device list | grep &lt;DeviceIdentifier&gt;
</code></pre>
    <aside class="admonition info">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-info">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="16" x2="12" y2="12"></line>
      <line x1="12" y1="8" x2="12.01" y2="8"></line>
   </svg></div><b>Info</b>
        </div>
        <div class="admonition-content">Replace <!-- raw HTML omitted --> with the actual identifier of your iSCSI device (naa.xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx).&quot;</div>
    </aside>
<ul>
<li>Output after chages</li>
</ul>
<pre tabindex="0"><code>naa.60014058f1117188efe49cb8b5de2273
   Device Display Name: LIO-ORG iSCSI Disk (naa.60014058f1117188efe49cb8b5de2273)
   Storage Array Type: VMW_SATP_ALUA
   Storage Array Type Device Config: {implicit_support=on; explicit_support=on; explicit_allow=on; alua_followover=on; action_OnRetryErrors=on; {TPG_id=0,TPG_state=AO}}
   Path Selection Policy: VMW_PSP_RR
   Path Selection Policy Device Config: {policy=iops,iops=1000,bytes=10485760,useANO=0; lastPathIndex=0: NumIOsPending=0,numBytesPending=0}
   Path Selection Policy Device Custom Config: policy=iops;iops=1000;bytes=10485760;samplingCycles=16;latencyEvalTime=180000;useANO=0;
   Working Paths: vmhba64:C1:T0:L1, vmhba64:C0:T0:L1
   Is USB: false
</code></pre>
    <aside class="admonition info">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-info">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="16" x2="12" y2="12"></line>
      <line x1="12" y1="8" x2="12.01" y2="8"></line>
   </svg></div><b>Info</b>
        </div>
        <div class="admonition-content">the multipath policy must now be VMW_PSP_RR</div>
    </aside>
<p>To further optimize path usage and load distribution, I adjusted the IOPS parameter for the Round Robin policy. By default, ESXi switches storage paths after 1000 I/O operations, but I used the following command snippet to change this behavior to switch after every single I/O:</p>
<pre tabindex="0"><code>for i in `esxcfg-scsidevs -c |awk &#39;{print $1}&#39; | grep naa.xxxx`; do 
   esxcli storage nmp psp roundrobin deviceconfig set --type=iops --iops=1 --device=$i
done
</code></pre><p>Where .xxxx matches the first few characters of your NAA IDs. Reducing the IOPS value from 1000 to 1 means the ESXi host will alternate between available paths much more frequently. In practice, this can help evenly distribute the workload across all paths, potentially improving overall responsiveness and performance.</p>
<p>However, when combined with changes like increasing MaxIoSizeKB, the outcome can vary. In some cases, this adjustment may yield better results, while in others it could degrade performance. Therefore, it’s crucial to test these parameters individually for each storage environment to determine the most effective configuration.</p>
<h2 id="why-jumbo-frames-matter-bonus">Why Jumbo Frames Matter (bonus)</h2>
<p>Jumbo frames allow Ethernet frames larger than the standard 1500 bytes to be transmitted, reducing the total number of frames required to send the same amount of data. This results in lower CPU overhead and better performance, particularly in high-bandwidth and storage-intensive environments. However, it’s essential to ensure that every device in the network path—NICs, switches, and storage systems—supports and is configured for jumbo frames for optimal performance.</p>
<p>To verify that jumbo frames are functioning correctly in your environment, you can use the <em><strong>vmkping</strong></em> command on your ESXi host:</p>
<pre tabindex="0"><code>vmkping -I vmk1 -s 8973 -d 192.168.67.250
</code></pre>
    <aside class="admonition info">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-info">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="16" x2="12" y2="12"></line>
      <line x1="12" y1="8" x2="12.01" y2="8"></line>
   </svg></div><b> vmkping parameters</b>
        </div>
        <div class="admonition-content"><ul>
<li>Replace <em><strong>vmk1</strong></em> with the VMkernel adapter used for iSCSI.</li>
<li><em><strong>-s 8972</strong></em> sets the packet size to match the jumbo frame size (8972 bytes, including headers).</li>
<li><em><strong>-d</strong></em> enables the Don&rsquo;t Fragment flag to ensure the packet isn&rsquo;t fragmented along the way.</li>
</ul>
</div>
    </aside>
<p>If you see the error:  <em><strong>sendto() failed (Message too long)</strong></em> this indicates that the packet size is too large to be transmitted without fragmentation. For a setup with an MTU of 9000 configured on the distributed or standard switch, a packet size of 8972 bytes should work correctly. If the error occurs, check your vswitch settings, your physical switches and your iSCSI target.</p>
<h2 id="conclusion">Conclusion</h2>
<p>By increasing the MaxIoSizeKB to 512 KB, verifying jumbo frame functionality with vmkping, and enabling Round Robin, I optimized my iSCSI setup to leverage the full potential of my 2.5G network.</p>

<figure><picture>
          <source srcset="/iscsi-tuning/test_hu12020423995241410634.webp" type="image/webp">
          <source srcset="/iscsi-tuning/test_hu16349894886800222160.jpg" type="image/jpeg">
          <img src="/iscsi-tuning/test_hu12020423995241410634.webp"alt="CriytalDiskMark Benchmark"  width="479"  height="351" />
        </picture><figcaption>
            <p>CriytalDiskMark Benchmark</p>
          </figcaption></figure>
<p>In my tests with CrystalDiskMark, I observed that both network adapters showed significant performance improvements as a result of these optimizations. These adjustments allow my Unraid and iSCSI configuration to deliver a robust and high-performance block storage solution tailored to my workloads.</p>
<h2 id="disclaimer">Disclaimer</h2>
<p>The settings and configurations described in this article are specific to my environment and were tested extensively within my setup. While these adjustments significantly improved performance for my use case, they may not be universally applicable. It’s essential to test these settings in your environment before implementing them, as results may vary depending on hardware, network, and workload specifics. These optimizations are not intended as a blanket recommendation.</p>
]]></content>
		</item>
		
		<item>
			<title>MAC Learning is your friend</title>
			<link>https://sdn-warrior.org/posts/mac-learning/</link>
			<pubDate>Wed, 27 Nov 2024 19:54:18 +0100</pubDate>
			
			<guid>https://sdn-warrior.org/posts/mac-learning/</guid>
			<description><![CDATA[Why you should use MAC Learning]]></description>
			<content type="html"><![CDATA[<h2 id="intro">Intro</h2>
<p>When working with nested ESXi environments, understanding the interplay between MAC Learning, Promiscuous Mode, and Forged Transmits is critical. These settings significantly affect how traffic flows in virtualized networks, especially in scenarios involving virtualized hypervisors or advanced network configurations.</p>
<ul>
<li>
<p>MAC Learning: Think of it as a switch-like behavior in your virtual environment. It optimizes network traffic by ensuring that each virtual machine (VM) receives only the packets meant for its MAC address.
Without MAC learning, when the ESXi VM&rsquo;s vNIC connects to a switch port, it only contains a static MAC address.
<a href="https://docs.vmware.com/en/VMware-vSphere/8.0/vsphere-networking/GUID-E0246B3D-9FB1-4976-8217-5C085863EA9A.html">(more Information about MAC learning)</a></p>
</li>
<li>
<p>Promiscuous Mode: On the other hand, this allows a VM or virtual switch to capture all traffic on a port group, whether addressed to it or not. It&rsquo;s a useful feature for troubleshooting and monitoring but comes with potential security and performance implications.</p>
</li>
<li>
<p>Forged Transmits: Forged Transmits plays a complementary role in this configuration. It ensures that traffic originating from a VM with a source MAC address different from its assigned MAC address is allowed to leave the virtual switch. This is crucial in nested environments.</p>
</li>
</ul>
<h2 id="lab-environment">Lab environment</h2>
<p>In this lab environment, I am using two Minisforum MS-01 workstations, each equipped with ESXi 8.0.3 as the hypervisor. These compact systems provide a balance of performance and energy efficiency, fitting perfectly into my goal of maintaining a powerful yet quiet setup.</p>
<p>Each workstation is interconnected via dual 10 Gb/s network links, ensuring high-speed communication with minimal latency. This setup is particularly advantageous for simulating complex network scenarios and nested virtualization environments.</p>
<p>On each workstation, a nested ESXi host is deployed. These nested hosts act as virtualized hypervisors for a future VCF deployment.</p>
<h2 id="the-problems-ive-caused">The problems I&rsquo;ve caused</h2>
<p>In my previous lab setups, Promiscuous Mode was my go-to solution for nested virtualization. It was reliable, simple to configure, and worked flawlessly for years. While I was aware of the security risks associated with it, in a controlled homelab environment, those risks were not a significant concern.</p>
<p>However, everything changed when I upgraded my lab to dual 10 Gb/s network links and, powered by the i9 CPU, gained the ability to run multiple nested ESXi hosts on a single physical machine.
One of the first challenges I encountered was during the configuration of a vSAN port group for my nested ESXi hosts. This port group was configured to use Active/Active load balancing across both 10 Gb/s uplinks on the MS-01 workstations.
Almost immediately, I noticed unexpected performance issues. Nested VMs were experiencing slow network speeds, and vSAN operations were significantly hindered. Initially, I struggled to pinpoint the root cause. Given my past success with Promiscuous Mode, I didn’t suspect it could be contributing to the problem.</p>
<p><a href="https://cybernils.net/2024/11/26/the-effect-of-using-mac-learning-in-esxi-nested-labs/">This article</a> by my fellow vExpert colleague Nils Kristiansen inspired me to delve deeper into the topic.</p>
<h2 id="why-promiscuous-mode-became-a-problem">Why Promiscuous Mode Became a Problem</h2>
<p>The performance degradation stemmed from how traffic was handled with Promiscuous Mode in a dual-uplink, Active/Active configuration:</p>
<ul>
<li>
<p>Broadcasting Traffic Across Both Uplinks: Promiscuous Mode caused the virtual switch to deliver all traffic to every uplink, regardless of the destination. With two high-speed uplinks in an Active/Active configuration, this created excessive overhead, saturating the uplinks and causing packet drops.</p>
</li>
<li>
<p>vSAN’s High Sensitivity to Latency: vSAN traffic is highly dependent on low latency and consistent performance. The unnecessary broadcast of packets interfered with its ability to operate efficiently.</p>
</li>
<li>
<p>Nested Virtualization Amplified the Problem: Nested ESXi hosts added another layer of complexity. The inner VMs were sending and receiving traffic that the parent ESXi host’s virtual switch struggled to handle efficiently under Promiscuous Mode.</p>
</li>
</ul>
<h2 id="ok-but-how-bad-is-the-performance">OK, but how bad is the performance?</h2>
<p>To quantify the performance issues, I turned to iPerf3, a reliable tool for measuring network throughput that is conveniently included in ESXi 8. Using iPerf3, I conducted a series of tests to better understand the extent of the performance degradation.</p>
<h3 id="performance-measurement-1-both-physical-nics-active-nested-hosts-on-the-same-physical-host">Performance Measurement 1: Both Physical NICs Active, Nested Hosts on the Same Physical Host</h3>
<p>For the first test, I configured both pNICs (10 Gb/s) as active in an Active/Active load balancing setup and placed both nested ESXi hosts on the same physical host. Additionally, Promiscuous Mode was enabled on the port group to ensure traffic could flow properly between the nested hosts.</p>
<p><img src="test1.png" alt="Test 1"></p>
<h3 id="results">Results</h3>
<pre tabindex="0"><code>[ ID] Interval           Transfer     Bitrate         Retr
[  5]   0.00-10.00  sec  1.35 GBytes  1.16 Gbits/sec    0            sender  
[  5]   0.00-10.00  sec  1.35 GBytes  1.16 Gbits/sec                 receiver 
</code></pre><h3 id="performance-measurement-2-single-nic-active-nested-hosts-on-the-same-physical-host">Performance Measurement 2: Single NIC Active, Nested Hosts on the Same Physical Host</h3>
<p>For the second test, I modified the setup to use only one physical NIC (pNIC) while keeping both nested ESXi hosts on the same physical host. Promiscuous Mode was still enabled on the port group to ensure traffic routing between the nested hosts. By disabling the second uplink, the traffic path was simplified, reducing potential conflicts.</p>
<p><img src="test2.png" alt="Test 2"></p>
<h3 id="results-1">Results</h3>
<pre tabindex="0"><code>[ ID] Interval           Transfer     Bitrate         Retr
[  5]   0.00-10.00  sec  11.4 GBytes  9.82 Gbits/sec    0            sender
[  5]   0.00-10.01  sec  11.4 GBytes  9.80 Gbits/sec                 receiver
</code></pre><h3 id="performance-measurement-3-mac-learning-and-forged-transmits-dual-uplinks-nested-hosts-on-the-same-physical-host">Performance Measurement 3: MAC Learning and Forged Transmits, Dual Uplinks, Nested Hosts on the Same Physical Host</h3>
<p>For the third test, I switched to using MAC Learning and Forged Transmits, while keeping both physical NICs (pNICs) active in the Active/Active load balancing configuration. Both nested ESXi hosts were still located on the same physical host. This configuration was designed to optimize traffic handling without relying on Promiscuous Mode</p>
<p><img src="test3.png" alt="Test 3"></p>
<h3 id="results-2">Results</h3>
<pre tabindex="0"><code>[ ID] Interval           Transfer     Bitrate         Retr  
[  5]   0.00-10.00  sec  24.2 GBytes  20.8 Gbits/sec    0            sender  
[  5]   0.00-10.01  sec  24.2 GBytes  20.8 Gbits/sec                 receiver  
</code></pre><h3 id="performance-measurement-4-mac-learning-and-forged-transmits-single-uplink-nested-hosts-on-the-same-physical-host">Performance Measurement 4: MAC Learning and Forged Transmits, Single Uplink, Nested Hosts on the Same Physical Host</h3>
<p>For the fourth test, I used a single uplink (pNIC) with both nested ESXi hosts on the same ESXi server. MAC Learning and Forged Transmits were enabled to optimize traffic handling.
The throughput was 20.7 Gbits/sec, almost identical to Test 3. This confirms that, since the traffic did not need to traverse the physical network infrastructure, the single uplink configuration with MAC Learning and Forged Transmits performed just as efficiently, without the overhead of Promiscuous Mode.</p>
<p><img src="test4.png" alt="Test 4"></p>
<h3 id="results-3">Results</h3>
<pre tabindex="0"><code>[ ID] Interval           Transfer     Bitrate         Retr  
[  5]   0.00-10.00  sec  24.2 GBytes  20.8 Gbits/sec    0            sender  
[  5]   0.00-10.01  sec  24.2 GBytes  20.8 Gbits/sec                 receiver  
</code></pre><h3 id="further-performance-measurements-and-security-considerations">Further Performance Measurements and Security Considerations</h3>
<p>Additional performance tests revealed that the difference between Promiscuous Mode and MAC Learning was minimal or even non-existent when the nested hosts were placed on two different physical hosts.
The traffic between the nested VMs did not significantly differ whether Promiscuous Mode or MAC Learning was enabled, indicating that both configurations performed similarly in a multi-host environment.</p>

    <aside class="admonition warning">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-alert-circle">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="8" x2="12" y2="12"></line>
      <line x1="12" y1="16" x2="12.01" y2="16"></line>
   </svg></div><b>It&#39;s important to note the security implications of using Promiscuous Mode</b>
        </div>
        <div class="admonition-content">Enabling Promiscuous Mode on a network interface allows all traffic to be sent to the VM, even traffic not intended for it, which can expose sensitive data or potentially allow malicious activity.
Because of this security concern, Promiscuous Mode should only be used temporarily, and for troubleshooting purposes, in production environments.
It is recommended to disable it once the issue is resolved to maintain a secure network setup.</div>
    </aside>
<h3 id="side-effect-of-promiscuous-mode-duplicate-packets">Side Effect of Promiscuous Mode: Duplicate Packets</h3>
<p>Enabling Promiscuous Mode on a network interface can lead to duplicate packets when both the source and destination are on the same ESXi host. In this mode, the virtual machine receives all traffic, including its own outbound packets, causing unnecessary duplication. This can result in performance degradation due to increased CPU usage and network inefficiencies.</p>
<pre tabindex="0"><code>[root@esxnuc04:/usr/lib/vmware/vsan/bin] vmkping -I vmk1 192.168.69.203
PING 192.168.69.203 (192.168.69.203): 56 data bytes
64 bytes from 192.168.69.203: icmp_seq=0 ttl=64 time=0.356 ms
64 bytes from 192.168.69.203: icmp_seq=0 ttl=64 time=0.423 ms (DUP!)
64 bytes from 192.168.69.203: icmp_seq=0 ttl=64 time=0.426 ms (DUP!)
64 bytes from 192.168.69.203: icmp_seq=0 ttl=64 time=0.429 ms (DUP!)
64 bytes from 192.168.69.203: icmp_seq=1 ttl=64 time=0.249 ms
64 bytes from 192.168.69.203: icmp_seq=1 ttl=64 time=0.274 ms (DUP!)
64 bytes from 192.168.69.203: icmp_seq=1 ttl=64 time=0.277 ms (DUP!)
64 bytes from 192.168.69.203: icmp_seq=1 ttl=64 time=0.281 ms (DUP!)
64 bytes from 192.168.69.203: icmp_seq=2 ttl=64 time=0.261 ms
</code></pre><h2 id="sidequest-using-iperf-on-esxi-803">Sidequest: Using iPerf on ESXi 8.0.3</h2>
<p>To use iPerf for network performance testing on ESXi 8.0.3, you&rsquo;ll need to follow a few steps to enable and configure the necessary settings.</p>
<ul>
<li>Step 1: Disable the ESXi firewall temporarily
First, disable the ESXi firewall to allow the iPerf tool to operate without restrictions:</li>
</ul>
<pre tabindex="0"><code>esxcli network firewall set --enabled false
</code></pre><ul>
<li>Step 2: Allow executing iPerf
Next, set the system to allow execution of non-installed binaries (such as iPerf), which are not part of the default ESXi installation:</li>
</ul>
<pre tabindex="0"><code>localcli system settings advanced set -o /User/execInstalledOnly -i 0
</code></pre><ul>
<li>Step 3: Execute iPerf (Client example)
Once you&rsquo;ve set the necessary configuration, you can execute iPerf to test the network performance. Use the following command to run iPerf as a client (-c) and specify the target IP address (e.g., 192.168.69.203):</li>
</ul>
<pre tabindex="0"><code>./usr/lib/vmware/vsan/bin/iperf3 -c 192.168.69.203
</code></pre><ul>
<li>Step 4: Re-enable the firewall
Once you’ve finished testing, remember to re-enable the firewall for security reasons:</li>
</ul>
<pre tabindex="0"><code>esxcli network firewall set --enabled true
</code></pre><ul>
<li>Step 5: Restrict execution of non-installed binaries
To revert the system to its default behavior and restrict the execution of non-installed binaries, run the following command:</li>
</ul>
<pre tabindex="0"><code>localcli system settings advanced set -o /User/execInstalledOnly -i 1
</code></pre><h2 id="why-mac-learning-and-forged-transmits-replace-promiscuous-mode-in-nested-environments">Why MAC Learning and Forged Transmits Replace Promiscuous Mode in Nested Environments</h2>
<p>In a typical nested ESXi environment, each inner VM sends packets with its unique MAC address, which the virtual switch on the parent ESXi host does not recognize by default. This creates a challenge because:</p>
<ul>
<li>Without Promiscuous Mode, the switch drops packets destined for or originating from these MAC addresses.</li>
<li>Without Forged Transmits, packets from inner VMs with &ldquo;forged&rdquo; source MAC addresses are also dropped.</li>
</ul>
<h3 id="by-enabling-mac-learning-and-forged-transmits">By enabling MAC Learning and Forged Transmits:</h3>
<p><em><strong>MAC Learning</strong></em> ensures that the virtual switch learns the inner VMs’ MAC addresses dynamically, so it can correctly forward traffic to them without requiring Promiscuous Mode.
<em><strong>Forged Transmits</strong></em> ensures that traffic from inner VMs with different source MAC addresses is allowed to leave the parent VM&rsquo;s vNIC.</p>

    <aside class="admonition info">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-info">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="16" x2="12" y2="12"></line>
      <line x1="12" y1="8" x2="12.01" y2="8"></line>
   </svg></div><b>Conclusion</b>
        </div>
        <div class="admonition-content">The combination of MAC Learning and Forged Transits removes the need for promiscuous mode, while maintaining:
Better performance, as traffic is only sent where needed.
Stronger security, since traffic is not broadcast unnecessarily.</div>
    </aside>
<p>MAC Learning with Forged Transmits is a significant performance gamechanger, especially when running multiple nested VMs on a single physical ESXi host.
However, it&rsquo;s important to note that MAC Learning with Forged Transmits requires a Distributed Switch. If you&rsquo;re using a Standard Switch, you&rsquo;ll still need to rely on Promiscuous Mode to achieve similar functionality.</p>
]]></content>
		</item>
		
		<item>
			<title>Unraid - A Storage Journey</title>
			<link>https://sdn-warrior.org/posts/unraid-storage/</link>
			<pubDate>Tue, 19 Nov 2024 23:00:00 +0100</pubDate>
			
			<guid>https://sdn-warrior.org/posts/unraid-storage/</guid>
			<description><![CDATA[How i use Unraid]]></description>
			<content type="html"><![CDATA[<h2 id="my-custom-unraid-storage-build---flexibility-simplicity-and-future-proofing">My Custom Unraid Storage Build - Flexibility, Simplicity, and Future-Proofing</h2>
<p>As a passionate homelaber, I enjoy not only using technology but also understanding and customizing it to suit my needs. My Unraid storage system is one of my longest-running projects, continuously evolving to meet the demands of my homelab.</p>
<p>After thorough research, I decided to go with Unraid – an operating system renowned for its simplicity, flexibility, and scalability. These key features were the deciding factors for me:</p>
<ul>
<li>Easy Expansion: Unraid allows combining drives of different sizes and expanding the array later without replacing all disks at once.</li>
<li>Docker Integration: The ability to run Docker containers directly on Unraid unlocks immense potential for personal projects and applications.</li>
<li>Versatility: Whether it’s managing data, running a media server, or hosting virtual machines, Unraid offers the freedom to adapt the system to your needs.</li>
</ul>
<p>In this blog post, I’ll share my experience and guide you through how I’ve planned, built, and continuously improved my Unraid storage system. It’s a perfect solution for anyone seeking a scalable, cost-effective setup without sacrificing performance or ease of use.</p>
<h2 id="the-beginning-my-first-steps-with-unraid">The Beginning: My First Steps with Unraid</h2>
<p>Unraid is a commercial product that initially caught my attention due to its unique approach to storage management. Historically, Unraid licenses were available for a one-time purchase, providing lifetime access to its features. Today, however, users can choose between a subscription model or a lifetime license, offering flexibility depending on individual needs.</p>
<p>One of the standout features of Unraid is that it boots directly from a USB stick. This design choice not only simplifies installation but also makes it incredibly easy to replace hardware. Simply move the USB stick to a new machine, and the system is ready to run without the need for extensive reconfiguration.</p>
<p>My first Unraid “server” was far from conventional: a Lenovo notebook powered by an old Intel Dual-Core processor. To build my initial array, I used external USB disks – a true makeshift setup but perfect for testing the waters. For three weeks, this setup served as my proof of concept (POC), allowing me to explore Unraid’s capabilities and ensure it met my needs before committing to more suitable hardware.</p>
<p>This early experimentation confirmed that Unraid was the right choice for my homelab, and I soon began planning and acquiring the components for my first proper build.</p>
<h2 id="building-a-3-tier-performance-storage-system">Building a 3-Tier Performance Storage System</h2>
<p>As my Unraid setup evolved, I implemented a 3-tier performance storage system to meet the varying demands of my homelab. Each tier is tailored for a specific purpose, optimizing the balance between speed, capacity, and efficiency:</p>
<ul>
<li>Tier 1: The Unraid Array (Slow Storage)
At the foundation of my storage system is the Unraid Array, which serves as the slowest but most capacious tier. Unlike traditional RAID, an Unraid Array does not stripe data across disks. Instead, each disk holds individual files, while parity disks provide fault tolerance for data recovery. This unique design allows mixing drives of different sizes, making upgrades straightforward and cost-effective.
My Unraid Array is hosted in an external USB 3.2 storage shelf, which presents each drive individually to Unraid. The shelf delivers enough bandwidth to operate all six 6TB enterprise SATA drives at full speed, ensuring reliable performance even during intensive data access.</li>
</ul>

    <aside class="admonition warning">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-alert-circle">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="8" x2="12" y2="12"></line>
      <line x1="12" y1="16" x2="12.01" y2="16"></line>
   </svg></div><b>A Quick Warning About Using Unraid with USB Disk Shelves</b>
        </div>
        <div class="admonition-content"><p>It’s important to note that Unraid does not officially recommend running the array on a USB disk shelf, as USB connections can sometimes lead to instability or performance issues. However, my personal experience has shown that it can work reliably with the right hardware.</p>
<p>In my setup, I use a TerraMaster D6-320 USB 3.2 disk shelf, paired with a high-quality USB controller like those found in devices such as Intel NUCs. This combination has proven stable and capable of delivering full-speed performance for all six enterprise SATA drives in my array.
While this setup works well for me, I recommend testing thoroughly in your environment to ensure stability and compatibility before committing to a similar configuration.</p>
</div>
    </aside>
<ul>
<li>
<p>Tier 2: Consumer NVMe Drives (Fast Cache and Docker Storage)
The second performance tier consists of consumer-grade NVMe drives, configured in a btrfs pool within Unraid. This configuration not only allows advanced features like snapshots but also supports RAID levels within the pool, providing a balance between performance and redundancy.
This tier is designed to handle tasks requiring high-speed I/O, such as hosting Docker containers. Keeping Docker data on the NVMe tier ensures that the Unraid Array doesn’t need to spin up unnecessarily, prolonging the life of the disks and improving system responsiveness.
The NVMe drives also serve as a fast cache for incoming data. Files uploaded to the NAS during the day are stored on the NVMe tier and then moved to the slower Unraid Array overnight—except for Docker data, which always remains on the NVMe storage to maintain optimal performance.
Unraid’s flexibility makes it easy to decide whether specific shares or data should stay on the NVMe pool or be automatically moved to the Array on a scheduled basis. This level of control ensures you can optimize storage placement to suit your workload, balancing speed and capacity seamlessly.</p>
</li>
<li>
<p>Tier 3: Enterprise NVMe via iSCSI (Fast and Durable Storage)
The top tier features a 4TB enterprise NVMe drive, designed for high-speed and durable performance under constant load. This storage tier is shared with my homelab servers via iSCSI Multichannel, utilizing a 2x10Gb Intel X710 NIC for redundancy and maximum throughput.
This tier provides fast, reliable storage for workloads that demand consistent performance, such as virtual machines or other critical applications in my homelab. By leveraging enterprise-grade hardware and robust networking, this storage layer ensures low-latency access and can handle the demands of continuous use without compromising reliability.</p>
</li>
</ul>
<h2 id="current-setup">Current Setup</h2>
<p>My Unraid server is built on a Intel NUC Extreme 11th Gen i7 with 64GB of RAM, offering a powerful and compact platform for my homelab. The storage setup includes 2x 1TB consumer-grade NVMe drives for fast cache and 4TB enterprise-grade NVMe for ultra-reliable, high-performance storage.</p>
<p>The Unraid Array has a total capacity of 42TB, with 33.4TB usable for data storage. This provides ample space for both my active projects and long-term storage needs.</p>
<p>On the software side, I host 29 Docker container services and 4 virtual machines, including critical services such as my Active Directory (AD), Certificate Authority (CA), and a Veeam Proxy for file backups. This setup allows for a highly efficient and flexible environment that supports a wide range of use cases while maintaining reliable performance.</p>
<p><img src="unraid2.jpg" alt="Gui"></p>
<h2 id="performance">Performance</h2>
<p>The performance of my Unraid setup was measured using CrystalDiskMark with a 16GB test file (on a Windows 11 VM) to evaluate both sequential and random read and write speeds, as well as IOPS (Input/Output Operations Per Second) of my iSCSI Storage (Tier 3). The results highlight the impressive capabilities of the system:</p>
<p>Read Performance:</p>
<ul>
<li>Sequential Read (Q8T1): 1.993 GB/s | IOPS: 1900.35</li>
<li>Sequential Read (Q1T1): 0.782 GB/s | IOPS: 746.21</li>
<li>Random Read 4K (Q32T1): 0.322 GB/s | IOPS: 78,651.61</li>
<li>Random Read 4K (Q1T1): 0.021 GB/s | IOPS: 5,149.17</li>
</ul>
<p>Write Performance:</p>
<ul>
<li>Sequential Write (Q8T1): 1.247 GB/s | IOPS: 1,189.37</li>
<li>Sequential Write (Q1T1): 0.802 GB/s | IOPS: 764.48</li>
<li>Random Write 4K (Q32T1): 0.298 GB/s | IOPS: 72,776.61</li>
<li>Random Write 4K (Q1T1): 0.036 GB/s | IOPS: 8,835.45</li>
</ul>
<p>These performance metrics demonstrate both the high throughput and responsiveness of the NVMe storage.
The sequential read and write speeds are excellent for large file transfers, while the random IOPS (especially at Q32T1) indicate the drive’s ability to handle a high volume of small, random data requests.
Despite the lower random read/write speeds at Q1T1, the system still shows strong overall performance for a variety of tasks.</p>
<h2 id="understanding-the-crystaldiskmark-test-parameters">Understanding the CrystalDiskMark Test Parameters</h2>
<p>In CrystalDiskMark, several key parameters define how the storage device is tested. Here’s a breakdown of what each test represents:</p>
<p>Q8T1: This stands for Queue Depth 8, Thread 1. It simulates a scenario where 8 data requests are queued up, but only 1 thread (or process) is handling those requests. This type of test is useful for measuring the performance of the storage device when handling multiple sequential tasks at once, but not with excessive parallelism.</p>
<p>Q1T1: This stands for Queue Depth 1, Thread 1. Here, only 1 data request is in the queue, and a single thread handles it. This test represents the performance when a single request is being processed at a time, simulating typical user scenarios where one operation is occurring without significant multitasking.</p>
<p>Q32T1: This stands for Queue Depth 32, Thread 1. In this case, there are 32 queued data requests with a single thread handling them. This test simulates heavy workloads where many data requests are pending, but only one thread is processing them. It can show how the device handles stress under larger, more sustained read operations.</p>
<h3 id="sequential-vs-random-read-tests">Sequential vs. Random Read Tests</h3>
<p>Sequential Read: This test measures how fast the storage device can read large, contiguous chunks of data, like streaming a video or transferring large files. It simulates real-world scenarios where large files need to be read from the storage at a steady rate.</p>
<p>Sequential Read (Q8T1): 1.993 GB/s – This high performance indicates the drive can handle multiple large file read operations quickly, with 8 data requests queued up.
Sequential Read (Q1T1): 0.782 GB/s – This is slower than the Q8T1 test because only one request is processed at a time, simulating less intensive operations.</p>
<p>Random Read: This test measures the performance when the drive has to read small, non-contiguous chunks of data from different parts of the storage. This type of test is more representative of workloads like database operations or running small applications that frequently access different data blocks.</p>
<p>Random Read 4K (Q32T1): 0.322 GB/s – With 32 queued requests and 4KB blocks, this performance shows how the system handles multiple random reads.
Random Read 4K (Q1T1): 0.021 GB/s – Here, only one small request is being handled at a time, leading to slower performance because random 4K reads are typically slower due to the overhead of accessing many different locations on the disk.</p>
<p>These tests give a complete picture of the drive&rsquo;s performance under different scenarios: from high-speed, sequential reads (large files) to more intensive, random access (small files), and with varying levels of workload concurrency.</p>
<h2 id="bom-bill-of-materials">BOM (Bill of Materials)</h2>
<ul>
<li>NUC11DBBi7 , Version M17027-404</li>
<li>2x 32 GB RAM Kingston KF3200C20S4 SODIMM DDR4 Synchronous 3200 MHz</li>
<li>TerraMaster D6-320 USB 3.2(Gen2)</li>
<li>3x TOSHIBA_MG09ACA18TE 18 TB Enterprise SATA</li>
<li>1x TOSHIBA_MG08ADA600E 6 TB Enterpise SATA (to change)</li>
<li>2x Samsung 970 EVO Plus 1TB</li>
<li>1x Samsung MZ1L23T8HBLA-00A07 4 TB Enterprise NVMe 110mm</li>
<li>1x Intel X710 2x 10 Gb/s</li>
<li>1x Good USB Stick (32GB)</li>
</ul>
<p><img src="das.jpg" alt="DAS Disk Array">
<img src="unraid.jpg" alt="Unraid"></p>
<h3 id="fun-fact-my-unraid-server-has-underglow-lighting">Fun fact: My Unraid server has underglow lighting!</h3>
]]></content>
		</item>
		
		<item>
			<title>How to get most out of your Nuc </title>
			<link>https://sdn-warrior.org/posts/nuc/</link>
			<pubDate>Sun, 17 Nov 2024 11:57:43 +0100</pubDate>
			
			<guid>https://sdn-warrior.org/posts/nuc/</guid>
			<description><![CDATA[Performance tuning for NUCs]]></description>
			<content type="html"><![CDATA[<h2 id="first-things-first">First things first</h2>
<p>Get a second NIC. The Intel NUC Pro has an IO expansion and supports an additional NIC.
Unfortunately, these are relatively difficult to get in Germany, but it&rsquo;s worth the effort.

    <aside class="admonition info">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-info">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="16" x2="12" y2="12"></line>
      <line x1="12" y1="8" x2="12.01" y2="8"></line>
   </svg></div><b>Search for</b>
        </div>
        <div class="admonition-content">ASUS NUC LAN and USB Expansion Module (90AR0000-P00010)</div>
    </aside></p>
<p>
<figure><picture>
          <source srcset="/NIC_hu6903087498817470336.webp" type="image/webp">
          <source srcset="/NIC_hu4204005753805133783.jpg" type="image/jpeg">
          <img src="/NIC_hu6903087498817470336.webp"alt="Image of an IO expansion"  width="1200"  height="800" />
        </picture><figcaption>
            <p>IO expansion</p>
          </figcaption></figure>
vSphere 8 supports the cards natively and you don&rsquo;t have to install any drivers.
It also supports jumbo frames, which is relevant for NSX Labs.
It is recommended to use a 2.5 GB managed switch. I am using a Mikrotik with the wonderful name <code>CRS326-4C +20G+2Q</code>.</p>

    <aside class="admonition tip">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
   </svg></div><b>Practical tip</b>
        </div>
        <div class="admonition-content">My experience with 2.5 Gb/s Lan has shown that it makes sense to set the ports to a fixed speed in the hypervisor and on the switch, otherwise I kept having network failures.</div>
    </aside>
<h2 id="memory-tiering">Memory Tiering</h2>
<p>Memory Tiering is very new in ESXi vSphere 8.0U3 and is still a Tech Preview.
With memory tiering, you can use up to 400% of the physical RAM. This requires a fast NVMe.
I would recommend a PCIe4 NVMe with at least 5000 MB/s read/write.
Memory Tiering stores very cold (unused RAM pages) and cold RAM pages (less than 20% used) on the NVMe (Memory Tier).
There is a wonderful <a href="https://www.vmware.com/explore/video-library/video/6360757998112" title="Explore USA">Explore Session</a> on this.</p>
<p>To enable memory tiering, you have to enter the following commands via the ESX Cli:</p>
<ul>
<li>This command turns on memory tiering</li>
</ul>
<pre tabindex="0"><code>esxcli system settings kernel set -s MemoryTiering -v TRUE
</code></pre><ul>
<li>This command selects the NVMe</li>
</ul>
<pre tabindex="0"><code>esxcli system tierdevice create -d /vmfs/devices/disks/&lt;Your NVME&gt;
</code></pre><ul>
<li>Enter the factor here (0-400%).</li>
</ul>
<pre tabindex="0"><code>esxcli system settings advanced set -o /Mem/TierNvmePct -i 400
</code></pre><p>After a reboot, you have the selected amount of additional memory.</p>

    <aside class="admonition warning">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-alert-circle">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="8" x2="12" y2="12"></line>
      <line x1="12" y1="16" x2="12.01" y2="16"></line>
   </svg></div><b>Attention</b>
        </div>
        <div class="admonition-content">The selected disk is no longer available to the ESXi.
The minimum capacity must match the selected factor.
If the disk is larger, it will still be used entirely for memory tiering.
My recommendation is to use 1 TB NVMe with 64 GB of physical RAM and 400% as the factor.
ESXi will use the NVMe evenly so that the disk doesn&rsquo;t break as quickly.</div>
    </aside>
<h2 id="using-pe-cores">Using P/E Cores</h2>
<p>Intel has introduced the big.little CPU architecture from the 12th generation of their consumer CPUs. This leads to some problems with ESXi. If the efficiency cores are activated, the ESXi starts with a PSOD (Purble Screen of Death).
Fortunately, there are a few workarounds here.</p>
<ul>
<li>Disable the E cores in the BIOS</li>
</ul>
<p>This means that you can use hyperthreading and the P Cores. However, you are clearly wasting potential here. That&rsquo;s why we don&rsquo;t want to.</p>
<ul>
<li>Use P and E cores and sacrifice hyperthreading for them</li>
</ul>
<p>My tests showed that I got significantly more performance out of my 13th generation i7 if I didn&rsquo;t use hyperthreading and only used “real” CPU cores, even if the E cores have a lower clock rate.
<a href="https://williamlam.com/2023/01/video-of-esxi-install-workaround-for-fatal-cpu-mismatch-on-feature-for-intel-12th-gen-cpus-and-newer.html">William Lam</a> has written very detailed blog articles about this, I link to him here for more information, as this article was actually only intended to be a short summary.</p>
<p>We actually only need two ESX CLI commands to make it all work.</p>
<ul>
<li>With this command, we prevent the PSOD from occurring when the ESXi boots.</li>
</ul>
<pre tabindex="0"><code>esxcli system settings kernel set -s cpuUniformityHardCheckPanic -v FALSE
</code></pre><ul>
<li>With this command, we prevent the ESXi from getting a PSOD when the VMs are switched on.</li>
</ul>
<pre tabindex="0"><code>esxcli system settings kernel set -s ignoreMsrFaults -v TRUE
</code></pre>
    <aside class="admonition tip">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
   </svg></div><b>Practical tip</b>
        </div>
        <div class="admonition-content">When reinstalling an ESXi server, I always switch off the E Cores, which saves me from having to manipulate the boot loader. After I have allowed memory tiering and the E/P Cores via the ESX CLI, I switch the E/P Cores back on in the BIOS.</div>
    </aside>
<p>If everything is correct, an ESX NUC of the 13th generation looks like this.

<figure><picture>
          <source srcset="/nuc_hu16244612542378356649.webp" type="image/webp">
          <source srcset="/nuc_hu988947436421053682.jpg" type="image/jpeg">
          <img src="/nuc_hu16244612542378356649.webp"alt="NUC i7"  width="1098"  height="458" />
        </picture><figcaption>
            <p>NUC i7 13th Gen with Memory Tiering and P/E Cores</p>
          </figcaption></figure></p>
]]></content>
		</item>
		
		<item>
			<title>Homelab V4</title>
			<link>https://sdn-warrior.org/posts/labv4/</link>
			<pubDate>Sat, 16 Nov 2024 20:00:00 +0000</pubDate>
			
			<guid>https://sdn-warrior.org/posts/labv4/</guid>
			<description><![CDATA[Homelab v4]]></description>
			<content type="html"><![CDATA[<h2 id="ready-for-vcf">Ready for VCF</h2>
<p>I have done a huge redesign of my Homelab.
To better test VCF scenarios, 3 new Minisforum MS-01 have been added.
These have a 13th generation i9 and are equipped with fast NVMes for memory tiering.
They also have 2x10G and 2x2.5G networking on board for various VM workloads.
Furthermore, I converted my storage from NFS to iSCSI with multipathing, which gets even more performance out of my self-built Unraid.
I manage about 2 GB/s read / 1.2 GB GB/s write and 78K IOPS (Random 4K with 32Q) in a Windows 11 VM.</p>

<figure><picture>
          <source srcset="/bench1_hu2966482509308598586.webp" type="image/webp">
          <source srcset="/bench1_hu2660173491905647916.jpg" type="image/jpeg">
          <img src="/bench1_hu2966482509308598586.webp"alt="Disk Performance iSCSI Multipathing"  width="483"  height="351" />
        </picture><figcaption>
            <p>Disk Performance iSCSI Multipathing</p>
          </figcaption></figure>

<figure><picture>
          <source srcset="/bench2_hu3593698816181082095.webp" type="image/webp">
          <source srcset="/bench2_hu10297252823232288012.jpg" type="image/jpeg">
          <img src="/bench2_hu3593698816181082095.webp"alt="IOPS iSCSI Multipathing"  width="483"  height="356" />
        </picture><figcaption>
            <p>IOPS iSCSI Multipathing</p>
          </figcaption></figure>
<p>Pretty impressive for my setup. I still have to customize the rack a bit so that I can add the 10G Mikrotik switch and clean up the VLANs from old labs.
I&rsquo;m already planning a further expansion stage though.\</p>
]]></content>
		</item>
		
		<item>
			<title>NSX Integration Fortigate</title>
			<link>https://sdn-warrior.org/posts/nsx-integration-fortigate/</link>
			<pubDate>Mon, 26 Aug 2024 19:49:23 +0100</pubDate>
			
			<guid>https://sdn-warrior.org/posts/nsx-integration-fortigate/</guid>
			<description><![CDATA[NSX and Fortigate]]></description>
			<content type="html"><![CDATA[<h2 id="nsx-integration-for-fortinet-fortigate-firewalls">NSX integration for Fortinet Fortigate Firewalls</h2>
<p>Modern SDN solutions are flexible, fast and effective. The rules of the classic perimeter firewall should be exactly the same. To make life easier, Fortinet has an NSX integration that allows us to write our perimeter rules to dynamic NSX groups.</p>
<h2 id="first-things-first">First things first</h2>
<p>The Fortinet NSX integration works via a so-called external connector. For this purpose, the Fortigate contacts the NSX Manager at regular intervals and updates the previously imported groups.
This allows us to use dynamic groups that were previously created in NSX using tags, for example.</p>
<p>First we need to configure our connector. To do this, go to the Fortigate at <em><strong>Security Fabric / External Connectors</strong></em> and click on <em><strong>Create New</strong></em>.</p>
<p><img src="01.webp" alt="Fortigate Dialog"></p>
<p>Here we need to enter our NSX Manager, if we have an NSX Manager Cluster then of course the Cluster VIP or FQDN is needed.
We can define an update interval, this determines how long it takes to update the groups on the Fortigate.
In my lab I chose 30 seconds, depending on the environment lower or higher values may make sense. In a productive environment, the certificate should always be verified.
In my homelab environment I deliberately turned this off.</p>
<p><img src="02_External-Connector.webp" alt="External Connector"></p>
<h2 id="importing-the-dynamic-nsx-groups">Importing the dynamic NSX groups</h2>
<p>The groups need to be imported via the Fortigate CLI. This is relatively easy to do for all groups and specifically for individual groups.
Groups imported this way will be automatically updated in the future. If new groups are configured in NSX, they must be imported via the CLI if they play a role in the rules.</p>

    <aside class="admonition info">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-info">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="16" x2="12" y2="12"></line>
      <line x1="12" y1="8" x2="12.01" y2="8"></line>
   </svg></div><b>Fortigate CLI</b>
        </div>
        <div class="admonition-content"><code>execute nsx group import &lt;SDN Connector&gt; &lt;VDOM&gt; &lt;group&gt;</code></div>
    </aside>
<p>If you want to import all NSX groups, you need to omit the group name in the CLI call. In the screenshot you can see me importing the <em><strong>dFG_AlpineLinux</strong></em> NSX group.
This uses an NSX tag to combine all VMs of type Alpine Linux into one security group.</p>
<p><img src="03_Group-Import.png" alt="Group-Import"></p>
<p>In the Fortigate, you can now find the group under <em><strong>Policy &amp; Objects / Addresses</strong></em> and use it like any other group in firewall policies. The NSX groups can be used not only for firewall rules, but also for policy-based routing via the SD-WAN feature.</p>
<p><img src="04_FW-Groups.webp" alt="Firewall Groups"></p>
<p>In my example, I am prohibiting the Alpine Linux VMs from accessing the Internet. The current realised group assignment can be checked at any time via <em><strong>Policy &amp; Objects&gt; / Addresses</strong></em> and a double click on the group.</p>
<p><img src="05_matched-adress.webp" alt="Matched Adrewss"></p>
<h2 id="delete-groups">Delete groups</h2>
<p>Groups need to be deleted manually. The easiest way to do this is via the Fortigate CLI. To do this, execute the following CLI command:</p>

    <aside class="admonition info">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-info">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="16" x2="12" y2="12"></line>
      <line x1="12" y1="8" x2="12.01" y2="8"></line>
   </svg></div><b>Fortigate CLI</b>
        </div>
        <div class="admonition-content"><code>execute nsx group delete &lt;VDOM&gt; &lt;filter&gt;</code></div>
    </aside>
<p>If you want to delete all groups, you can simply leave the filter empty. If a group is used in a firewall policy, it cannot be deleted and you will receive a message that the group is in use.</p>
<h2 id="testing-the-solution">Testing the solution</h2>
<p>To do this, I log on to the Alpine2 VM and check the current IP. The VM has currently been assigned 172.31.2.10. We can also find this on the Fortigate in our dFG_AlpineLinux group. I am trying to send an ICMP to the Internet, which is blocked by the Fortigate firewall as expected.</p>
<p><img src="06_test-1.webp" alt="First Test"></p>
<p>Next, I remove the Alpine Linux tag in the NSX, which ensures that the VM is no longer realised in the dFG_Alpine Linux group on the Fortigate after 30 seconds at the latest.</p>
<p><img src="07_test-2.webp" alt="Second Test"></p>
<p>Finally, I repeated my ping test. As expected, Internet access is now without any problems.</p>
<p><img src="08_icmp.png" alt="Test Number three"></p>
<h2 id="remarks">Remarks</h2>
<p>If the connection to NSX Manager is interrupted, group membership remains at the last synchronised state. This means that in highly dynamic environments, too much or too little traffic may be allowed or blocked. For this reason, the SDN connection should always be monitored. All group changes are saved in the Log SDN Connector Log of the Fortigate.</p>
<h2 id="use-cases">Use cases</h2>
<p>One conceivable scenario would be to enable a dynamic firewall for VMs that are allowed to access the Internet. This can be done in NSX using a tag and a group. Every VM that does not have a tag and is therefore not in the group will be blocked at the Fortigate perimeter firewall.</p>
<p><img src="09_firewall_rule.webp" alt="Firewall Rules"></p>
<p>The firewall rule allows everything that does not go into RFC1918 networks (private IP range). Of course, this is only a simple example and more complex setups are possible.</p>
<h2 id="additional-information">Additional information</h2>
<p><a href="https://docs.fortinet.com/document/fortigate/7.4.4/administration-guide/753961/public-and-private-sdn-connectors">Fortinet Documentation: Public and private SDN connectors</a></p>
]]></content>
		</item>
		
		<item>
			<title>NSX Identity Firewall – A Case Study With the Flavour VDI</title>
			<link>https://sdn-warrior.org/posts/nsx-idfw-vdi/</link>
			<pubDate>Fri, 02 Aug 2024 08:34:23 +0100</pubDate>
			
			<guid>https://sdn-warrior.org/posts/nsx-idfw-vdi/</guid>
			<description><![CDATA[IDFW with NSX and Windows Clients]]></description>
			<content type="html"><![CDATA[
    <aside class="admonition info">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-info">
      <circle cx="12" cy="12" r="10"></circle>
      <line x1="12" y1="16" x2="12" y2="12"></line>
      <line x1="12" y1="8" x2="12.01" y2="8"></line>
   </svg></div><b>Disclaimer</b>
        </div>
        <div class="admonition-content">There are of course other ways of using the Identity Firewall. This is the way I have used it with my customers so far. Of course, the whole thing is colored by personal preferences, experiences and customer requirements so take this as an idea for your own implementations.</div>
    </aside>
<h2 id="intro">Intro</h2>
<p>A customer of mine has the use case that his entire environment must be micro-segmented. Of course, this does not stop at the VDI environment. Since my customer uses non-persistent VDIs (the VMs are deleted after each logoff), no tags can be used for the security groups. After deleting the VM, the tag would also be removed and a new VDI would have no tags and would thus be isolated. This can be solved with automation or with the Identity Firewall (a combination of both solutions is also conceivable and makes sense). In the first step, my customer opted for the Identity Firewall because it allows generic VDIs to be used and authorisations to resources can be conveniently controlled via the customer AD. In addition, each user/user group receives individual firewall rules, which corresponds to the need-to-know principle.</p>
<h3 id="you-can-use-the-identity-firewall-in-2-ways">You can use the Identity Firewall in 2 ways:</h3>
<ul>
<li>Variant 1 would be with VMware Tools and Guest Introspection.</li>
<li>Variant 2 would be with log scraping and, for example, Aria Operations for Logs as a log server.</li>
</ul>
<p>I use variant 1 because the implementation means less effort in my customer setup and we only want to protect the VDIs with Idendity Firewall.</p>
<h3 id="limitations">Limitations</h3>
<ul>
<li>No User /Group ID Support for Federation.</li>
<li>No direct integration with VDI and RDSH.</li>
<li>User-ID based rules are supported for only firewall rules.</li>
<li>No User-ID based policy for IDS/IPS and TLS Inspection.</li>
<li>Multi-User (RDSH) does not support Server Message Block (SMP) protocol.</li>
</ul>
<h3 id="supported-protocols">Supported protocols</h3>
<ul>
<li>Single user (VDI, or Non-RDSH Server) use case support – TCP, UDP</li>
<li>Multi-User (RDSH) use case support – TCP, UDP</li>
</ul>
<h3 id="supported-clients">Supported clients</h3>
<ul>
<li>Windows 8,10,11</li>
<li>Windows Server 2012 – 2022</li>
</ul>
<h3 id="what-is-needed">What is needed?</h3>
<ul>
<li>NSX in the VDI cluster</li>
<li>AD infrastructure</li>
<li>VMware tools</li>
<li>VMware Aria Operations for Logs (optional)</li>
</ul>
<h2 id="first-things-first">First things first</h2>
<p>Identity based groups can only be used as a source for a firewall rule. In addition, the rules only come into effect after a successful logon to the client. Thus, normal dFW rules must be written for all communication that happens before the user logs on. This applies, for example, to Windows AD communication. The NSX Manager synchronises cyclically with the domain controllers. The default interval is 180 minutes. If changes are made to the group membership at short notice, a manual sync can be performed. Alternatively, the sync interval can also be shortened or extended. Using network introspection, the NSX Manager recognises when a user logs on to a client and can perform matching with the AD groups and thus add the client dynamically to the security group of the IDFW firewall rule.</p>

<figure><picture>
          <source srcset="/idfw/00_idfw_hu13340240812954672917.webp" type="image/webp">
          <source srcset="/idfw/00_idfw_hu2021098600553032142.jpg" type="image/jpeg">
          <img src="/idfw/00_idfw_hu13340240812954672917.webp"alt="Idendity Firewall function"  width="1782"  height="1164" />
        </picture><figcaption>
            <p>Idendity Firewall function</p>
          </figcaption></figure>
<h2 id="getting-started">Getting started</h2>
<p>Firstly, I start by customising the golden images of the VDI and installing NSX Guest Introspection. These are not installed by default and have to be installed explicitly. You can find them under VMware Device Driver – NSX Network Introspection. File Introspection is installed automatically.</p>

<figure><picture>
          <source srcset="/idfw/01_vmwaretools_hu10766174744744931261.webp" type="image/webp">
          <source srcset="/idfw/01_vmwaretools_hu2019103790225014772.jpeg" type="image/jpeg">
          <img src="/idfw/01_vmwaretools_hu10766174744744931261.webp"alt="VMware Tools"  width="555"  height="417" />
        </picture><figcaption>
            <p>VMware Tools</p>
          </figcaption></figure>
<p>Once Guestintrospection has been successfully installed, we no longer need to do anything on our Windows clients. Next, the domain must be integrated.</p>
<p>This is done in the NSX Manager under System -&gt; Identity Firewall AD. Several domains can be entered so that multi-tenant setups can also be realised. The NSX Manager requires firewall activations and must be able to reach the domain controllers via LDAP or LDAPS. I strongly recommend the use of LDAPS. These settings can also be used to perform a manual sync or check the synchronisation status.</p>

<figure><a href="02_nsx_idfw.jpeg"><picture>
          <source srcset="/idfw/02_nsx_idfw_hu5496888141303419910.webp" type="image/webp">
          <source srcset="/idfw/02_nsx_idfw_hu10295445750369093658.jpeg" type="image/jpeg">
          <img src="/idfw/02_nsx_idfw_hu5496888141303419910.webp"alt="Idetity firewall AD settings"  width="1452"  height="465" />
        </picture></a><figcaption>
            <p>Idetity firewall AD settings (click to enlarge)</p>
          </figcaption></figure>
<p>Under LDAP Server you can set several domain controllers for the previously set up domain. The protocol used is also selected here. I use the Domain Administrator in my lab. In a productive environment, an LDAP bind user should always be used.</p>

<figure><a href="03.jpeg"><picture>
          <source srcset="/idfw/03_hu16733942988963753118.webp" type="image/webp">
          <source srcset="/idfw/03_hu2839745583667809871.jpeg" type="image/jpeg">
          <img src="/idfw/03_hu16733942988963753118.webp"alt="LDAP Server settings"  width="1160"  height="460" />
        </picture></a><figcaption>
            <p>LDAP Server settings (click to enlarge)</p>
          </figcaption></figure>

    <aside class="admonition attention">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" class="feather feather-link" width="24" height="24" viewBox="0 0 24 24"
      fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
      <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
      <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
   </svg></div><b>Attention</b>
        </div>
        <div class="admonition-content">If LDAPS is used, NSX imports the SHA thumbprint of the domain controller certificate. As the certificate is usually renewed automatically after 2 years at the latest, NSX loses the trust with the domain controller. In this case, the trust must be established manually. To do this, delete the thumbprint and reconnect to the bind user.  It has proven to be practical to monitor certificate expiry times and to enter at least 2 domain controllers that exchange their certificates with a 4-week time lag. If the trust fails completely, no more identity rules are applied and the default firewall rule comes into effect. In practice, this should be an any/any default drop and log rule.</div>
    </aside>
<p>After we have successfully setup and synchronised our domain, we only need to activate the Identity Firewall. By default, this feature is disabled (it is a free feature that is available with the NSX Firewall VCF add-on). To activate the Identity Firewall, go to Security -&gt; Distributed Firewall -&gt; Settings -&gt; Identity Firewall Settings and activate the identity firewall service button. Then we select the cluster on which we want to activate the service. Now we can get started.</p>

<figure><a href="04.webp"><picture>
          <source srcset="/idfw/04_hu12560464345806440685.webp" type="image/webp">
          <source srcset="/idfw/04_hu11069272111777865810.jpg" type="image/jpeg">
          <img src="/idfw/04_hu12560464345806440685.webp"alt="Distributed Firewall Settings"  width="1699"  height="792" />
        </picture></a><figcaption>
            <p>Distributed Firewall Settings (click to enlarge)</p>
          </figcaption></figure>
<h2 id="identity-firewall-rules">Identity Firewall Rules</h2>
<p>A general recommendation that applies to all firewalls is to think about a naming concept. At my customer, we have different name prefixes for the various security groups in NSX or in the other firewalls. For my part, I prefer the following naming convention, for example:</p>
<p>Distributed firewall groups start with dFW_XXX, an LDAP backed security group with dFWU_XXX (the U stands for user). For a group that contains an NSX segment it would be dFWS_XXX and for the gateway firewall a gWF_XXX and so on.</p>
<p>So we create our first LDAP user group. As with any group, we can do this either when creating the rules or in the inventory under Groups. The process is the same as for a normal Distributed Firewall Group, except that we don’t use tags but Distinguished Names, which can be conveniently selected or filtered from the synchronised AD elements.</p>

<figure><a href="05.webp"><picture>
          <source srcset="/idfw/05_hu16308509706422833611.webp" type="image/webp">
          <source srcset="/idfw/05_hu13581943069617237554.jpg" type="image/jpeg">
          <img src="/idfw/05_hu16308509706422833611.webp"alt="Security Groups"  width="1164"  height="977" />
        </picture></a><figcaption>
            <p>Security Groups (click to enlarge)</p>
          </figcaption></figure>
<p>I also need at least one segment group and at least one group containing my target servers. The target servers are assigned to their groups via tags. The same applies to the segment group. To do this, I set one or more tags on the overlay segment and use this tag as a condition for group membership.</p>

<figure><a href="06.webp"><picture>
          <source srcset="/idfw/06_hu566842021385844307.webp" type="image/webp">
          <source srcset="/idfw/06_hu1670820342853564742.jpg" type="image/jpeg">
          <img src="/idfw/06_hu566842021385844307.webp"alt="Member selection"  width="1156"  height="964" />
        </picture></a><figcaption>
            <p>Member selection (click to enlarge)</p>
          </figcaption></figure>
<p>Our goal will be that we authorise our users assigned to dFWU_UserGroup1 to access our fileserver with SMB and the users of the group dFWU_UserGroup2 must not receive any authorisation. I have two domain users in my lab, User1 is in the AD group assigned to dFWU_UserGroup1 and User2 is only assigned to dFWU_UserGroup2.</p>
<h2 id="creating-the-firewall-rules">Creating the firewall rules</h2>
<p>For each identity firewall rule that allows traffic from a group of users to a destination, there must be a corresponding distributed firewall rule that allows traffic from a group of computers to the same destination specified in the identity firewall rule. We therefore need two firewall rules.</p>

<figure><a href="07.webp"><picture>
          <source srcset="/idfw/07_hu14371369714500250916.webp" type="image/webp">
          <source srcset="/idfw/07_hu7421683099691664086.jpg" type="image/jpeg">
          <img src="/idfw/07_hu14371369714500250916.webp"alt="Firewall Rules"  width="1630"  height="226" />
        </picture></a><figcaption>
            <p>Firewall Rules (click to enlarge)</p>
          </figcaption></figure>
<p>The first rule is pretty straight forward, as source we have our dFWU_UserGroup1, the target is our dFG_Fileserver and the service is SMB. The Applied To Field is even more important than usual for the Identity Firewall. We may only apply this rule to our VDIs. Since my customer has different pools that are named according to a specific naming scheme, I can further restrict the scope based on the computer name. Each pool has different rules and we only want the rules to be realised on VMs where they are needed. The second rule is a bit more interesting. As a source, we have our VDI segment or segments. As in the first rule, the target is our file server. Logically, the service is also the same.</p>

    <aside class="admonition attention">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" class="feather feather-link" width="24" height="24" viewBox="0 0 24 24"
      fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
      <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
      <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
   </svg></div><b>Attention</b>
        </div>
        <div class="admonition-content">It is important that the Apply To field can only be on the file servers or on the target that we want to enable. If we were to use the dFG_VDI_GroupA or the dFGS_VDI group here, for example, then the entire Identity Firewall Rule is cancelled out!</div>
    </aside>
<h2 id="testing-and-verifying">Testing and verifying</h2>
<p>The test is considered successful if User1 on the TestVDI can establish a successful SMB connection to the file server. If User2 is used instead of User1, the traffic to the file server must be blocked by the firewall.</p>
<p>For testing, I log in to my VDI with the credentials of User1 and perform a TestNetConnection with Powershell. This is a simple and quick way to test TCP connections. I also open a share on the file server.</p>

<figure><a href="08.webp"><picture>
          <source srcset="/idfw/08_hu11227237490731539297.webp" type="image/webp">
          <source srcset="/idfw/08_hu11206926558301644657.jpg" type="image/jpeg">
          <img src="/idfw/08_hu11227237490731539297.webp"alt="Network test user 1"  width="1072"  height="627" />
        </picture></a><figcaption>
            <p>Network test user 1 (click to enlarge)</p>
          </figcaption></figure>
<p>The test was successful, both the TNC command and the actual opening of the file share worked. Now I’m running the same test on the same VDI (after it was recreated, because non-persistent VDI), only this time I’m using User2, which has no explicit firewall rules and is therefore blocked by my default cleanup rule. As expected, the traffic was successfully blocked.</p>

<figure><a href="09.png"><picture>
          <source srcset="/idfw/09_hu2338535551923653157.webp" type="image/webp">
          <source srcset="/idfw/09_hu1977019721799240822.jpg" type="image/jpeg">
          <img src="/idfw/09_hu2338535551923653157.webp"alt="Network test user 2"  width="993"  height="499" />
        </picture></a><figcaption>
            <p>Network test user 2 (click to enlarge)</p>
          </figcaption></figure>
<h2 id="lessons-learned">Lessons learned</h2>
<p>This is where I would like to add a few more thoughts on the subject. Troubleshooting is more difficult in practice than I thought. Tools such as NSX Traceflow cannot be used because you cannot add an AD user to the request. This means that the traffic in the traceflow is always dropped or the identity rule is maybe configured incorrectly.</p>
<p>But there is light at the end of the tunnel. In NSX 4.X there is a session view of the active IDFW user session under Security -&gt; Security Overview -&gt; Configuration. All active sessions, UserIDs and VMs are displayed here, as well as the source of the information.</p>

<figure><a href="10.png"><picture>
          <source srcset="/idfw/10_hu10916967050054961034.webp" type="image/webp">
          <source srcset="/idfw/10_hu10504170639179592038.jpg" type="image/jpeg">
          <img src="/idfw/10_hu10916967050054961034.webp"alt="Active Sessions"  width="979"  height="278" />
        </picture></a><figcaption>
            <p>Active Sessions (click to enlarge)</p>
          </figcaption></figure>
<p>Next tip would be to always check the sync status with the AD. Ask your AD admin when the user was added to the group. If the user has several accounts, ask for the user name used. Experience has shown that this is where most problems occur.</p>
<p>Use a syslog server and check exactly with which rule ID the traffic was discarded. Have all deny rules logged.</p>
<p>Not all rules can be implemented as Identity Firewall Rules. The Windows domain basic communication can only be enabled via a classic set of rules, as no Identity Firewall rules are active for the VM without an active user session.</p>
<h2 id="important-things-to-know">Important things to know</h2>
<p>Never install Guest Introspection on a target. If a user has remote desktop permissions on the target and guest introspection is active there, then the target receives all of the user’s firewall rules. This can lead to unwanted firewall permissions.</p>
<p>If targets outside of NSX are addressed, such as a NAS or legacy infrastructure, a second rule is not required (unless the gateway firewall is also used). In this case, the distributed firewall will only check the traffic at the source VM.</p>
<p>Any change on a domain, including a domain name change, will trigger a full sync with Active Directory. Because a full sync can take a long time, i recommend syncing during off-peak or non-business hours.</p>
<p>MutiUser setups only work with RDSH (Remote Desktop Session Hosts) which requires a special configuration. Otherwise, if several users are logged on to a client at the same time, this leads to unwanted behavior and, in the worst case, to unwanted firewall permissions.</p>
<h2 id="conclusion">Conclusion</h2>
<p>The Identity Firewall is a wonderful extension of the Distributed firewall and should be treated as such. Used correctly, it provides a very nice way to manage and delegate firewall permissions dynamically and centrally for individual users or user groups. It enables generic VDIs that can be used for different purposes depending on the user. This can reduce the number of VDI pools required, which in turn makes it easier to manage the customers VDI environment. The RBAC concept is even more tightly bound to the firewall policies and AD tiering can also be enforced via the firewall. And best of all, this great feature is included in the NSX Firewall license. I would recommend every NSX firewall administrator to take a closer look at the Identity Firewall.</p>
<h2 id="additional-resources">Additional resources</h2>
<p><a href="https://docs.vmware.com/en/VMware-NSX/4.1/administration/GUID-9CD3FC21-9ED4-4FB3-9E19-67A7C4D1F53E.html">VMware Docs Idendity Firewall</a></p>
]]></content>
		</item>
		
		<item>
			<title>NSX 4.X Certificate exchange of the NSX Manager</title>
			<link>https://sdn-warrior.org/posts/nsx-cert-exchange/</link>
			<pubDate>Fri, 05 Apr 2024 23:22:00 +0100</pubDate>
			
			<guid>https://sdn-warrior.org/posts/nsx-cert-exchange/</guid>
			<description><![CDATA[Exchange your NSX Manager certificates]]></description>
			<content type="html"><![CDATA[<h1 id="nsx-4x-certificate-exchange-of-the-nsx-manager">NSX 4.X Certificate exchange of the NSX Manager</h1>
<h1 id="certificate-creation">Certificate creation</h1>
<p>First of all, we need a CSR request. This can be created with OPENSSL. It is important that the key is also exported. You can either create 4 individual certificates (VIP and the three manager nodes) or a SAN certificate with all DNS and IP names of the manager nodes. The easiest way is to carry out the request on a manager node. To do this, I create an openssl config file with VIM.</p>
<pre tabindex="0"><code>[req]
default_bits = 4096
default_md = sha256
days = 365
distinguished_name = req_distinguished_name
req_extensions = v3_req
prompt = no
 
[req_distinguished_name]
C   = DE
ST  = RLP
L   = NW
O   = Land RLP
OU  = sdnwarrior
CN  = nsxm0001.lab.home
emailAddress = mail@lab.home
 
[v3_req]
subjectAltName = @sans
 
[sans]
DNS.1 = nsxm0001.lab.home
DNS.2 = nsxm0002.lab.home
DNS.3 = nsxm0003.lab.home
DNS.4 = nsxm0004.lab.home
IP.1 = 192.168.12.110
IP.2 = 192.168.12.111
IP.3 = 192.168.12.112
IP.4 = 192.168.12.113
</code></pre><p>The CSR is generated with the following command:</p>
<pre tabindex="0"><code>openssl req -new -newkey rsa:4096 -nodes -keyout nsxm0001.key -out nsxm0001.csr -config opnssl.cnf
</code></pre><p>Two files are generated, a private key file and the actual request, which must be submitted to the CA.</p>

    <aside class="admonition attention">
        <div class="admonition-title">
            <div class="icon"><svg xmlns="http://www.w3.org/2000/svg" class="feather feather-link" width="24" height="24" viewBox="0 0 24 24"
      fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
      <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
      <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
   </svg></div><b>Attention</b>
        </div>
        <div class="admonition-content">The CA must issue the certificate with the extension basicConstraints = cA:FALSE, otherwise the certificate cannot be used. With a Windows CA, this must be explicitly permitted in the template. If the extension is missing, the certificate validation will fail with an error message that the certificate key does not match the certificate.</div>
    </aside>
<h2 id="import-certificate">Import certificate</h2>
<p>The certificate can be imported in the NSX Manager under System &gt; Certificates &gt; Import. Here it must be ensured that the service certificate slider is set to NO. The complete certificate chain is also required. The certificate chain must be in the industry standard order of ‘certificate – intermediate – root.</p>

<figure><picture>
          <source srcset="/nsx-cert/01_hu8747713429439574210.webp" type="image/webp">
          <source srcset="/nsx-cert/01_hu1064617936367254404.jpg" type="image/jpeg">
          <img src="/nsx-cert/01_hu8747713429439574210.webp"alt="NSX Cert"  width="582"  height="924" />
        </picture><figcaption>
            <p>Import NSX Cert</p>
          </figcaption></figure>
<p>After the import, the certificate can be validated using an API request.
API calls may vary depending on the NSX-T versions, in my example NSX version 4.1.2.3 is used.</p>
<pre tabindex="0"><code>GET https://&lt;nsx-mgr&gt;/api/v1/trust- management/certificates/&lt;cert-id&gt;?action=validate
</code></pre><h2 id="exchange-of-certificates">Exchange of certificates</h2>
<p>An API request must be executed for each manager node and for the VIP. This requires the certificate ID and the manager node ID. Both can be copied from the WebGUI or requested via API Get Requests.</p>
<p>The following API call is used to exchange the Manager Node certificate:</p>
<pre tabindex="0"><code>POST /api/v1/trust-management/certificates/&lt;cert- id&gt;?action=apply_certificate&amp;service_type=API&amp;node_id=&lt;node- id&gt;
</code></pre><p>The following API call is used to exchange the cluster VIP certificate:</p>
<pre tabindex="0"><code>POST /api/v1/trust-management/certificates/&lt;cert- id&gt;?action=apply_certificate&amp;service_type=MGMT_CLUSTER
</code></pre><p>After replacing the certificates, you should close all browser windows and log in to the NSX Manager again. The certificate should now have been successfully replaced.</p>
<h2 id="further-resources">Further resources:</h2>
<p><a href="https://docs.vmware.com/en/VMware-NSX-T-Data-Center/3.2/administration/GUID-50C36862-A29D-48FA-8CE7-697E64E10E37.html">VMware Administration Handbook</a></p>
]]></content>
		</item>
		
	</channel>
</rss>
